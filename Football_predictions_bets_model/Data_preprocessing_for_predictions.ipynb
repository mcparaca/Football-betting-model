{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103c815b-b57b-4f28-a012-e83c4b09cbd1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\r\n",
    "\r\n",
    "In this phase, we will implement the same preprocessing methods utilized in previous iterations to ensure that our dataset achieves a consistent format and is ready for modeling. This process involves several key steps to guarantee that the data is clean, well-organized, and optimized for analysis.\r\n",
    "\r\n",
    "### Steps for Data Preprocessing\r\n",
    "\r\n",
    "1. **Combining League Data**:  \r\n",
    "   The first step is to merge all league data into a single comprehensive dataframe. This involves concatenating the individual league datasets, ensuring that all relevant matches are included. Once combined, we will sort the dataset chronologically by match date, which will facilitate further analysis and modeling.\r\n",
    "\r\n",
    "2. **Appending H2H and Elo Columns**:  \r\n",
    "   After consolidating the league data, we will append the historical head-to-head (H2H) statistics and updated Elo ratings to the main dataframe. These additional columns provide valuable insights into team performance and rivalry dynamics, which are crucial for predicting match outcomes.\r\n",
    "\r\n",
    "3. **Removing Unnecessary Columns**:  \r\n",
    "   With the dataset now enriched with relevant features, we will identify and remove any columns that do not contribute meaningfully to our analysis or modeling efforts. This helps to streamline the dataset, reducing dimensionality and improving computational efficiency.\r\n",
    "\r\n",
    "4. **Handling Missing Values**:  \r\n",
    "   Any missing values in the dataset will be addressed using appropriate imputation techniques. This step is vital to ensure that our model has complete data to work with, as missing values can significantly impact the accuracy of predictions. We may choose to fill missing values with statistical measures (e.g., mean, median) or by employing more sophisticated imputation methods depending on the nature of the data.\r\n",
    "\r\n",
    "5. **Standardizing the Data**:  \r\n",
    "   To prepare the dataset for modeling, we will standardize the feature values using techniques such as min-max scaling or z-score normalization. This process ensures that all features are on a comparable scale, which is particularly important for algorithms that rely on distance measurements or gradient descent.\r\n",
    "\r\n",
    "6. **Saving the Processed Dataframe**:  \r\n",
    "   Finally, we will save the resulting standardized dataframe into a file, typically in CSV format. This file will serve as the input for our modeling efforts, ensuring that all preprocessing steps have been completed and that the data isy into the modeling phase.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a336f72e-a8c3-4ca9-94d0-649fe85378f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from Dataset_functions import *\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ad42f52-3c49-42a8-93b9-651de46e3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "premier_league_data = pd.read_csv('premier_league_data_v1.csv')\n",
    "championship_data = pd.read_csv('championship_data.csv')\n",
    "spanish1_data = pd.read_csv('spanish1_data_v1.csv')\n",
    "spanish2_data = pd.read_csv('spanish2_data.csv')\n",
    "italian1_data = pd.read_csv('italian1_data_v1.csv')\n",
    "italian2_data = pd.read_csv('italian2_data.csv')\n",
    "german1_data = pd.read_csv('german1_data_v1.csv')\n",
    "german2_data = pd.read_csv('german2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4658ab2-44fe-43fd-8619-bde0574e8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "england_data = pd.concat([championship_data,spanish1_data,spanish2_data,italian1_data,italian2_data,german1_data,german2_data, premier_league_data])\n",
    "england_data.fillna(0, inplace = True)\n",
    "england_data.sort_values(['Date'], ascending = True, inplace = True)\n",
    "\n",
    "home_team_columns = [col for col in england_data.columns if col.startswith('HomeTeam_')]\n",
    "england_data['HomeTeam'] = england_data[home_team_columns].idxmax(axis=1).str.replace('HomeTeam_', '')\n",
    "\n",
    "# Step 2: Dedummify AwayTeam columns\n",
    "away_team_columns = [col for col in england_data.columns if col.startswith('AwayTeam_')]\n",
    "england_data['AwayTeam'] = england_data[away_team_columns].idxmax(axis=1).str.replace('AwayTeam_', '')\n",
    "england_data.reset_index(inplace = True, drop = True)\n",
    "england_data[['HomeTeam','AwayTeam']].to_csv('home_away_v1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c96c66f-e344-4d08-aebb-ed07530ce54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2h = pd.read_csv('h2h_data_v1.csv')\n",
    "england_data.drop_duplicates(['Date',\t'HomeTeam',\t'AwayTeam'], inplace = True)\n",
    "h2h.drop_duplicates(['Date',\t'HomeTeam',\t'AwayTeam'], inplace = True)\n",
    "england_data.reset_index(inplace = True, drop = True)\n",
    "h2h.reset_index(inplace = True, drop = True)\n",
    "england_data = pd.merge(england_data, h2h[['Date',\t'HomeTeam',\t'AwayTeam','Home_h2h_Goals',\t'Home_h2h_Points',\t'Away_h2h_Goals',\t'Away_h2h_Points', 'home_elo', 'away_elo', 'month', 'year','Avg>2.5',\t'Avg<2.5']], on =['Date',\t'HomeTeam',\t'AwayTeam'], how = 'left')\n",
    "\n",
    "england_data[['HomeTeam',\t'AwayTeam', 'Date']].to_csv('next_fixture_teams.csv', index = False)\n",
    "england_data.drop(['HomeTeam', 'AwayTeam'], axis = 1, inplace = True)\n",
    "def assign_time_bucket(time_str):\n",
    "    hour = int(time_str.split(':')[0])\n",
    "    \n",
    "    if hour < 15:\n",
    "        return 'morning_afternoon'\n",
    "    elif 15 <= hour < 19:\n",
    "        return 'afternoon_evening'\n",
    "    else:\n",
    "        return 'evening_night'\n",
    "# Apply the bucketing function to the 'Time' column\n",
    "england_data['TimeBucket'] = england_data['Time'].apply(assign_time_bucket)\n",
    "england_data.drop(['Date', 'Home xG', 'Away xG', 'Time', 'ovr25_per_game'], axis = 1, inplace= True,errors = 'ignore')\n",
    "england_data = pd.get_dummies(england_data, columns=['Div', 'year', 'month','TimeBucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "835788fc-8860-4679-8b06-6e91dd4a7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = england_data[['FTHG', 'FTAG']]\n",
    "england_data.drop(['FTHG', 'FTAG'], inplace= True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6129e2a9-c0e7-4ac8-89dc-bc87697b1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "england_data['Away_h2h_Points'].fillna(0, inplace = True)\n",
    "england_data['Away_h2h_Goals'].fillna(0, inplace = True)\n",
    "england_data['Home_h2h_Points'].fillna(0, inplace = True)\n",
    "england_data['Home_h2h_Goals'].fillna(0, inplace = True)\n",
    "england_data['home_elo'].fillna(1400, inplace = True)\n",
    "england_data['away_elo'].fillna(1400, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d769ebd0-3947-4e6f-a0c6-52dcfb8a89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2h[['HomeTeam',\t'AwayTeam','Avg>2.5',\t'Avg<2.5']].to_csv('2_5_goals.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6021f46-ee32-44aa-be9f-34a9071f78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "numeric_columns = england_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "england_data_n = pd.DataFrame(scaler_minmax.fit_transform(england_data[numeric_columns]), columns=numeric_columns)\n",
    "england_data[numeric_columns] = england_data_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb4643cc-452a-4093-8c14-71d92f7b1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "england_data.to_csv('default_data_all_variables_v1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec8a6e11-43dd-421b-aec6-5c0d5661f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.to_csv('targets_v1.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f1b9d-dbc1-4ec0-a01c-7b96291c02f0",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "By following these preprocessing steps, we will ensure that our dataset is in optimal condition for modeling. The combination of league data, H2H statistics, and Elo ratings, along with the removal of unnecessary columns and handling of missing values, will enhance the quality of our predictions. Standardizing the data further prepares it for effective analysis, allowing us to proceed confidently into the modeling phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
