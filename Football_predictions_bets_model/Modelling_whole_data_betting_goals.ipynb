{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d85e45-cf48-4f5e-8220-70914a16f61b",
   "metadata": {},
   "source": [
    "## Modeling Part\r\n",
    "\r\n",
    "In this section, we will build a machine learning model to predict the number of goals scored by both teams during a game. Predicting football match outcomes is inherently challenging due to the high degree of variability and unpredictability involved. Many matches are influenced by intangible factors such as player form, individual decisions, and situational dynamics that are difficult to capture in numerical data. Despite these complexities, our goal is to construct a model that performs as accurately as possible across a diverse range of matches.\r\n",
    "\r\n",
    "### Challenges in Modeling Football Matches\r\n",
    "\r\n",
    "Football match outcomes are known to be influenced by numerous variables, both observable and hidden. Even with extensive data, some matches remain inherently unpredictable due to factors such as:\r\n",
    "\r\n",
    "- **Team Strategy and Lineups**: A change in lineup or tactical approach can significantly alter a team’s performance.\r\n",
    "- **Player Form and Injuries**: Variations in player fitness and day-to-day form can have a substantial impact on the final outcome.\r\n",
    "- **Psychological Factors**: Motivation, morale, and team dynamics play a significant role but are hard to quantify.\r\n",
    "\r\n",
    "Given these nuances, building a robust predictive model requires a blend of feature engineering, advanced modeling techniques, and iterative refinement.\r\n",
    "\r\n",
    "### Model Selection and Approach\r\n",
    "\r\n",
    "To tackle this problem, we will employ a deep learning architecture. Neural networks are particularly well-suited for this type of task because they can capture complex interactions and relationships within large datasets. The model will consist of multiple layers that can extract and learn intricate patterns from the input features, enabling us to identify subtle trends and associations that simpler algorithms might miss.\r\n",
    "\r\n",
    "The deep network architecture will allow us to process our extensive feature set and leverage the power of high-dimensional data representations. Specifically, we will experiment with different network configurations, such as:\r\n",
    "\r\n",
    "- **Fully Connected Neural Networks (Dense Layers)**: Useful for capturing non-linear relationships in tabular data.\r\n",
    "- **Dropout Layers**: To prevent overfitting by randomly deactivating certain neurons during training.\r\n",
    "- **Activation FunctionLinearch as ReLU and Sigmoid, which help introduce non-linearity into the model and handle complex data transformations.\r\n",
    "\r\n",
    "### Iterative Process\r\n",
    "\r\n",
    "The process of training a reliable model will require multiple iterations and fine-tuning. We will evaluate the model's performance using various metrics and adjust hyperparameters accordingly until we achieve satisfactory results. Given the unpredictable nature of football matches, our focus will be on optimizing the model to generalize well across different scenarios, rather than just maximizing accuracy on the training set.\r\n",
    "\r\n",
    "By leveraging the capabilities of deep learning, we aim to construct a model that can identify meaningful patterns within the data and provide robust predictions, even in a challenging and unpredictable domain like football match outcomes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a06c445-fe4c-4a40-92be-f816ad49c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2499210-80a3-4cc0-89af-f01929875ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('default_data_all_variables_v1.csv')\n",
    "target = pd.read_csv('targets_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e3cad-d334-4422-a584-167fa253be2b",
   "metadata": {},
   "source": [
    "We need to limit the number of variables that negatively affect the model's performance, as identified during the experimentation phase. In the process of building and testing various models, we observed that certain variables introduced noise or added complexity without contributing positively to the accuracy or generalization of the model.\r\n",
    "\r\n",
    "Although I will omit the detailed experimentation phase here, the key takeaway is that eliminating or reducing the influence of these problematic variables will improve the overall robustness of the model. This process of feature selection or reduction will help streamline the data, ensuring that the model focuses on the most impactful variables, leading to better predictions.\r\n",
    "\r\n",
    "By carefully curating the feature set, we aim to strike a balance between having enough information to capture meaningful patterns and avoiding overfitting or unnecessary complexity. This refined approach will allow the model to generalize better across unseen data, improving its ability to predict match outcomes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3bd3f92-6363-4b96-a3f9-a640b37dfd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains('HomeTeam|AwayTeam')]\n",
    "df = df.loc[:, ~df.columns.str.contains('year|month')]\n",
    "df = df.loc[:, ~df.columns.str.contains('ovr')]\n",
    "df = df.loc[:, ~df.columns.str.contains('corner')]\n",
    "df = df.loc[:, ~df.columns.str.contains('TimeBucket')]\n",
    "df = df.loc[:, ~df.columns.str.contains('Avg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed489eb3-d535-47fe-bec0-864fa2636a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[['FTHG', 'FTAG']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c06d6-3780-475b-9530-552115a74f00",
   "metadata": {},
   "source": [
    "Last glimpse of the table before we procced to the data division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6f5c32f-f111-40b9-90f1-27bc3a629322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchday</th>\n",
       "      <th>total_points</th>\n",
       "      <th>total_points_home_team</th>\n",
       "      <th>total_points_away_team</th>\n",
       "      <th>5_form</th>\n",
       "      <th>5_form_home_team</th>\n",
       "      <th>5_form_away_team</th>\n",
       "      <th>10_form</th>\n",
       "      <th>10_form_home_team</th>\n",
       "      <th>10_form_away_team</th>\n",
       "      <th>total_goals</th>\n",
       "      <th>total_goals_home_team</th>\n",
       "      <th>total_goals_away_team</th>\n",
       "      <th>total_goals_against</th>\n",
       "      <th>total_goals_against_home_team</th>\n",
       "      <th>total_goals_against_away_team</th>\n",
       "      <th>total_goals_per_game</th>\n",
       "      <th>total_goals_per_game_home_team</th>\n",
       "      <th>total_goals_per_game_away_team</th>\n",
       "      <th>total_goals_against_per_game</th>\n",
       "      <th>total_goals_against_per_game_home_team</th>\n",
       "      <th>total_goals_against_per_game_away_team</th>\n",
       "      <th>5_form_goals_scored</th>\n",
       "      <th>5_form_goals_scored_home_team</th>\n",
       "      <th>5_form_goals_scored_away_team</th>\n",
       "      <th>5_form_goals_against</th>\n",
       "      <th>5_form_goals_against_home_team</th>\n",
       "      <th>5_form_goals_against_away_team</th>\n",
       "      <th>10_form_goals_scored</th>\n",
       "      <th>10_form_goals_scored_home_team</th>\n",
       "      <th>10_form_goals_scored_away_team</th>\n",
       "      <th>10_form_goals_against</th>\n",
       "      <th>10_form_goals_against_home_team</th>\n",
       "      <th>10_form_goals_against_away_team</th>\n",
       "      <th>total_shots_per_game</th>\n",
       "      <th>total_shots_per_game_home_team</th>\n",
       "      <th>total_shots_per_game_away_team</th>\n",
       "      <th>total_shots_against_per_game</th>\n",
       "      <th>total_shots_against_per_game_home_team</th>\n",
       "      <th>total_shots_against_per_game_away_team</th>\n",
       "      <th>total_shots_on_target_per_game</th>\n",
       "      <th>total_shots_on_target_per_game_home_team</th>\n",
       "      <th>total_shots_on_target_per_game_away_team</th>\n",
       "      <th>total_shots_on_target_against_per_game</th>\n",
       "      <th>total_shots_on_target_against_per_game_home_team</th>\n",
       "      <th>total_shots_on_target_against_per_game_away_team</th>\n",
       "      <th>5_form_shots</th>\n",
       "      <th>5_form_shots_home_team</th>\n",
       "      <th>5_form_shots_away_team</th>\n",
       "      <th>5_form_shots_against</th>\n",
       "      <th>5_form_shots_against_home_team</th>\n",
       "      <th>5_form_shots_against_away_team</th>\n",
       "      <th>10_form_shots</th>\n",
       "      <th>10_form_shots_home_team</th>\n",
       "      <th>10_form_shots_away_team</th>\n",
       "      <th>10_form_shots_against</th>\n",
       "      <th>10_form_shots_against_home_team</th>\n",
       "      <th>10_form_shots_against_away_team</th>\n",
       "      <th>5_form_shots_on_target</th>\n",
       "      <th>5_form_shots_on_target_home_team</th>\n",
       "      <th>5_form_shots_on_target_away_team</th>\n",
       "      <th>5_form_shots_on_target_against</th>\n",
       "      <th>5_form_shots_on_target_against_home_team</th>\n",
       "      <th>5_form_shots_on_target_against_away_team</th>\n",
       "      <th>10_form_shots_on_target</th>\n",
       "      <th>10_form_shots_on_target_home_team</th>\n",
       "      <th>10_form_shots_on_target_away_team</th>\n",
       "      <th>10_form_shots_on_target_against</th>\n",
       "      <th>10_form_shots_on_target_against_home_team</th>\n",
       "      <th>10_form_shots_on_target_against_away_team</th>\n",
       "      <th>total_yellow_cards_per_game</th>\n",
       "      <th>total_yellow_cards_per_game_home_team</th>\n",
       "      <th>total_yellow_cards_per_game_away_team</th>\n",
       "      <th>total_yellow_cards_against_per_game</th>\n",
       "      <th>total_yellow_cards_against_per_game_home_team</th>\n",
       "      <th>total_yellow_cards_against_per_game_away_team</th>\n",
       "      <th>total_red_cards_per_game</th>\n",
       "      <th>total_red_cards_per_game_home_team</th>\n",
       "      <th>total_red_cards_per_game_away_team</th>\n",
       "      <th>total_red_cards_against_per_game</th>\n",
       "      <th>total_red_cards_against_per_game_home_team</th>\n",
       "      <th>total_red_cards_against_per_game_away_team</th>\n",
       "      <th>total_xg_per_game</th>\n",
       "      <th>total_xg_per_game_home_team</th>\n",
       "      <th>total_xg_per_game_away_team</th>\n",
       "      <th>total_xg_against_per_game</th>\n",
       "      <th>total_xg_against_per_game_home_team</th>\n",
       "      <th>total_xg_against_per_game_away_team</th>\n",
       "      <th>5_form_xg</th>\n",
       "      <th>5_form_xg_home_team</th>\n",
       "      <th>5_form_xg_away_team</th>\n",
       "      <th>5_form_xg_against</th>\n",
       "      <th>5_form_xg_against_home_team</th>\n",
       "      <th>5_form_xg_against_away_team</th>\n",
       "      <th>10_form_xg</th>\n",
       "      <th>10_form_xg_home_team</th>\n",
       "      <th>10_form_xg_away_team</th>\n",
       "      <th>10_form_xg_against</th>\n",
       "      <th>10_form_xg_against_home_team</th>\n",
       "      <th>10_form_xg_against_away_team</th>\n",
       "      <th>total_points_away</th>\n",
       "      <th>total_points_home</th>\n",
       "      <th>5_form_away</th>\n",
       "      <th>10_form_away</th>\n",
       "      <th>5_form_home</th>\n",
       "      <th>10_form_home</th>\n",
       "      <th>total_goals_away</th>\n",
       "      <th>total_goals_against_away</th>\n",
       "      <th>total_goals_per_game_away</th>\n",
       "      <th>total_goals_against_per_game_away</th>\n",
       "      <th>total_goals_home</th>\n",
       "      <th>total_goals_against_home</th>\n",
       "      <th>total_goals_per_game_home</th>\n",
       "      <th>total_goals_against_per_game_home</th>\n",
       "      <th>5_form_goals_scored_away</th>\n",
       "      <th>10_form_goals_scored_away</th>\n",
       "      <th>5_form_goals_against_away</th>\n",
       "      <th>10_form_goals_against_away</th>\n",
       "      <th>5_form_goals_scored_home</th>\n",
       "      <th>10_form_goals_scored_home</th>\n",
       "      <th>5_form_goals_against_home</th>\n",
       "      <th>10_form_goals_against_home</th>\n",
       "      <th>total_shots_per_game_home</th>\n",
       "      <th>total_shots_against_per_game_home</th>\n",
       "      <th>total_shots_per_game_away</th>\n",
       "      <th>total_shots_against_per_game_away</th>\n",
       "      <th>total_shots_on_target_per_game_home</th>\n",
       "      <th>total_shots_on_target_against_per_game_home</th>\n",
       "      <th>total_shots_on_target_per_game_away</th>\n",
       "      <th>total_shots_on_target_against_per_game_away</th>\n",
       "      <th>5_form_shots_away</th>\n",
       "      <th>10_form_shots_away</th>\n",
       "      <th>5_form_shots_against_away</th>\n",
       "      <th>10_form_shots_against_away</th>\n",
       "      <th>5_form_shots_home</th>\n",
       "      <th>10_form_shots_home</th>\n",
       "      <th>5_form_shots_against_home</th>\n",
       "      <th>10_form_shots_against_home</th>\n",
       "      <th>5_form_shots_on_target_away</th>\n",
       "      <th>10_form_shots_on_target_away</th>\n",
       "      <th>5_form_shots_on_target_against_away</th>\n",
       "      <th>10_form_shots_on_target_against_away</th>\n",
       "      <th>5_form_shots_on_target_home</th>\n",
       "      <th>10_form_shots_on_target_home</th>\n",
       "      <th>5_form_shots_on_target_against_home</th>\n",
       "      <th>10_form_shots_on_target_against_home</th>\n",
       "      <th>total_yellow_cards_per_game_home</th>\n",
       "      <th>total_yellow_cards_against_per_game_home</th>\n",
       "      <th>total_yellow_cards_per_game_away</th>\n",
       "      <th>total_yellow_cards_against_per_game_away</th>\n",
       "      <th>total_red_cards_per_game_home</th>\n",
       "      <th>total_red_cards_against_per_game_home</th>\n",
       "      <th>total_red_cards_per_game_away</th>\n",
       "      <th>total_red_cards_against_per_game_away</th>\n",
       "      <th>total_xg_per_game_home</th>\n",
       "      <th>total_xg_against_per_game_home</th>\n",
       "      <th>total_xg_per_game_away</th>\n",
       "      <th>total_xg_against_per_game_away</th>\n",
       "      <th>5_form_xg_away</th>\n",
       "      <th>10_form_xg_away</th>\n",
       "      <th>5_form_xg_against_away</th>\n",
       "      <th>10_form_xg_against_away</th>\n",
       "      <th>5_form_xg_home</th>\n",
       "      <th>10_form_xg_home</th>\n",
       "      <th>5_form_xg_against_home</th>\n",
       "      <th>10_form_xg_against_home</th>\n",
       "      <th>home_win_odds</th>\n",
       "      <th>draw_odds</th>\n",
       "      <th>away_win_odds</th>\n",
       "      <th>HC</th>\n",
       "      <th>AC</th>\n",
       "      <th>Home_h2h_Goals</th>\n",
       "      <th>Home_h2h_Points</th>\n",
       "      <th>Away_h2h_Goals</th>\n",
       "      <th>Away_h2h_Points</th>\n",
       "      <th>home_elo</th>\n",
       "      <th>away_elo</th>\n",
       "      <th>Div_D1</th>\n",
       "      <th>Div_D2</th>\n",
       "      <th>Div_E0</th>\n",
       "      <th>Div_E1</th>\n",
       "      <th>Div_I1</th>\n",
       "      <th>Div_SP1</th>\n",
       "      <th>Div_SP2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.696570</td>\n",
       "      <td>0.439803</td>\n",
       "      <td>0.166090</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.691680</td>\n",
       "      <td>0.521222</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438064</td>\n",
       "      <td>0.609870</td>\n",
       "      <td>0.343866</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430666</td>\n",
       "      <td>0.440082</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180271</td>\n",
       "      <td>0.477044</td>\n",
       "      <td>0.688533</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377394</td>\n",
       "      <td>0.675734</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646363</td>\n",
       "      <td>0.505708</td>\n",
       "      <td>0.182422</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490968</td>\n",
       "      <td>0.434453</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.615690</td>\n",
       "      <td>0.532923</td>\n",
       "      <td>0.199622</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567330</td>\n",
       "      <td>0.451446</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17252</th>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.426950</td>\n",
       "      <td>0.4128</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.582955</td>\n",
       "      <td>0.568224</td>\n",
       "      <td>0.494362</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0.465306</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.438462</td>\n",
       "      <td>0.357664</td>\n",
       "      <td>0.176955</td>\n",
       "      <td>0.438462</td>\n",
       "      <td>0.238494</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.236515</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.123711</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.359184</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298667</td>\n",
       "      <td>0.305455</td>\n",
       "      <td>0.391111</td>\n",
       "      <td>0.685321</td>\n",
       "      <td>0.614815</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.284974</td>\n",
       "      <td>0.479769</td>\n",
       "      <td>0.509202</td>\n",
       "      <td>0.381503</td>\n",
       "      <td>0.124629</td>\n",
       "      <td>0.128049</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.309701</td>\n",
       "      <td>0.291228</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04878</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.489130</td>\n",
       "      <td>0.387205</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.108491</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>0.198529</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.139896</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259887</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.207679</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.098837</td>\n",
       "      <td>0.054662</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.122995</td>\n",
       "      <td>0.073016</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.411667</td>\n",
       "      <td>0.572953</td>\n",
       "      <td>0.392063</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.466496</td>\n",
       "      <td>0.552350</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17253</th>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.317730</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.480682</td>\n",
       "      <td>0.506750</td>\n",
       "      <td>0.474184</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.440816</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.248062</td>\n",
       "      <td>0.373016</td>\n",
       "      <td>0.469231</td>\n",
       "      <td>0.343066</td>\n",
       "      <td>0.131687</td>\n",
       "      <td>0.469231</td>\n",
       "      <td>0.133891</td>\n",
       "      <td>0.208889</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>0.198312</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.108911</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.301075</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04878</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.300505</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.286195</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.199005</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.080189</td>\n",
       "      <td>0.179856</td>\n",
       "      <td>0.105042</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.052434</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>0.072539</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064171</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.460946</td>\n",
       "      <td>0.598374</td>\n",
       "      <td>0.326161</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293262</td>\n",
       "      <td>0.449534</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17254</th>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.506383</td>\n",
       "      <td>0.3456</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.368182</td>\n",
       "      <td>0.767601</td>\n",
       "      <td>0.363205</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.293878</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.262774</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.213389</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.319502</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.227723</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04878</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.429293</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.080189</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.396396</td>\n",
       "      <td>0.227979</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373724</td>\n",
       "      <td>0.630050</td>\n",
       "      <td>0.400371</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.570864</td>\n",
       "      <td>0.594489</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17255</th>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.248227</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.634091</td>\n",
       "      <td>0.618069</td>\n",
       "      <td>0.383383</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.293878</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.410853</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.277372</td>\n",
       "      <td>0.102881</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.221757</td>\n",
       "      <td>0.275556</td>\n",
       "      <td>0.257261</td>\n",
       "      <td>0.160338</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.493878</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.404082</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.476444</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.073892</td>\n",
       "      <td>0.073892</td>\n",
       "      <td>0.347150</td>\n",
       "      <td>0.485549</td>\n",
       "      <td>0.515337</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.044510</td>\n",
       "      <td>0.045732</td>\n",
       "      <td>0.198813</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.294737</td>\n",
       "      <td>0.157343</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02439</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.489130</td>\n",
       "      <td>0.420875</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.318408</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.117925</td>\n",
       "      <td>0.165468</td>\n",
       "      <td>0.096639</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.139896</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.402299</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.415358</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.197674</td>\n",
       "      <td>0.109325</td>\n",
       "      <td>0.150838</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.048128</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.142276</td>\n",
       "      <td>0.265519</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.560013</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.330767</td>\n",
       "      <td>0.599204</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17256</th>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.526241</td>\n",
       "      <td>0.5088</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.419318</td>\n",
       "      <td>0.408723</td>\n",
       "      <td>0.332938</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.323077</td>\n",
       "      <td>0.323077</td>\n",
       "      <td>0.269388</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.315385</td>\n",
       "      <td>0.240876</td>\n",
       "      <td>0.218107</td>\n",
       "      <td>0.315385</td>\n",
       "      <td>0.263598</td>\n",
       "      <td>0.182222</td>\n",
       "      <td>0.170124</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.538776</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.391111</td>\n",
       "      <td>0.445872</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.384236</td>\n",
       "      <td>0.384236</td>\n",
       "      <td>0.284974</td>\n",
       "      <td>0.312139</td>\n",
       "      <td>0.331288</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.231454</td>\n",
       "      <td>0.237805</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.157343</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02439</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.589226</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>0.219298</td>\n",
       "      <td>0.517413</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.289256</td>\n",
       "      <td>0.165094</td>\n",
       "      <td>0.115108</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.108808</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429379</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>0.293194</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.077170</td>\n",
       "      <td>0.156425</td>\n",
       "      <td>0.098246</td>\n",
       "      <td>0.203209</td>\n",
       "      <td>0.120635</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.432667</td>\n",
       "      <td>0.519665</td>\n",
       "      <td>0.399467</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.586382</td>\n",
       "      <td>0.615623</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17257 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       matchday  total_points  total_points_home_team  total_points_away_team  \\\n",
       "0      0.000000      0.000000                0.000000                0.000000   \n",
       "1      0.000000      0.000000                0.000000                0.000000   \n",
       "2      0.000000      0.000000                0.000000                0.000000   \n",
       "3      0.000000      0.000000                0.000000                0.000000   \n",
       "4      0.000000      0.000000                0.000000                0.000000   \n",
       "...         ...           ...                     ...                     ...   \n",
       "17252  0.088889      0.000000                0.000000                0.020619   \n",
       "17253  0.088889      0.062500                0.020408                0.061856   \n",
       "17254  0.088889      0.072917                0.081633                0.072165   \n",
       "17255  0.088889      0.020833                0.020408                0.092784   \n",
       "17256  0.088889      0.062500                0.061224                0.041237   \n",
       "\n",
       "         5_form  5_form_home_team  5_form_away_team   10_form  \\\n",
       "0      0.000000          0.000000          0.000000  0.000000   \n",
       "1      0.000000          0.000000          0.000000  0.000000   \n",
       "2      0.000000          0.000000          0.000000  0.000000   \n",
       "3      0.000000          0.000000          0.000000  0.000000   \n",
       "4      0.000000          0.000000          0.000000  0.000000   \n",
       "...         ...               ...               ...       ...   \n",
       "17252  0.000000          0.000000          0.133333  0.000000   \n",
       "17253  0.400000          0.133333          0.400000  0.200000   \n",
       "17254  0.466667          0.533333          0.466667  0.233333   \n",
       "17255  0.133333          0.133333          0.600000  0.066667   \n",
       "17256  0.400000          0.400000          0.266667  0.200000   \n",
       "\n",
       "       10_form_home_team  10_form_away_team  total_goals  \\\n",
       "0               0.000000           0.000000     0.000000   \n",
       "1               0.000000           0.000000     0.000000   \n",
       "2               0.000000           0.000000     0.000000   \n",
       "3               0.000000           0.000000     0.000000   \n",
       "4               0.000000           0.000000     0.000000   \n",
       "...                  ...                ...          ...   \n",
       "17252           0.000000           0.066667     0.040404   \n",
       "17253           0.066667           0.200000     0.060606   \n",
       "17254           0.266667           0.233333     0.080808   \n",
       "17255           0.066667           0.300000     0.020202   \n",
       "17256           0.200000           0.133333     0.050505   \n",
       "\n",
       "       total_goals_home_team  total_goals_away_team  total_goals_against  \\\n",
       "0                   0.000000               0.000000             0.000000   \n",
       "1                   0.000000               0.000000             0.000000   \n",
       "2                   0.000000               0.000000             0.000000   \n",
       "3                   0.000000               0.000000             0.000000   \n",
       "4                   0.000000               0.000000             0.000000   \n",
       "...                      ...                    ...                  ...   \n",
       "17252               0.039216               0.037736             0.128713   \n",
       "17253               0.039216               0.056604             0.049505   \n",
       "17254               0.049020               0.075472             0.059406   \n",
       "17255               0.019608               0.066038             0.069307   \n",
       "17256               0.049020               0.056604             0.049505   \n",
       "\n",
       "       total_goals_against_home_team  total_goals_against_away_team  \\\n",
       "0                           0.000000                           0.00   \n",
       "1                           0.000000                           0.00   \n",
       "2                           0.000000                           0.00   \n",
       "3                           0.000000                           0.00   \n",
       "4                           0.000000                           0.00   \n",
       "...                              ...                            ...   \n",
       "17252                       0.128713                           0.07   \n",
       "17253                       0.089109                           0.05   \n",
       "17254                       0.029703                           0.06   \n",
       "17255                       0.069307                           0.06   \n",
       "17256                       0.049505                           0.04   \n",
       "\n",
       "       total_goals_per_game  total_goals_per_game_home_team  \\\n",
       "0                      0.00                        0.000000   \n",
       "1                      0.00                        0.000000   \n",
       "2                      0.00                        0.000000   \n",
       "3                      0.00                        0.000000   \n",
       "4                      0.00                        0.000000   \n",
       "...                     ...                             ...   \n",
       "17252                  0.20                        0.213333   \n",
       "17253                  0.30                        0.177778   \n",
       "17254                  0.40                        0.266667   \n",
       "17255                  0.10                        0.106667   \n",
       "17256                  0.25                        0.266667   \n",
       "\n",
       "       total_goals_per_game_away_team  total_goals_against_per_game  \\\n",
       "0                                0.00                        0.0000   \n",
       "1                                0.00                        0.0000   \n",
       "2                                0.00                        0.0000   \n",
       "3                                0.00                        0.0000   \n",
       "4                                0.00                        0.0000   \n",
       "...                               ...                           ...   \n",
       "17252                            0.20                        0.8125   \n",
       "17253                            0.30                        0.3125   \n",
       "17254                            0.40                        0.3750   \n",
       "17255                            0.35                        0.4375   \n",
       "17256                            0.30                        0.3125   \n",
       "\n",
       "       total_goals_against_per_game_home_team  \\\n",
       "0                                       0.000   \n",
       "1                                       0.000   \n",
       "2                                       0.000   \n",
       "3                                       0.000   \n",
       "4                                       0.000   \n",
       "...                                       ...   \n",
       "17252                                   0.650   \n",
       "17253                                   0.375   \n",
       "17254                                   0.150   \n",
       "17255                                   0.350   \n",
       "17256                                   0.250   \n",
       "\n",
       "       total_goals_against_per_game_away_team  5_form_goals_scored  \\\n",
       "0                                    0.000000             0.000000   \n",
       "1                                    0.000000             0.000000   \n",
       "2                                    0.000000             0.000000   \n",
       "3                                    0.000000             0.000000   \n",
       "4                                    0.000000             0.000000   \n",
       "...                                       ...                  ...   \n",
       "17252                                0.381818             0.166667   \n",
       "17253                                0.272727             0.250000   \n",
       "17254                                0.327273             0.333333   \n",
       "17255                                0.327273             0.083333   \n",
       "17256                                0.218182             0.208333   \n",
       "\n",
       "       5_form_goals_scored_home_team  5_form_goals_scored_away_team  \\\n",
       "0                           0.000000                       0.000000   \n",
       "1                           0.000000                       0.000000   \n",
       "2                           0.000000                       0.000000   \n",
       "3                           0.000000                       0.000000   \n",
       "4                           0.000000                       0.000000   \n",
       "...                              ...                            ...   \n",
       "17252                       0.166667                       0.173913   \n",
       "17253                       0.166667                       0.260870   \n",
       "17254                       0.208333                       0.347826   \n",
       "17255                       0.083333                       0.304348   \n",
       "17256                       0.208333                       0.260870   \n",
       "\n",
       "       5_form_goals_against  5_form_goals_against_home_team  \\\n",
       "0                  0.000000                        0.000000   \n",
       "1                  0.000000                        0.000000   \n",
       "2                  0.000000                        0.000000   \n",
       "3                  0.000000                        0.000000   \n",
       "4                  0.000000                        0.000000   \n",
       "...                     ...                             ...   \n",
       "17252              0.619048                        0.590909   \n",
       "17253              0.238095                        0.409091   \n",
       "17254              0.285714                        0.136364   \n",
       "17255              0.333333                        0.318182   \n",
       "17256              0.238095                        0.227273   \n",
       "\n",
       "       5_form_goals_against_away_team  10_form_goals_scored  \\\n",
       "0                            0.000000              0.000000   \n",
       "1                            0.000000              0.000000   \n",
       "2                            0.000000              0.000000   \n",
       "3                            0.000000              0.000000   \n",
       "4                            0.000000              0.000000   \n",
       "...                               ...                   ...   \n",
       "17252                        0.333333              0.108108   \n",
       "17253                        0.238095              0.162162   \n",
       "17254                        0.285714              0.216216   \n",
       "17255                        0.285714              0.054054   \n",
       "17256                        0.190476              0.135135   \n",
       "\n",
       "       10_form_goals_scored_home_team  10_form_goals_scored_away_team  \\\n",
       "0                            0.000000                        0.000000   \n",
       "1                            0.000000                        0.000000   \n",
       "2                            0.000000                        0.000000   \n",
       "3                            0.000000                        0.000000   \n",
       "4                            0.000000                        0.000000   \n",
       "...                               ...                             ...   \n",
       "17252                        0.105263                        0.102564   \n",
       "17253                        0.105263                        0.153846   \n",
       "17254                        0.131579                        0.205128   \n",
       "17255                        0.052632                        0.179487   \n",
       "17256                        0.131579                        0.153846   \n",
       "\n",
       "       10_form_goals_against  10_form_goals_against_home_team  \\\n",
       "0                    0.00000                         0.000000   \n",
       "1                    0.00000                         0.000000   \n",
       "2                    0.00000                         0.000000   \n",
       "3                    0.00000                         0.000000   \n",
       "4                    0.00000                         0.000000   \n",
       "...                      ...                              ...   \n",
       "17252                0.40625                         0.371429   \n",
       "17253                0.15625                         0.257143   \n",
       "17254                0.18750                         0.085714   \n",
       "17255                0.21875                         0.200000   \n",
       "17256                0.15625                         0.142857   \n",
       "\n",
       "       10_form_goals_against_away_team  total_shots_per_game  \\\n",
       "0                             0.000000              0.000000   \n",
       "1                             0.000000              0.000000   \n",
       "2                             0.000000              0.000000   \n",
       "3                             0.000000              0.000000   \n",
       "4                             0.000000              0.000000   \n",
       "...                                ...                   ...   \n",
       "17252                         0.200000              0.426950   \n",
       "17253                         0.142857              0.317730   \n",
       "17254                         0.171429              0.506383   \n",
       "17255                         0.171429              0.248227   \n",
       "17256                         0.114286              0.526241   \n",
       "\n",
       "       total_shots_per_game_home_team  total_shots_per_game_away_team  \\\n",
       "0                              0.0000                        0.000000   \n",
       "1                              0.0000                        0.000000   \n",
       "2                              0.0000                        0.000000   \n",
       "3                              0.0000                        0.000000   \n",
       "4                              0.0000                        0.000000   \n",
       "...                               ...                             ...   \n",
       "17252                          0.4128                        0.542857   \n",
       "17253                          0.4480                        0.304762   \n",
       "17254                          0.3456                        0.485714   \n",
       "17255                          0.2400                        0.504762   \n",
       "17256                          0.5088                        0.600000   \n",
       "\n",
       "       total_shots_against_per_game  total_shots_against_per_game_home_team  \\\n",
       "0                          0.000000                                0.000000   \n",
       "1                          0.000000                                0.000000   \n",
       "2                          0.000000                                0.000000   \n",
       "3                          0.000000                                0.000000   \n",
       "4                          0.000000                                0.000000   \n",
       "...                             ...                                     ...   \n",
       "17252                      0.582955                                0.568224   \n",
       "17253                      0.480682                                0.506750   \n",
       "17254                      0.368182                                0.767601   \n",
       "17255                      0.634091                                0.618069   \n",
       "17256                      0.419318                                0.408723   \n",
       "\n",
       "       total_shots_against_per_game_away_team  total_shots_on_target_per_game  \\\n",
       "0                                    0.000000                        0.000000   \n",
       "1                                    0.000000                        0.000000   \n",
       "2                                    0.000000                        0.000000   \n",
       "3                                    0.000000                        0.000000   \n",
       "4                                    0.000000                        0.000000   \n",
       "...                                       ...                             ...   \n",
       "17252                                0.494362                        0.272727   \n",
       "17253                                0.474184                        0.250000   \n",
       "17254                                0.363205                        0.522727   \n",
       "17255                                0.383383                        0.181818   \n",
       "17256                                0.332938                        0.500000   \n",
       "\n",
       "       total_shots_on_target_per_game_home_team  \\\n",
       "0                                      0.000000   \n",
       "1                                      0.000000   \n",
       "2                                      0.000000   \n",
       "3                                      0.000000   \n",
       "4                                      0.000000   \n",
       "...                                         ...   \n",
       "17252                                  0.257143   \n",
       "17253                                  0.267857   \n",
       "17254                                  0.321429   \n",
       "17255                                  0.171429   \n",
       "17256                                  0.471429   \n",
       "\n",
       "       total_shots_on_target_per_game_away_team  \\\n",
       "0                                          0.00   \n",
       "1                                          0.00   \n",
       "2                                          0.00   \n",
       "3                                          0.00   \n",
       "4                                          0.00   \n",
       "...                                         ...   \n",
       "17252                                      0.30   \n",
       "17253                                      0.22   \n",
       "17254                                      0.46   \n",
       "17255                                      0.38   \n",
       "17256                                      0.50   \n",
       "\n",
       "       total_shots_on_target_against_per_game  \\\n",
       "0                                    0.000000   \n",
       "1                                    0.000000   \n",
       "2                                    0.000000   \n",
       "3                                    0.000000   \n",
       "4                                    0.000000   \n",
       "...                                       ...   \n",
       "17252                                0.623077   \n",
       "17253                                0.415385   \n",
       "17254                                0.276923   \n",
       "17255                                0.461538   \n",
       "17256                                0.323077   \n",
       "\n",
       "       total_shots_on_target_against_per_game_home_team  \\\n",
       "0                                              0.000000   \n",
       "1                                              0.000000   \n",
       "2                                              0.000000   \n",
       "3                                              0.000000   \n",
       "4                                              0.000000   \n",
       "...                                                 ...   \n",
       "17252                                          0.623077   \n",
       "17253                                          0.538462   \n",
       "17254                                          0.553846   \n",
       "17255                                          0.461538   \n",
       "17256                                          0.323077   \n",
       "\n",
       "       total_shots_on_target_against_per_game_away_team  5_form_shots  \\\n",
       "0                                              0.000000      0.000000   \n",
       "1                                              0.000000      0.000000   \n",
       "2                                              0.000000      0.000000   \n",
       "3                                              0.000000      0.000000   \n",
       "4                                              0.000000      0.000000   \n",
       "...                                                 ...           ...   \n",
       "17252                                          0.465306      0.335938   \n",
       "17253                                          0.440816      0.250000   \n",
       "17254                                          0.293878      0.398438   \n",
       "17255                                          0.293878      0.195312   \n",
       "17256                                          0.269388      0.414062   \n",
       "\n",
       "       5_form_shots_home_team  5_form_shots_away_team  5_form_shots_against  \\\n",
       "0                    0.000000                0.000000              0.000000   \n",
       "1                    0.000000                0.000000              0.000000   \n",
       "2                    0.000000                0.000000              0.000000   \n",
       "3                    0.000000                0.000000              0.000000   \n",
       "4                    0.000000                0.000000              0.000000   \n",
       "...                       ...                     ...                   ...   \n",
       "17252                0.335938                0.441860              0.452381   \n",
       "17253                0.437500                0.248062              0.373016   \n",
       "17254                0.281250                0.395349              0.285714   \n",
       "17255                0.195312                0.410853              0.492063   \n",
       "17256                0.414062                0.488372              0.325397   \n",
       "\n",
       "       5_form_shots_against_home_team  5_form_shots_against_away_team  \\\n",
       "0                            0.000000                        0.000000   \n",
       "1                            0.000000                        0.000000   \n",
       "2                            0.000000                        0.000000   \n",
       "3                            0.000000                        0.000000   \n",
       "4                            0.000000                        0.000000   \n",
       "...                               ...                             ...   \n",
       "17252                        0.438462                        0.357664   \n",
       "17253                        0.469231                        0.343066   \n",
       "17254                        0.592308                        0.262774   \n",
       "17255                        0.476923                        0.277372   \n",
       "17256                        0.315385                        0.240876   \n",
       "\n",
       "       10_form_shots  10_form_shots_home_team  10_form_shots_away_team  \\\n",
       "0           0.000000                 0.000000                 0.000000   \n",
       "1           0.000000                 0.000000                 0.000000   \n",
       "2           0.000000                 0.000000                 0.000000   \n",
       "3           0.000000                 0.000000                 0.000000   \n",
       "4           0.000000                 0.000000                 0.000000   \n",
       "...              ...                      ...                      ...   \n",
       "17252       0.176955                 0.438462                 0.238494   \n",
       "17253       0.131687                 0.469231                 0.133891   \n",
       "17254       0.209877                 0.592308                 0.213389   \n",
       "17255       0.102881                 0.476923                 0.221757   \n",
       "17256       0.218107                 0.315385                 0.263598   \n",
       "\n",
       "       10_form_shots_against  10_form_shots_against_home_team  \\\n",
       "0                   0.000000                         0.000000   \n",
       "1                   0.000000                         0.000000   \n",
       "2                   0.000000                         0.000000   \n",
       "3                   0.000000                         0.000000   \n",
       "4                   0.000000                         0.000000   \n",
       "...                      ...                              ...   \n",
       "17252               0.253333                         0.236515   \n",
       "17253               0.208889                         0.253112   \n",
       "17254               0.160000                         0.319502   \n",
       "17255               0.275556                         0.257261   \n",
       "17256               0.182222                         0.170124   \n",
       "\n",
       "       10_form_shots_against_away_team  5_form_shots_on_target  \\\n",
       "0                             0.000000                0.000000   \n",
       "1                             0.000000                0.000000   \n",
       "2                             0.000000                0.000000   \n",
       "3                             0.000000                0.000000   \n",
       "4                             0.000000                0.000000   \n",
       "...                                ...                     ...   \n",
       "17252                         0.206751                0.214286   \n",
       "17253                         0.198312                0.196429   \n",
       "17254                         0.151899                0.410714   \n",
       "17255                         0.160338                0.142857   \n",
       "17256                         0.139241                0.392857   \n",
       "\n",
       "       5_form_shots_on_target_home_team  5_form_shots_on_target_away_team  \\\n",
       "0                              0.000000                          0.000000   \n",
       "1                              0.000000                          0.000000   \n",
       "2                              0.000000                          0.000000   \n",
       "3                              0.000000                          0.000000   \n",
       "4                              0.000000                          0.000000   \n",
       "...                                 ...                               ...   \n",
       "17252                          0.206897                          0.267857   \n",
       "17253                          0.258621                          0.196429   \n",
       "17254                          0.258621                          0.410714   \n",
       "17255                          0.137931                          0.339286   \n",
       "17256                          0.379310                          0.446429   \n",
       "\n",
       "       5_form_shots_on_target_against  \\\n",
       "0                            0.000000   \n",
       "1                            0.000000   \n",
       "2                            0.000000   \n",
       "3                            0.000000   \n",
       "4                            0.000000   \n",
       "...                               ...   \n",
       "17252                        0.519231   \n",
       "17253                        0.346154   \n",
       "17254                        0.230769   \n",
       "17255                        0.384615   \n",
       "17256                        0.269231   \n",
       "\n",
       "       5_form_shots_on_target_against_home_team  \\\n",
       "0                                      0.000000   \n",
       "1                                      0.000000   \n",
       "2                                      0.000000   \n",
       "3                                      0.000000   \n",
       "4                                      0.000000   \n",
       "...                                         ...   \n",
       "17252                                  0.519231   \n",
       "17253                                  0.538462   \n",
       "17254                                  0.461538   \n",
       "17255                                  0.384615   \n",
       "17256                                  0.269231   \n",
       "\n",
       "       5_form_shots_on_target_against_away_team  10_form_shots_on_target  \\\n",
       "0                                          0.00                 0.000000   \n",
       "1                                          0.00                 0.000000   \n",
       "2                                          0.00                 0.000000   \n",
       "3                                          0.00                 0.000000   \n",
       "4                                          0.00                 0.000000   \n",
       "...                                         ...                      ...   \n",
       "17252                                      0.38                 0.123711   \n",
       "17253                                      0.36                 0.113402   \n",
       "17254                                      0.24                 0.237113   \n",
       "17255                                      0.24                 0.082474   \n",
       "17256                                      0.22                 0.226804   \n",
       "\n",
       "       10_form_shots_on_target_home_team  10_form_shots_on_target_away_team  \\\n",
       "0                               0.000000                           0.000000   \n",
       "1                               0.000000                           0.000000   \n",
       "2                               0.000000                           0.000000   \n",
       "3                               0.000000                           0.000000   \n",
       "4                               0.000000                           0.000000   \n",
       "...                                  ...                                ...   \n",
       "17252                           0.121212                           0.148515   \n",
       "17253                           0.151515                           0.108911   \n",
       "17254                           0.151515                           0.227723   \n",
       "17255                           0.080808                           0.188119   \n",
       "17256                           0.222222                           0.247525   \n",
       "\n",
       "       10_form_shots_on_target_against  \\\n",
       "0                             0.000000   \n",
       "1                             0.000000   \n",
       "2                             0.000000   \n",
       "3                             0.000000   \n",
       "4                             0.000000   \n",
       "...                                ...   \n",
       "17252                         0.313953   \n",
       "17253                         0.209302   \n",
       "17254                         0.139535   \n",
       "17255                         0.232558   \n",
       "17256                         0.162791   \n",
       "\n",
       "       10_form_shots_on_target_against_home_team  \\\n",
       "0                                       0.000000   \n",
       "1                                       0.000000   \n",
       "2                                       0.000000   \n",
       "3                                       0.000000   \n",
       "4                                       0.000000   \n",
       "...                                          ...   \n",
       "17252                                   0.290323   \n",
       "17253                                   0.301075   \n",
       "17254                                   0.258065   \n",
       "17255                                   0.215054   \n",
       "17256                                   0.150538   \n",
       "\n",
       "       10_form_shots_on_target_against_away_team  total_yellow_cards_per_game  \\\n",
       "0                                       0.000000                     0.000000   \n",
       "1                                       0.000000                     0.000000   \n",
       "2                                       0.000000                     0.000000   \n",
       "3                                       0.000000                     0.000000   \n",
       "4                                       0.000000                     0.000000   \n",
       "...                                          ...                          ...   \n",
       "17252                                   0.211111                     0.314286   \n",
       "17253                                   0.200000                     0.628571   \n",
       "17254                                   0.133333                     0.314286   \n",
       "17255                                   0.133333                     0.493878   \n",
       "17256                                   0.122222                     0.538776   \n",
       "\n",
       "       total_yellow_cards_per_game_home_team  \\\n",
       "0                                   0.000000   \n",
       "1                                   0.000000   \n",
       "2                                   0.000000   \n",
       "3                                   0.000000   \n",
       "4                                   0.000000   \n",
       "...                                      ...   \n",
       "17252                               0.318182   \n",
       "17253                               0.378788   \n",
       "17254                               0.545455   \n",
       "17255                               0.500000   \n",
       "17256                               0.545455   \n",
       "\n",
       "       total_yellow_cards_per_game_away_team  \\\n",
       "0                                   0.000000   \n",
       "1                                   0.000000   \n",
       "2                                   0.000000   \n",
       "3                                   0.000000   \n",
       "4                                   0.000000   \n",
       "...                                      ...   \n",
       "17252                               0.359184   \n",
       "17253                               0.628571   \n",
       "17254                               0.314286   \n",
       "17255                               0.404082   \n",
       "17256                               0.448980   \n",
       "\n",
       "       total_yellow_cards_against_per_game  \\\n",
       "0                                     0.00   \n",
       "1                                     0.00   \n",
       "2                                     0.00   \n",
       "3                                     0.00   \n",
       "4                                     0.00   \n",
       "...                                    ...   \n",
       "17252                                 0.15   \n",
       "17253                                 0.25   \n",
       "17254                                 0.30   \n",
       "17255                                 0.50   \n",
       "17256                                 0.30   \n",
       "\n",
       "       total_yellow_cards_against_per_game_home_team  \\\n",
       "0                                           0.000000   \n",
       "1                                           0.000000   \n",
       "2                                           0.000000   \n",
       "3                                           0.000000   \n",
       "4                                           0.000000   \n",
       "...                                              ...   \n",
       "17252                                       0.138462   \n",
       "17253                                       0.384615   \n",
       "17254                                       0.507692   \n",
       "17255                                       0.461538   \n",
       "17256                                       0.276923   \n",
       "\n",
       "       total_yellow_cards_against_per_game_away_team  \\\n",
       "0                                               0.00   \n",
       "1                                               0.00   \n",
       "2                                               0.00   \n",
       "3                                               0.00   \n",
       "4                                               0.00   \n",
       "...                                              ...   \n",
       "17252                                           0.45   \n",
       "17253                                           0.25   \n",
       "17254                                           0.30   \n",
       "17255                                           0.60   \n",
       "17256                                           0.40   \n",
       "\n",
       "       total_red_cards_per_game  total_red_cards_per_game_home_team  \\\n",
       "0                           0.0                            0.000000   \n",
       "1                           0.0                            0.000000   \n",
       "2                           0.0                            0.000000   \n",
       "3                           0.0                            0.000000   \n",
       "4                           0.0                            0.000000   \n",
       "...                         ...                                 ...   \n",
       "17252                       0.2                            0.216667   \n",
       "17253                       0.0                            0.000000   \n",
       "17254                       0.0                            0.000000   \n",
       "17255                       0.0                            0.000000   \n",
       "17256                       0.0                            0.000000   \n",
       "\n",
       "       total_red_cards_per_game_away_team  total_red_cards_against_per_game  \\\n",
       "0                                     0.0                               0.0   \n",
       "1                                     0.0                               0.0   \n",
       "2                                     0.0                               0.0   \n",
       "3                                     0.0                               0.0   \n",
       "4                                     0.0                               0.0   \n",
       "...                                   ...                               ...   \n",
       "17252                                 0.0                               0.0   \n",
       "17253                                 0.0                               0.0   \n",
       "17254                                 0.0                               0.2   \n",
       "17255                                 0.0                               0.0   \n",
       "17256                                 0.0                               0.2   \n",
       "\n",
       "       total_red_cards_against_per_game_home_team  \\\n",
       "0                                             0.0   \n",
       "1                                             0.0   \n",
       "2                                             0.0   \n",
       "3                                             0.0   \n",
       "4                                             0.0   \n",
       "...                                           ...   \n",
       "17252                                         0.0   \n",
       "17253                                         0.0   \n",
       "17254                                         0.0   \n",
       "17255                                         0.0   \n",
       "17256                                         0.2   \n",
       "\n",
       "       total_red_cards_against_per_game_away_team  total_xg_per_game  \\\n",
       "0                                             0.0           0.000000   \n",
       "1                                             0.0           0.000000   \n",
       "2                                             0.0           0.000000   \n",
       "3                                             0.0           0.000000   \n",
       "4                                             0.0           0.000000   \n",
       "...                                           ...                ...   \n",
       "17252                                         0.0           0.298667   \n",
       "17253                                         0.0           0.000000   \n",
       "17254                                         0.2           0.000000   \n",
       "17255                                         0.0           0.106667   \n",
       "17256                                         0.0           0.554667   \n",
       "\n",
       "       total_xg_per_game_home_team  total_xg_per_game_away_team  \\\n",
       "0                         0.000000                     0.000000   \n",
       "1                         0.000000                     0.000000   \n",
       "2                         0.000000                     0.000000   \n",
       "3                         0.000000                     0.000000   \n",
       "4                         0.000000                     0.000000   \n",
       "...                            ...                          ...   \n",
       "17252                     0.305455                     0.391111   \n",
       "17253                     0.000000                     0.000000   \n",
       "17254                     0.000000                     0.000000   \n",
       "17255                     0.109091                     0.476444   \n",
       "17256                     0.567273                     0.391111   \n",
       "\n",
       "       total_xg_against_per_game  total_xg_against_per_game_home_team  \\\n",
       "0                       0.000000                             0.000000   \n",
       "1                       0.000000                             0.000000   \n",
       "2                       0.000000                             0.000000   \n",
       "3                       0.000000                             0.000000   \n",
       "4                       0.000000                             0.000000   \n",
       "...                          ...                                  ...   \n",
       "17252                   0.685321                             0.614815   \n",
       "17253                   0.000000                             0.000000   \n",
       "17254                   0.000000                             0.000000   \n",
       "17255                   0.693578                             0.622222   \n",
       "17256                   0.445872                             0.400000   \n",
       "\n",
       "       total_xg_against_per_game_away_team  5_form_xg  5_form_xg_home_team  \\\n",
       "0                                    0.000   0.000000             0.000000   \n",
       "1                                    0.000   0.000000             0.000000   \n",
       "2                                    0.000   0.000000             0.000000   \n",
       "3                                    0.000   0.000000             0.000000   \n",
       "4                                    0.000   0.000000             0.000000   \n",
       "...                                    ...        ...                  ...   \n",
       "17252                                0.550   0.206897             0.206897   \n",
       "17253                                0.000   0.000000             0.000000   \n",
       "17254                                0.000   0.000000             0.000000   \n",
       "17255                                0.375   0.073892             0.073892   \n",
       "17256                                0.375   0.384236             0.384236   \n",
       "\n",
       "       5_form_xg_away_team  5_form_xg_against  5_form_xg_against_home_team  \\\n",
       "0                 0.000000           0.000000                     0.000000   \n",
       "1                 0.000000           0.000000                     0.000000   \n",
       "2                 0.000000           0.000000                     0.000000   \n",
       "3                 0.000000           0.000000                     0.000000   \n",
       "4                 0.000000           0.000000                     0.000000   \n",
       "...                    ...                ...                          ...   \n",
       "17252             0.284974           0.479769                     0.509202   \n",
       "17253             0.000000           0.000000                     0.000000   \n",
       "17254             0.000000           0.000000                     0.000000   \n",
       "17255             0.347150           0.485549                     0.515337   \n",
       "17256             0.284974           0.312139                     0.331288   \n",
       "\n",
       "       5_form_xg_against_away_team  10_form_xg  10_form_xg_home_team  \\\n",
       "0                         0.000000    0.000000              0.000000   \n",
       "1                         0.000000    0.000000              0.000000   \n",
       "2                         0.000000    0.000000              0.000000   \n",
       "3                         0.000000    0.000000              0.000000   \n",
       "4                         0.000000    0.000000              0.000000   \n",
       "...                            ...         ...                   ...   \n",
       "17252                     0.381503    0.124629              0.128049   \n",
       "17253                     0.000000    0.000000              0.000000   \n",
       "17254                     0.000000    0.000000              0.000000   \n",
       "17255                     0.260116    0.044510              0.045732   \n",
       "17256                     0.260116    0.231454              0.237805   \n",
       "\n",
       "       10_form_xg_away_team  10_form_xg_against  10_form_xg_against_home_team  \\\n",
       "0                  0.000000            0.000000                      0.000000   \n",
       "1                  0.000000            0.000000                      0.000000   \n",
       "2                  0.000000            0.000000                      0.000000   \n",
       "3                  0.000000            0.000000                      0.000000   \n",
       "4                  0.000000            0.000000                      0.000000   \n",
       "...                     ...                 ...                           ...   \n",
       "17252              0.163205            0.309701                      0.291228   \n",
       "17253              0.000000            0.000000                      0.000000   \n",
       "17254              0.000000            0.000000                      0.000000   \n",
       "17255              0.198813            0.313433                      0.294737   \n",
       "17256              0.163205            0.201493                      0.189474   \n",
       "\n",
       "       10_form_xg_against_away_team  total_points_away  total_points_home  \\\n",
       "0                          0.000000           0.000000           0.000000   \n",
       "1                          0.000000           0.000000           0.000000   \n",
       "2                          0.000000           0.000000           0.000000   \n",
       "3                          0.000000           0.000000           0.000000   \n",
       "4                          0.000000           0.000000           0.000000   \n",
       "...                             ...                ...                ...   \n",
       "17252                      0.230769           0.020833           0.000000   \n",
       "17253                      0.000000           0.062500           0.017857   \n",
       "17254                      0.000000           0.000000           0.071429   \n",
       "17255                      0.157343           0.125000           0.017857   \n",
       "17256                      0.157343           0.020833           0.053571   \n",
       "\n",
       "       5_form_away  10_form_away  5_form_home  10_form_home  total_goals_away  \\\n",
       "0         0.000000      0.000000     0.000000      0.000000              0.00   \n",
       "1         0.000000      0.000000     0.000000      0.000000              0.00   \n",
       "2         0.000000      0.000000     0.000000      0.000000              0.00   \n",
       "3         0.000000      0.000000     0.000000      0.000000              0.00   \n",
       "4         0.000000      0.000000     0.000000      0.000000              0.00   \n",
       "...            ...           ...          ...           ...               ...   \n",
       "17252     0.066667      0.033333     0.000000      0.000000              0.04   \n",
       "17253     0.200000      0.100000     0.066667      0.033333              0.06   \n",
       "17254     0.000000      0.000000     0.266667      0.133333              0.02   \n",
       "17255     0.400000      0.200000     0.066667      0.033333              0.08   \n",
       "17256     0.066667      0.033333     0.200000      0.100000              0.04   \n",
       "\n",
       "       total_goals_against_away  total_goals_per_game_away  \\\n",
       "0                      0.000000                   0.000000   \n",
       "1                      0.000000                   0.000000   \n",
       "2                      0.000000                   0.000000   \n",
       "3                      0.000000                   0.000000   \n",
       "4                      0.000000                   0.000000   \n",
       "...                         ...                        ...   \n",
       "17252                  0.050847                   0.153846   \n",
       "17253                  0.033898                   0.230769   \n",
       "17254                  0.033898                   0.115385   \n",
       "17255                  0.033898                   0.307692   \n",
       "17256                  0.050847                   0.153846   \n",
       "\n",
       "       total_goals_against_per_game_away  total_goals_home  \\\n",
       "0                               0.000000          0.000000   \n",
       "1                               0.000000          0.000000   \n",
       "2                               0.000000          0.000000   \n",
       "3                               0.000000          0.000000   \n",
       "4                               0.000000          0.000000   \n",
       "...                                  ...               ...   \n",
       "17252                           0.230769          0.033898   \n",
       "17253                           0.153846          0.033898   \n",
       "17254                           0.230769          0.033898   \n",
       "17255                           0.153846          0.016949   \n",
       "17256                           0.230769          0.016949   \n",
       "\n",
       "       total_goals_against_home  total_goals_per_game_home  \\\n",
       "0                      0.000000                   0.000000   \n",
       "1                      0.000000                   0.000000   \n",
       "2                      0.000000                   0.000000   \n",
       "3                      0.000000                   0.000000   \n",
       "4                      0.000000                   0.000000   \n",
       "...                         ...                        ...   \n",
       "17252                  0.111111                   0.156863   \n",
       "17253                  0.037037                   0.235294   \n",
       "17254                  0.018519                   0.156863   \n",
       "17255                  0.055556                   0.078431   \n",
       "17256                  0.055556                   0.078431   \n",
       "\n",
       "       total_goals_against_per_game_home  5_form_goals_scored_away  \\\n",
       "0                               0.000000                      0.00   \n",
       "1                               0.000000                      0.00   \n",
       "2                               0.000000                      0.00   \n",
       "3                               0.000000                      0.00   \n",
       "4                               0.000000                      0.00   \n",
       "...                                  ...                       ...   \n",
       "17252                           0.571429                      0.10   \n",
       "17253                           0.285714                      0.15   \n",
       "17254                           0.095238                      0.05   \n",
       "17255                           0.285714                      0.20   \n",
       "17256                           0.285714                      0.10   \n",
       "\n",
       "       10_form_goals_scored_away  5_form_goals_against_away  \\\n",
       "0                       0.000000                   0.000000   \n",
       "1                       0.000000                   0.000000   \n",
       "2                       0.000000                   0.000000   \n",
       "3                       0.000000                   0.000000   \n",
       "4                       0.000000                   0.000000   \n",
       "...                          ...                        ...   \n",
       "17252                   0.055556                   0.130435   \n",
       "17253                   0.083333                   0.086957   \n",
       "17254                   0.027778                   0.086957   \n",
       "17255                   0.111111                   0.086957   \n",
       "17256                   0.055556                   0.130435   \n",
       "\n",
       "       10_form_goals_against_away  5_form_goals_scored_home  \\\n",
       "0                        0.000000                      0.00   \n",
       "1                        0.000000                      0.00   \n",
       "2                        0.000000                      0.00   \n",
       "3                        0.000000                      0.00   \n",
       "4                        0.000000                      0.00   \n",
       "...                           ...                       ...   \n",
       "17252                    0.083333                      0.08   \n",
       "17253                    0.055556                      0.08   \n",
       "17254                    0.055556                      0.08   \n",
       "17255                    0.055556                      0.04   \n",
       "17256                    0.083333                      0.04   \n",
       "\n",
       "       10_form_goals_scored_home  5_form_goals_against_home  \\\n",
       "0                        0.00000                   0.000000   \n",
       "1                        0.00000                   0.000000   \n",
       "2                        0.00000                   0.000000   \n",
       "3                        0.00000                   0.000000   \n",
       "4                        0.00000                   0.000000   \n",
       "...                          ...                        ...   \n",
       "17252                    0.04878                   0.272727   \n",
       "17253                    0.04878                   0.090909   \n",
       "17254                    0.04878                   0.045455   \n",
       "17255                    0.02439                   0.136364   \n",
       "17256                    0.02439                   0.136364   \n",
       "\n",
       "       10_form_goals_against_home  total_shots_per_game_home  \\\n",
       "0                        0.000000                   0.000000   \n",
       "1                        0.000000                   0.000000   \n",
       "2                        0.000000                   0.000000   \n",
       "3                        0.000000                   0.000000   \n",
       "4                        0.000000                   0.000000   \n",
       "...                           ...                        ...   \n",
       "17252                    0.181818                   0.386364   \n",
       "17253                    0.060606                   0.300505   \n",
       "17254                    0.030303                   0.257576   \n",
       "17255                    0.090909                   0.257576   \n",
       "17256                    0.090909                   0.314815   \n",
       "\n",
       "       total_shots_against_per_game_home  total_shots_per_game_away  \\\n",
       "0                               0.000000                   0.000000   \n",
       "1                               0.000000                   0.000000   \n",
       "2                               0.000000                   0.000000   \n",
       "3                               0.000000                   0.000000   \n",
       "4                               0.000000                   0.000000   \n",
       "...                                  ...                        ...   \n",
       "17252                           0.489130                   0.387205   \n",
       "17253                           0.380435                   0.286195   \n",
       "17254                           0.797101                   0.429293   \n",
       "17255                           0.489130                   0.420875   \n",
       "17256                           0.380435                   0.589226   \n",
       "\n",
       "       total_shots_against_per_game_away  total_shots_on_target_per_game_home  \\\n",
       "0                                  0.000                             0.000000   \n",
       "1                                  0.000                             0.000000   \n",
       "2                                  0.000                             0.000000   \n",
       "3                                  0.000                             0.000000   \n",
       "4                                  0.000                             0.000000   \n",
       "...                                  ...                                  ...   \n",
       "17252                              0.330                             0.307692   \n",
       "17253                              0.375                             0.205128   \n",
       "17254                              0.180                             0.205128   \n",
       "17255                              0.345                             0.205128   \n",
       "17256                              0.240                             0.273504   \n",
       "\n",
       "       total_shots_on_target_against_per_game_home  \\\n",
       "0                                         0.000000   \n",
       "1                                         0.000000   \n",
       "2                                         0.000000   \n",
       "3                                         0.000000   \n",
       "4                                         0.000000   \n",
       "...                                            ...   \n",
       "17252                                     0.526316   \n",
       "17253                                     0.263158   \n",
       "17254                                     0.614035   \n",
       "17255                                     0.394737   \n",
       "17256                                     0.219298   \n",
       "\n",
       "       total_shots_on_target_per_game_away  \\\n",
       "0                                 0.000000   \n",
       "1                                 0.000000   \n",
       "2                                 0.000000   \n",
       "3                                 0.000000   \n",
       "4                                 0.000000   \n",
       "...                                    ...   \n",
       "17252                             0.358209   \n",
       "17253                             0.199005   \n",
       "17254                             0.298507   \n",
       "17255                             0.318408   \n",
       "17256                             0.517413   \n",
       "\n",
       "       total_shots_on_target_against_per_game_away  5_form_shots_away  \\\n",
       "0                                         0.000000           0.000000   \n",
       "1                                         0.000000           0.000000   \n",
       "2                                         0.000000           0.000000   \n",
       "3                                         0.000000           0.000000   \n",
       "4                                         0.000000           0.000000   \n",
       "...                                            ...                ...   \n",
       "17252                                     0.444444           0.190083   \n",
       "17253                                     0.296296           0.140496   \n",
       "17254                                     0.166667           0.140496   \n",
       "17255                                     0.222222           0.206612   \n",
       "17256                                     0.222222           0.289256   \n",
       "\n",
       "       10_form_shots_away  5_form_shots_against_away  \\\n",
       "0                0.000000                   0.000000   \n",
       "1                0.000000                   0.000000   \n",
       "2                0.000000                   0.000000   \n",
       "3                0.000000                   0.000000   \n",
       "4                0.000000                   0.000000   \n",
       "...                   ...                        ...   \n",
       "17252            0.108491                   0.158273   \n",
       "17253            0.080189                   0.179856   \n",
       "17254            0.080189                   0.057554   \n",
       "17255            0.117925                   0.165468   \n",
       "17256            0.165094                   0.115108   \n",
       "\n",
       "       10_form_shots_against_away  5_form_shots_home  10_form_shots_home  \\\n",
       "0                        0.000000           0.000000            0.000000   \n",
       "1                        0.000000           0.000000            0.000000   \n",
       "2                        0.000000           0.000000            0.000000   \n",
       "3                        0.000000           0.000000            0.000000   \n",
       "4                        0.000000           0.000000            0.000000   \n",
       "...                           ...                ...                 ...   \n",
       "17252                    0.092437           0.198529            0.101124   \n",
       "17253                    0.105042           0.102941            0.052434   \n",
       "17254                    0.033613           0.132353            0.067416   \n",
       "17255                    0.096639           0.132353            0.067416   \n",
       "17256                    0.067227           0.161765            0.082397   \n",
       "\n",
       "       5_form_shots_against_home  10_form_shots_against_home  \\\n",
       "0                       0.000000                    0.000000   \n",
       "1                       0.000000                    0.000000   \n",
       "2                       0.000000                    0.000000   \n",
       "3                       0.000000                    0.000000   \n",
       "4                       0.000000                    0.000000   \n",
       "...                          ...                         ...   \n",
       "17252                   0.243243                    0.139896   \n",
       "17253                   0.126126                    0.072539   \n",
       "17254                   0.396396                    0.227979   \n",
       "17255                   0.243243                    0.139896   \n",
       "17256                   0.189189                    0.108808   \n",
       "\n",
       "       5_form_shots_on_target_away  10_form_shots_on_target_away  \\\n",
       "0                         0.000000                      0.000000   \n",
       "1                         0.000000                      0.000000   \n",
       "2                         0.000000                      0.000000   \n",
       "3                         0.000000                      0.000000   \n",
       "4                         0.000000                      0.000000   \n",
       "...                            ...                           ...   \n",
       "17252                     0.155172                      0.094737   \n",
       "17253                     0.086207                      0.052632   \n",
       "17254                     0.086207                      0.052632   \n",
       "17255                     0.137931                      0.084211   \n",
       "17256                     0.224138                      0.136842   \n",
       "\n",
       "       5_form_shots_on_target_against_away  \\\n",
       "0                                 0.000000   \n",
       "1                                 0.000000   \n",
       "2                                 0.000000   \n",
       "3                                 0.000000   \n",
       "4                                 0.000000   \n",
       "...                                    ...   \n",
       "17252                             0.230769   \n",
       "17253                             0.153846   \n",
       "17254                             0.057692   \n",
       "17255                             0.115385   \n",
       "17256                             0.115385   \n",
       "\n",
       "       10_form_shots_on_target_against_away  5_form_shots_on_target_home  \\\n",
       "0                                  0.000000                     0.000000   \n",
       "1                                  0.000000                     0.000000   \n",
       "2                                  0.000000                     0.000000   \n",
       "3                                  0.000000                     0.000000   \n",
       "4                                  0.000000                     0.000000   \n",
       "...                                     ...                          ...   \n",
       "17252                              0.130435                     0.157895   \n",
       "17253                              0.086957                     0.070175   \n",
       "17254                              0.032609                     0.105263   \n",
       "17255                              0.065217                     0.105263   \n",
       "17256                              0.065217                     0.140351   \n",
       "\n",
       "       10_form_shots_on_target_home  5_form_shots_on_target_against_home  \\\n",
       "0                          0.000000                                 0.00   \n",
       "1                          0.000000                                 0.00   \n",
       "2                          0.000000                                 0.00   \n",
       "3                          0.000000                                 0.00   \n",
       "4                          0.000000                                 0.00   \n",
       "...                             ...                                  ...   \n",
       "17252                      0.088235                                 0.24   \n",
       "17253                      0.039216                                 0.08   \n",
       "17254                      0.058824                                 0.28   \n",
       "17255                      0.058824                                 0.18   \n",
       "17256                      0.078431                                 0.10   \n",
       "\n",
       "       10_form_shots_on_target_against_home  total_yellow_cards_per_game_home  \\\n",
       "0                                  0.000000                          0.000000   \n",
       "1                                  0.000000                          0.000000   \n",
       "2                                  0.000000                          0.000000   \n",
       "3                                  0.000000                          0.000000   \n",
       "4                                  0.000000                          0.000000   \n",
       "...                                     ...                               ...   \n",
       "17252                              0.144578                          0.241379   \n",
       "17253                              0.048193                          0.241379   \n",
       "17254                              0.168675                          0.321839   \n",
       "17255                              0.108434                          0.402299   \n",
       "17256                              0.060241                          0.482759   \n",
       "\n",
       "       total_yellow_cards_against_per_game_home  \\\n",
       "0                                      0.000000   \n",
       "1                                      0.000000   \n",
       "2                                      0.000000   \n",
       "3                                      0.000000   \n",
       "4                                      0.000000   \n",
       "...                                         ...   \n",
       "17252                                  0.166667   \n",
       "17253                                  0.125000   \n",
       "17254                                  0.583333   \n",
       "17255                                  0.333333   \n",
       "17256                                  0.333333   \n",
       "\n",
       "       total_yellow_cards_per_game_away  \\\n",
       "0                              0.000000   \n",
       "1                              0.000000   \n",
       "2                              0.000000   \n",
       "3                              0.000000   \n",
       "4                              0.000000   \n",
       "...                                 ...   \n",
       "17252                          0.518519   \n",
       "17253                          0.592593   \n",
       "17254                          0.000000   \n",
       "17255                          0.518519   \n",
       "17256                          0.370370   \n",
       "\n",
       "       total_yellow_cards_against_per_game_away  \\\n",
       "0                                      0.000000   \n",
       "1                                      0.000000   \n",
       "2                                      0.000000   \n",
       "3                                      0.000000   \n",
       "4                                      0.000000   \n",
       "...                                         ...   \n",
       "17252                                  0.416667   \n",
       "17253                                  0.333333   \n",
       "17254                                  0.000000   \n",
       "17255                                  0.416667   \n",
       "17256                                  0.416667   \n",
       "\n",
       "       total_red_cards_per_game_home  total_red_cards_against_per_game_home  \\\n",
       "0                           0.000000                                    0.0   \n",
       "1                           0.000000                                    0.0   \n",
       "2                           0.000000                                    0.0   \n",
       "3                           0.000000                                    0.0   \n",
       "4                           0.000000                                    0.0   \n",
       "...                              ...                                    ...   \n",
       "17252                       0.333333                                    0.0   \n",
       "17253                       0.000000                                    0.0   \n",
       "17254                       0.000000                                    0.0   \n",
       "17255                       0.000000                                    0.0   \n",
       "17256                       0.000000                                    0.0   \n",
       "\n",
       "       total_red_cards_per_game_away  total_red_cards_against_per_game_away  \\\n",
       "0                                0.0                                    0.0   \n",
       "1                                0.0                                    0.0   \n",
       "2                                0.0                                    0.0   \n",
       "3                                0.0                                    0.0   \n",
       "4                                0.0                                    0.0   \n",
       "...                              ...                                    ...   \n",
       "17252                            0.0                                    0.0   \n",
       "17253                            0.0                                    0.0   \n",
       "17254                            0.0                                    0.5   \n",
       "17255                            0.0                                    0.0   \n",
       "17256                            0.0                                    0.0   \n",
       "\n",
       "       total_xg_per_game_home  total_xg_against_per_game_home  \\\n",
       "0                    0.000000                        0.000000   \n",
       "1                    0.000000                        0.000000   \n",
       "2                    0.000000                        0.000000   \n",
       "3                    0.000000                        0.000000   \n",
       "4                    0.000000                        0.000000   \n",
       "...                       ...                             ...   \n",
       "17252                0.259887                        0.535211   \n",
       "17253                0.203390                        0.253521   \n",
       "17254                0.000000                        0.000000   \n",
       "17255                0.101695                        0.492958   \n",
       "17256                0.429379                        0.309859   \n",
       "\n",
       "       total_xg_per_game_away  total_xg_against_per_game_away  5_form_xg_away  \\\n",
       "0                    0.000000                        0.000000        0.000000   \n",
       "1                    0.000000                        0.000000        0.000000   \n",
       "2                    0.000000                        0.000000        0.000000   \n",
       "3                    0.000000                        0.000000        0.000000   \n",
       "4                    0.000000                        0.000000        0.000000   \n",
       "...                       ...                             ...             ...   \n",
       "17252                0.207679                        0.493827        0.098837   \n",
       "17253                0.000000                        0.000000        0.000000   \n",
       "17254                0.000000                        0.000000        0.000000   \n",
       "17255                0.415358                        0.333333        0.197674   \n",
       "17256                0.293194                        0.345679        0.139535   \n",
       "\n",
       "       10_form_xg_away  5_form_xg_against_away  10_form_xg_against_away  \\\n",
       "0             0.000000                0.000000                 0.000000   \n",
       "1             0.000000                0.000000                 0.000000   \n",
       "2             0.000000                0.000000                 0.000000   \n",
       "3             0.000000                0.000000                 0.000000   \n",
       "4             0.000000                0.000000                 0.000000   \n",
       "...                ...                     ...                      ...   \n",
       "17252         0.054662                0.223464                 0.140351   \n",
       "17253         0.000000                0.000000                 0.000000   \n",
       "17254         0.000000                0.000000                 0.000000   \n",
       "17255         0.109325                0.150838                 0.094737   \n",
       "17256         0.077170                0.156425                 0.098246   \n",
       "\n",
       "       5_form_xg_home  10_form_xg_home  5_form_xg_against_home  \\\n",
       "0            0.000000         0.000000                0.000000   \n",
       "1            0.000000         0.000000                0.000000   \n",
       "2            0.000000         0.000000                0.000000   \n",
       "3            0.000000         0.000000                0.000000   \n",
       "4            0.000000         0.000000                0.000000   \n",
       "...               ...              ...                     ...   \n",
       "17252        0.122995         0.073016                0.230303   \n",
       "17253        0.064171         0.038095                0.072727   \n",
       "17254        0.000000         0.000000                0.000000   \n",
       "17255        0.048128         0.028571                0.212121   \n",
       "17256        0.203209         0.120635                0.133333   \n",
       "\n",
       "       10_form_xg_against_home  home_win_odds  draw_odds  away_win_odds    HC  \\\n",
       "0                     0.000000       0.696570   0.439803       0.166090  0.45   \n",
       "1                     0.000000       0.438064   0.609870       0.343866  0.40   \n",
       "2                     0.000000       0.180271   0.477044       0.688533  0.15   \n",
       "3                     0.000000       0.646363   0.505708       0.182422  0.65   \n",
       "4                     0.000000       0.615690   0.532923       0.199622  0.30   \n",
       "...                        ...            ...        ...            ...   ...   \n",
       "17252                 0.154472       0.411667   0.572953       0.392063  0.20   \n",
       "17253                 0.048780       0.460946   0.598374       0.326161  0.45   \n",
       "17254                 0.000000       0.373724   0.630050       0.400371  0.45   \n",
       "17255                 0.142276       0.265519   0.547244       0.560013  0.20   \n",
       "17256                 0.089431       0.432667   0.519665       0.399467  0.20   \n",
       "\n",
       "             AC  Home_h2h_Goals  Home_h2h_Points  Away_h2h_Goals  \\\n",
       "0      0.210526        0.000000         0.000000        0.000000   \n",
       "1      0.105263        0.000000         0.000000        0.000000   \n",
       "2      0.157895        0.000000         0.000000        0.000000   \n",
       "3      0.000000        0.000000         0.000000        0.000000   \n",
       "4      0.368421        0.000000         0.000000        0.000000   \n",
       "...         ...             ...              ...             ...   \n",
       "17252  0.368421        0.285714         0.642857        0.125000   \n",
       "17253  0.315789        0.000000         0.000000        0.000000   \n",
       "17254  0.315789        0.152778         0.305556        0.187500   \n",
       "17255  0.368421        0.166667         0.166667        0.187500   \n",
       "17256  0.368421        0.250000         0.571429        0.196429   \n",
       "\n",
       "       Away_h2h_Points  home_elo  away_elo  Div_D1  Div_D2  Div_E0  Div_E1  \\\n",
       "0             0.000000  0.691680  0.521222   False   False    True   False   \n",
       "1             0.000000  0.430666  0.440082   False   False    True   False   \n",
       "2             0.000000  0.377394  0.675734   False   False    True   False   \n",
       "3             0.000000  0.490968  0.434453   False   False    True   False   \n",
       "4             0.000000  0.567330  0.451446   False   False    True   False   \n",
       "...                ...       ...       ...     ...     ...     ...     ...   \n",
       "17252         0.214286  0.466496  0.552350   False   False    True   False   \n",
       "17253         0.000000  0.293262  0.449534   False   False   False   False   \n",
       "17254         0.555556  0.570864  0.594489   False   False   False   False   \n",
       "17255         0.666667  0.330767  0.599204   False   False    True   False   \n",
       "17256         0.357143  0.586382  0.615623   False   False    True   False   \n",
       "\n",
       "       Div_I1  Div_SP1  Div_SP2  \n",
       "0       False    False    False  \n",
       "1       False    False    False  \n",
       "2       False    False    False  \n",
       "3       False    False    False  \n",
       "4       False    False    False  \n",
       "...       ...      ...      ...  \n",
       "17252   False    False    False  \n",
       "17253    True    False    False  \n",
       "17254    True    False    False  \n",
       "17255   False    False    False  \n",
       "17256   False    False    False  \n",
       "\n",
       "[17257 rows x 184 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7981bc-cd88-498c-99e7-694352651f04",
   "metadata": {},
   "source": [
    "Now, we need to divide the dataset into training, validation, and test sets. This is a critical step in the modeling process to ensure that our model is trained effectively and evaluated properly. In this case, the split is done manually, with a specific strategy in mind.\r\n",
    "\r\n",
    "### Data Splitting Strategy\r\n",
    "\r\n",
    "Instead of using a random split, I opted for a chronological split of the data. The reasoning behind this approach is to leave the most recent observations as the test set, mimicking real-world conditions where we use the model to predict upcoming matches. This simulates the process of applying the model to live betting scenarios, where the latest games are the ones we aim to predict.\r\n",
    "\r\n",
    "- **Training Set**: This portion of the data contains the bulk of historical matches and is used to train the model. By training on a wide range of past games, we allow the model to learn from various situations and patterns.\r\n",
    "  \r\n",
    "- **Validation Set**: A smaller, intermediate set that allows us to tune hyperparameters and evaluate the model’s performance during the training process. This helps us avoid overfitting by ensuring that the model generalizes well to unseen data during training.\r\n",
    "  \r\n",
    "- **Test Set**: The most recent matches are set aside as the final test set. These observations will be used as a final evaluation of the model before deploying it in a real betting scenario. By doing this, we ensure that the test set reflects the same conditions the model will face when making predictions in live matches.\r\n",
    "\r\n",
    "This manual approach to splitting the data ensures that we are working with a realistic time-series setup, where past data is used to predict future outcomes. It also prepares the model for its ultimate goal: performing well on unseen matches.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43548f84-8067-43d5-996e-69bde190cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[:-1000]\n",
    "y_train = target[:-1000]\n",
    "X_val = df[-1000:-550]\n",
    "y_val = target[-1000:-550]\n",
    "X_test = df[-550:-40]\n",
    "y_test = target[-550:-40]\n",
    "X_to_pred = df[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965fcdc-54dc-4be3-bb32-536ef54d343a",
   "metadata": {},
   "source": [
    "Now, we will construct the Deep Neural Network (DNN) architecture for our model. The primary objective is to predict the number of goals scored by both teams, so we need to design a neural network that can effectively learn from the input data and generalize well to new, unseen matches.\r\n",
    "\r\n",
    "### DNN Architecture\r\n",
    "\r\n",
    "1. **Input and Output Layers**: \r\n",
    "   We begin by defining the input and output layers, ensuring that their sizes are compatible with our training data and target data. The input layer will have a size equal to the number of features in our dataset, while the output layer will have a size of 2 (for the goals scored by the home and away teams).\r\n",
    "\r\n",
    "2. **Hidden Layers**:\r\n",
    "   To enhance the learning capability of the model, we add two hidden layers. These hidden layers enable the network to capture more complex patterns and interactions in the data. A typical approach is to use fully connected (dense) layers, where each neuron is connected to all neurons in the previous layer.\r\n",
    "   \r\n",
    "   The activation function used in these layers will be **ReLU (Rectified Linear Unit)**, which introduces non-linearity to the model and helps it learn complex relationships between the features.\r\n",
    "\r\n",
    "3. **Dropout Layer**:\r\n",
    "   To prevent overfitting, we include a **dropout layer**, which randomly deactivates a fraction of neurons during training. This regularization technique encourages the model to be more general and not rely too heavily on specific neurons, improving its robustness.\r\n",
    "\r\n",
    "4. **Training Hyperparameters**:\r\n",
    "   We will manually set the learning rate, batch size, and number of epochs to control the training process:\r\n",
    "   - **Learning Rate**: Controls how quickly the model updates its weights during training.\r\n",
    "   - **Batch Size**: The number of training examples processed in one iteration before updating the model weights.\r\n",
    "   - **Epochs**: The number of times the entire training dataset is passed\n",
    "     through the model.\r\n",
    "\r\n",
    "5. **Early Stopping**:\r\n",
    "   To avoid overfitting, we will implement an **early stopping** mechanism. This technique monitors the model’s performance on the validation set and stops training when the model's performance stops improving. This ensures that the model doesn’t continue to train unnecessarily, which could lead to overfitting.\r\n",
    "\r\n",
    "### Evaluation Metrics\r\n",
    "\r\n",
    "We will evaluate the model using two metrics:\r\n",
    "- **Mean Squared Error (MSE)**: This metric measures the average squared difference between the predicted and actual values. While useful, it can be sensitive to outliers, which is why we focus primarily on the next metric.\r\n",
    "- **Mean Absolute Error (MAE)**: MAE gives us the mean absolute difference between the predicted and actual goals, making it more interpretable for our specific problem. This metric directly shows how far off our predictions are from the actual number of goals scored, which is crucial for real-world betting applications.\r\n",
    "\r\n",
    "By optimizing the model based on MAE, we will be able to better assess the average error in our predictions for goals scored, providing more meaningful insights into the model’s accuracy.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44a8ba-8b41-4d5b-906f-c2ef028cd734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model = Sequential()\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu', input_shape=(199,)))  # 278 is the number of input features\n",
    "model.add(Dropout(0.1)) \n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "# model.add(Dropout(0.1))  # Optional, to prevent overfitting\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=500, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "results = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00960073-5b1f-4b7f-a170-dceab5277abd",
   "metadata": {},
   "source": [
    "For our initial deep learning model, we obtained a mean **Mean Absolute Error (MAE)** of about 1.2 on the test set. While this result is a good starting point, it leaves room for improvement. To enhance the model's performance, we will incorporate a **hyperparameter optimization tool** provided by Keras. This tool will allow us to fine-tune various aspects of the model by searching for the optimal values of key hyperparameters.\r\n",
    "\r\n",
    "### Hyperparameter Optimization with Keras\r\n",
    "\r\n",
    "Keras provides a powerful tool for hyperparameter tuning, which can automatically search through a range of possible values for different parameters. This optimization process is essential because the performance of a neural network is heavily influenced by the specific configuration of its hyperparameters.\r\n",
    "\r\n",
    "Instead of manually adjusting the parameters through trial and error, we will use a **random search algorithm** to systematically explore different combinations of hyperparameters. By leveraging this tool, we aim to significantly improve the model’s performance, especially in terms of reducing the prediction error (MAE).\r\n",
    "\r\n",
    "### Hyperparameters to Tune\r\n",
    "\r\n",
    "The following hyperparameters will be optimized during the random search process:\r\n",
    "1. **Number of Hidden Layers**: The number of hidden layers in the network plays a crucial role in determining the depth of the model and its ability to capture complex patterns. The optimal number of hidden layers will be determined through the search process.\r\n",
    "  \r\n",
    "2. **Number of Perceptrons (Neurons) in Each Layer**: For each hidden layer, the number of perceptrons (neurons) can vary. Too few neurons may lead to underfitting, while too many neurons may result in overfitting. Finding the right balance is key to improving the model's generalization.\r\n",
    "\r\n",
    "3. **Learning Rate**: The learning rate controls the step size during the model’s optimization process. A learning rate that is too high can cause the model to converge too quickly and miss optimal solutions, while a rate that is too low can lead to slow training. We will search for the optimal learning rate to speed up convergence without sacrificing accuracy.\r\n",
    "\r\n",
    "4. **Dropout Rate**: Dropout is a regularization technique used to prevent overfitting by randomly deactivating a percentage of neurons during training. We will tune the dropout rate to find the best balance between model complexity and generalization.\r\n",
    "\r\n",
    "5. **Activation Function**: Different activation functions can be used in hidden layers to introduce non-linearity. The choice of activation function (e.g., ReLU, Sigmoid, Tanh) can significantly impact how well the model learns complex relationships in the data. The random search will explore various options.\r\n",
    "\r\n",
    "6. **Number of Epochs**: The number of epochs determines how many times the model will pass through the entire training dataset. Too few epochs might result in underfitting, while too many might lead to overfitting. We will tune this parameter to find the optimal number of epochs.\r\n",
    "\r\n",
    "7. **Batch Size**: The batch size specifies how many samples are processed before updating the model’s weights. Smaller batch sizes provide more frequent updates, while larger ones can speed up training but may miss finer details. Tuning this parameter will help improve model efficiency and performance.\r\n",
    "\r\n",
    "### Random Search Algorithm\r\n",
    "\r\n",
    "The random search algorithm will iteratively try different combinations of the hyperparameters listed above, and evaluate each combination based on the **validation set** performance (e.g., using MAE as the metric). This process will help us identify the most effective model configuration without the need for exhaustive grid searches, which can be computationally expensive.\r\n",
    "\r\n",
    "### Goal\r\n",
    "\r\n",
    "By utilizing hyperparameter optimization, we aim to improve the model’s mean MAE on the test set, bringing it below the current value of 1.2. Optimizing the model in this way will allow us to extract more meaningful patterns from the data and make more accurate predictions, which is especially important when forecasting unpredictable events like football matches.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1659,
   "id": "8ad36d62-82ea-4f6f-afeb-dfcf38604993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from hyperparameter_search3\\regression_tuning3\\tuner0.json\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ciesl\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - loss: 8.0943 - mae: 2.1976 - val_loss: 2.9380 - val_mae: 1.2781\n",
      "Epoch 2/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.7098 - mae: 1.2351 - val_loss: 1.5796 - val_mae: 0.9851\n",
      "Epoch 3/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.6801 - mae: 1.0029 - val_loss: 2.3395 - val_mae: 1.3085\n",
      "Epoch 4/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.0078 - mae: 1.1653 - val_loss: 1.7934 - val_mae: 1.1306\n",
      "Epoch 5/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.6364 - mae: 1.0315 - val_loss: 1.4737 - val_mae: 0.9476\n",
      "Epoch 6/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.5731 - mae: 0.9558 - val_loss: 1.5179 - val_mae: 0.9498\n",
      "Epoch 7/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.6344 - mae: 0.9637 - val_loss: 1.5110 - val_mae: 0.9606\n",
      "Epoch 8/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.5711 - mae: 0.9574 - val_loss: 1.4488 - val_mae: 0.9670\n",
      "Epoch 9/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.4874 - mae: 0.9516 - val_loss: 1.5757 - val_mae: 1.0516\n",
      "Epoch 10/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.5184 - mae: 0.9985 - val_loss: 1.5494 - val_mae: 1.0420\n",
      "Epoch 11/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.4819 - mae: 0.9829 - val_loss: 1.4470 - val_mae: 0.9818\n",
      "Epoch 12/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.4303 - mae: 0.9414 - val_loss: 1.4321 - val_mae: 0.9427\n",
      "Epoch 13/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.4309 - mae: 0.9238 - val_loss: 1.4271 - val_mae: 0.9550\n",
      "Epoch 14/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.4028 - mae: 0.9274 - val_loss: 1.4496 - val_mae: 0.9904\n",
      "Epoch 15/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3966 - mae: 0.9427 - val_loss: 1.4123 - val_mae: 0.9569\n",
      "Epoch 16/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3777 - mae: 0.9203 - val_loss: 1.3955 - val_mae: 0.9254\n",
      "Epoch 17/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3641 - mae: 0.9056 - val_loss: 1.3925 - val_mae: 0.9351\n",
      "Epoch 18/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3601 - mae: 0.9175 - val_loss: 1.4088 - val_mae: 0.9456\n",
      "Epoch 19/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.3666 - mae: 0.9170 - val_loss: 1.3873 - val_mae: 0.9227\n",
      "Epoch 20/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3563 - mae: 0.9023 - val_loss: 1.3726 - val_mae: 0.9214\n",
      "Epoch 21/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3410 - mae: 0.9004 - val_loss: 1.4131 - val_mae: 0.9608\n",
      "Epoch 22/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3467 - mae: 0.9184 - val_loss: 1.3778 - val_mae: 0.9486\n",
      "Epoch 23/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3336 - mae: 0.9116 - val_loss: 1.3768 - val_mae: 0.9380\n",
      "Epoch 24/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.3277 - mae: 0.9009 - val_loss: 1.3717 - val_mae: 0.9352\n",
      "Epoch 25/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3224 - mae: 0.9017 - val_loss: 1.3678 - val_mae: 0.9415\n",
      "Epoch 26/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.3196 - mae: 0.9050 - val_loss: 1.3706 - val_mae: 0.9345\n",
      "Epoch 27/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.3204 - mae: 0.9011 - val_loss: 1.3546 - val_mae: 0.9230\n",
      "Epoch 28/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3138 - mae: 0.8970 - val_loss: 1.3528 - val_mae: 0.9304\n",
      "Epoch 29/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3156 - mae: 0.9026 - val_loss: 1.3651 - val_mae: 0.9353\n",
      "Epoch 30/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.3158 - mae: 0.9037 - val_loss: 1.3470 - val_mae: 0.9177\n",
      "Epoch 31/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3062 - mae: 0.8939 - val_loss: 1.3523 - val_mae: 0.9241\n",
      "Epoch 32/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3071 - mae: 0.8974 - val_loss: 1.3528 - val_mae: 0.9277\n",
      "Epoch 33/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3058 - mae: 0.8995 - val_loss: 1.3445 - val_mae: 0.9192\n",
      "Epoch 34/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3009 - mae: 0.8935 - val_loss: 1.3536 - val_mae: 0.9261\n",
      "Epoch 35/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3049 - mae: 0.8962 - val_loss: 1.3507 - val_mae: 0.9298\n",
      "Epoch 36/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3048 - mae: 0.8994 - val_loss: 1.3446 - val_mae: 0.9251\n",
      "Epoch 37/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2984 - mae: 0.8939 - val_loss: 1.3523 - val_mae: 0.9261\n",
      "Epoch 38/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.3015 - mae: 0.8952 - val_loss: 1.3437 - val_mae: 0.9236\n",
      "Epoch 39/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2963 - mae: 0.8939 - val_loss: 1.3434 - val_mae: 0.9239\n",
      "Epoch 40/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3003 - mae: 0.8953 - val_loss: 1.3506 - val_mae: 0.9261\n",
      "Epoch 41/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2934 - mae: 0.8926 - val_loss: 1.3396 - val_mae: 0.9214\n",
      "Epoch 42/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2911 - mae: 0.8916 - val_loss: 1.3416 - val_mae: 0.9259\n",
      "Epoch 43/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2949 - mae: 0.8943 - val_loss: 1.3499 - val_mae: 0.9295\n",
      "Epoch 44/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2913 - mae: 0.8938 - val_loss: 1.3368 - val_mae: 0.9199\n",
      "Epoch 45/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2930 - mae: 0.8920 - val_loss: 1.3368 - val_mae: 0.9221\n",
      "Epoch 46/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2911 - mae: 0.8932 - val_loss: 1.3497 - val_mae: 0.9271\n",
      "Epoch 47/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2971 - mae: 0.8954 - val_loss: 1.3345 - val_mae: 0.9169\n",
      "Epoch 48/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2881 - mae: 0.8908 - val_loss: 1.3350 - val_mae: 0.9203\n",
      "Epoch 49/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2906 - mae: 0.8926 - val_loss: 1.3500 - val_mae: 0.9268\n",
      "Epoch 50/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2950 - mae: 0.8933 - val_loss: 1.3338 - val_mae: 0.9174\n",
      "Epoch 51/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2819 - mae: 0.8889 - val_loss: 1.3332 - val_mae: 0.9205\n",
      "Epoch 52/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2849 - mae: 0.8908 - val_loss: 1.3457 - val_mae: 0.9266\n",
      "Epoch 53/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2904 - mae: 0.8926 - val_loss: 1.3322 - val_mae: 0.9172\n",
      "Epoch 54/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2797 - mae: 0.8876 - val_loss: 1.3327 - val_mae: 0.9216\n",
      "Epoch 55/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2827 - mae: 0.8898 - val_loss: 1.3491 - val_mae: 0.9294\n",
      "Epoch 56/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2938 - mae: 0.8947 - val_loss: 1.3341 - val_mae: 0.9170\n",
      "Epoch 57/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2791 - mae: 0.8871 - val_loss: 1.3264 - val_mae: 0.9155\n",
      "Epoch 58/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2776 - mae: 0.8892 - val_loss: 1.3419 - val_mae: 0.9264\n",
      "Epoch 59/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2886 - mae: 0.8921 - val_loss: 1.3340 - val_mae: 0.9190\n",
      "Epoch 60/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.2781 - mae: 0.8871 - val_loss: 1.3250 - val_mae: 0.9148\n",
      "Epoch 61/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2756 - mae: 0.8868 - val_loss: 1.3355 - val_mae: 0.9237\n",
      "Epoch 62/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2853 - mae: 0.8920 - val_loss: 1.3368 - val_mae: 0.9194\n",
      "Epoch 63/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2855 - mae: 0.8893 - val_loss: 1.3234 - val_mae: 0.9126\n",
      "Epoch 64/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2773 - mae: 0.8873 - val_loss: 1.3329 - val_mae: 0.9237\n",
      "Epoch 65/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2761 - mae: 0.8902 - val_loss: 1.3450 - val_mae: 0.9264\n",
      "Epoch 66/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2808 - mae: 0.8888 - val_loss: 1.3226 - val_mae: 0.9141\n",
      "Epoch 67/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.2695 - mae: 0.8853 - val_loss: 1.3249 - val_mae: 0.9191\n",
      "Epoch 68/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2717 - mae: 0.8876 - val_loss: 1.3387 - val_mae: 0.9220\n",
      "Epoch 69/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2832 - mae: 0.8886 - val_loss: 1.3220 - val_mae: 0.9133\n",
      "Epoch 70/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2694 - mae: 0.8837 - val_loss: 1.3243 - val_mae: 0.9196\n",
      "Epoch 71/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2708 - mae: 0.8875 - val_loss: 1.3373 - val_mae: 0.9218\n",
      "Epoch 72/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2824 - mae: 0.8881 - val_loss: 1.3241 - val_mae: 0.9145\n",
      "Epoch 73/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2699 - mae: 0.8850 - val_loss: 1.3222 - val_mae: 0.9176\n",
      "Epoch 74/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2721 - mae: 0.8870 - val_loss: 1.3342 - val_mae: 0.9215\n",
      "Epoch 75/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2795 - mae: 0.8876 - val_loss: 1.3271 - val_mae: 0.9168\n",
      "Epoch 76/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2679 - mae: 0.8848 - val_loss: 1.3208 - val_mae: 0.9165\n",
      "Epoch 77/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2654 - mae: 0.8854 - val_loss: 1.3259 - val_mae: 0.9182\n",
      "Epoch 78/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2690 - mae: 0.8858 - val_loss: 1.3312 - val_mae: 0.9200\n",
      "Epoch 79/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2692 - mae: 0.8857 - val_loss: 1.3208 - val_mae: 0.9152\n",
      "Epoch 80/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2664 - mae: 0.8850 - val_loss: 1.3210 - val_mae: 0.9160\n",
      "Epoch 81/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2651 - mae: 0.8839 - val_loss: 1.3279 - val_mae: 0.9206\n",
      "Epoch 82/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2682 - mae: 0.8859 - val_loss: 1.3254 - val_mae: 0.9175\n",
      "Epoch 83/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2711 - mae: 0.8852 - val_loss: 1.3197 - val_mae: 0.9154\n",
      "Epoch 84/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2633 - mae: 0.8854 - val_loss: 1.3208 - val_mae: 0.9147\n",
      "Epoch 85/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2639 - mae: 0.8822 - val_loss: 1.3268 - val_mae: 0.9179\n",
      "Epoch 86/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2708 - mae: 0.8861 - val_loss: 1.3248 - val_mae: 0.9171\n",
      "Epoch 87/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2672 - mae: 0.8849 - val_loss: 1.3190 - val_mae: 0.9144\n",
      "Epoch 88/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2656 - mae: 0.8828 - val_loss: 1.3217 - val_mae: 0.9167\n",
      "Epoch 89/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2632 - mae: 0.8835 - val_loss: 1.3290 - val_mae: 0.9202\n",
      "Epoch 90/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2664 - mae: 0.8840 - val_loss: 1.3241 - val_mae: 0.9171\n",
      "Epoch 91/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2619 - mae: 0.8836 - val_loss: 1.3184 - val_mae: 0.9141\n",
      "Epoch 92/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2618 - mae: 0.8822 - val_loss: 1.3220 - val_mae: 0.9158\n",
      "Epoch 93/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2616 - mae: 0.8824 - val_loss: 1.3230 - val_mae: 0.9169\n",
      "Epoch 94/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2598 - mae: 0.8818 - val_loss: 1.3213 - val_mae: 0.9174\n",
      "Epoch 95/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2596 - mae: 0.8824 - val_loss: 1.3238 - val_mae: 0.9162\n",
      "Epoch 96/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2633 - mae: 0.8833 - val_loss: 1.3210 - val_mae: 0.9168\n",
      "Epoch 97/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2574 - mae: 0.8820 - val_loss: 1.3191 - val_mae: 0.9169\n",
      "Epoch 98/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2565 - mae: 0.8815 - val_loss: 1.3213 - val_mae: 0.9158\n",
      "Epoch 99/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2570 - mae: 0.8799 - val_loss: 1.3216 - val_mae: 0.9178\n",
      "Epoch 100/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2581 - mae: 0.8822 - val_loss: 1.3186 - val_mae: 0.9127\n",
      "Epoch 101/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2585 - mae: 0.8799 - val_loss: 1.3192 - val_mae: 0.9138\n",
      "Epoch 102/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2596 - mae: 0.8819 - val_loss: 1.3173 - val_mae: 0.9143\n",
      "Epoch 103/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2551 - mae: 0.8810 - val_loss: 1.3216 - val_mae: 0.9164\n",
      "Epoch 104/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2642 - mae: 0.8827 - val_loss: 1.3242 - val_mae: 0.9169\n",
      "Epoch 105/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2598 - mae: 0.8810 - val_loss: 1.3198 - val_mae: 0.9167\n",
      "Epoch 106/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2543 - mae: 0.8812 - val_loss: 1.3172 - val_mae: 0.9137\n",
      "Epoch 107/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2572 - mae: 0.8800 - val_loss: 1.3204 - val_mae: 0.9152\n",
      "Epoch 108/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2550 - mae: 0.8791 - val_loss: 1.3177 - val_mae: 0.9148\n",
      "Epoch 109/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2578 - mae: 0.8822 - val_loss: 1.3140 - val_mae: 0.9119\n",
      "Epoch 110/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2532 - mae: 0.8801 - val_loss: 1.3169 - val_mae: 0.9138\n",
      "Epoch 111/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2559 - mae: 0.8814 - val_loss: 1.3236 - val_mae: 0.9169\n",
      "Epoch 112/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2544 - mae: 0.8804 - val_loss: 1.3209 - val_mae: 0.9150\n",
      "Epoch 113/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2561 - mae: 0.8806 - val_loss: 1.3117 - val_mae: 0.9112\n",
      "Epoch 114/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2511 - mae: 0.8798 - val_loss: 1.3116 - val_mae: 0.9100\n",
      "Epoch 115/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.2523 - mae: 0.8787 - val_loss: 1.3174 - val_mae: 0.9122\n",
      "Epoch 116/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2559 - mae: 0.8804 - val_loss: 1.3143 - val_mae: 0.9125\n",
      "Epoch 117/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2530 - mae: 0.8804 - val_loss: 1.3098 - val_mae: 0.9087\n",
      "Epoch 118/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2474 - mae: 0.8777 - val_loss: 1.3126 - val_mae: 0.9116\n",
      "Epoch 119/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2471 - mae: 0.8777 - val_loss: 1.3213 - val_mae: 0.9158\n",
      "Epoch 120/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2591 - mae: 0.8814 - val_loss: 1.3149 - val_mae: 0.9109\n",
      "Epoch 121/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.2485 - mae: 0.8778 - val_loss: 1.3106 - val_mae: 0.9101\n",
      "Epoch 122/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.2479 - mae: 0.8780 - val_loss: 1.3121 - val_mae: 0.9089\n",
      "Epoch 123/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2516 - mae: 0.8774 - val_loss: 1.3191 - val_mae: 0.9129\n",
      "Epoch 124/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2541 - mae: 0.8800 - val_loss: 1.3128 - val_mae: 0.9093\n",
      "Epoch 125/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2467 - mae: 0.8777 - val_loss: 1.3091 - val_mae: 0.9089\n",
      "Epoch 126/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2455 - mae: 0.8789 - val_loss: 1.3097 - val_mae: 0.9095\n",
      "Epoch 127/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2467 - mae: 0.8768 - val_loss: 1.3191 - val_mae: 0.9142\n",
      "Epoch 128/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2506 - mae: 0.8787 - val_loss: 1.3122 - val_mae: 0.9097\n",
      "Epoch 129/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2482 - mae: 0.8788 - val_loss: 1.3071 - val_mae: 0.9049\n",
      "Epoch 130/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2454 - mae: 0.8771 - val_loss: 1.3117 - val_mae: 0.9100\n",
      "Epoch 131/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2502 - mae: 0.8784 - val_loss: 1.3199 - val_mae: 0.9136\n",
      "Epoch 132/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2462 - mae: 0.8777 - val_loss: 1.3093 - val_mae: 0.9090\n",
      "Epoch 133/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2451 - mae: 0.8778 - val_loss: 1.3093 - val_mae: 0.9084\n",
      "Epoch 134/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2438 - mae: 0.8752 - val_loss: 1.3157 - val_mae: 0.9143\n",
      "Epoch 135/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2479 - mae: 0.8795 - val_loss: 1.3164 - val_mae: 0.9117\n",
      "Epoch 136/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2485 - mae: 0.8788 - val_loss: 1.3067 - val_mae: 0.9063\n",
      "Epoch 137/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2449 - mae: 0.8761 - val_loss: 1.3083 - val_mae: 0.9086\n",
      "Epoch 138/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2453 - mae: 0.8756 - val_loss: 1.3153 - val_mae: 0.9128\n",
      "Epoch 139/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2433 - mae: 0.8766 - val_loss: 1.3090 - val_mae: 0.9086\n",
      "Epoch 140/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2470 - mae: 0.8792 - val_loss: 1.3084 - val_mae: 0.9053\n",
      "Epoch 141/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2447 - mae: 0.8747 - val_loss: 1.3168 - val_mae: 0.9148\n",
      "Epoch 142/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2454 - mae: 0.8780 - val_loss: 1.3125 - val_mae: 0.9083\n",
      "Epoch 143/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2434 - mae: 0.8754 - val_loss: 1.3093 - val_mae: 0.9086\n",
      "Epoch 144/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2413 - mae: 0.8751 - val_loss: 1.3117 - val_mae: 0.9102\n",
      "Epoch 145/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2411 - mae: 0.8751 - val_loss: 1.3126 - val_mae: 0.9140\n",
      "Epoch 146/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2453 - mae: 0.8782 - val_loss: 1.3088 - val_mae: 0.9069\n",
      "Epoch 147/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2380 - mae: 0.8731 - val_loss: 1.3077 - val_mae: 0.9085\n",
      "Epoch 148/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2360 - mae: 0.8750 - val_loss: 1.3097 - val_mae: 0.9075\n",
      "Epoch 149/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2483 - mae: 0.8768 - val_loss: 1.3098 - val_mae: 0.9103\n",
      "Epoch 150/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2389 - mae: 0.8762 - val_loss: 1.3129 - val_mae: 0.9119\n",
      "Epoch 151/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2452 - mae: 0.8768 - val_loss: 1.3105 - val_mae: 0.9082\n",
      "Epoch 152/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.2445 - mae: 0.8761 - val_loss: 1.3075 - val_mae: 0.9075\n",
      "Epoch 153/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.2402 - mae: 0.8753 - val_loss: 1.3092 - val_mae: 0.9070\n",
      "Epoch 154/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2360 - mae: 0.8736 - val_loss: 1.3144 - val_mae: 0.9136\n",
      "Epoch 155/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2465 - mae: 0.8787 - val_loss: 1.3123 - val_mae: 0.9073\n",
      "Epoch 156/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2427 - mae: 0.8756 - val_loss: 1.3085 - val_mae: 0.9104\n",
      "Epoch 157/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2354 - mae: 0.8747 - val_loss: 1.3063 - val_mae: 0.9053\n",
      "Epoch 158/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2378 - mae: 0.8730 - val_loss: 1.3200 - val_mae: 0.9154\n",
      "Epoch 159/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2523 - mae: 0.8811 - val_loss: 1.3115 - val_mae: 0.9052\n",
      "Epoch 160/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2394 - mae: 0.8724 - val_loss: 1.3109 - val_mae: 0.9100\n",
      "Epoch 161/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.2351 - mae: 0.8752 - val_loss: 1.3068 - val_mae: 0.9052\n",
      "Epoch 162/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2362 - mae: 0.8709 - val_loss: 1.3216 - val_mae: 0.9166\n",
      "Epoch 163/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2460 - mae: 0.8799 - val_loss: 1.3077 - val_mae: 0.9064\n",
      "Epoch 164/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2365 - mae: 0.8736 - val_loss: 1.3075 - val_mae: 0.9066\n",
      "Epoch 165/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2345 - mae: 0.8731 - val_loss: 1.3115 - val_mae: 0.9108\n",
      "Epoch 166/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2404 - mae: 0.8749 - val_loss: 1.3076 - val_mae: 0.9078\n",
      "Epoch 167/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2387 - mae: 0.8761 - val_loss: 1.3077 - val_mae: 0.9063\n",
      "Epoch 168/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2331 - mae: 0.8714 - val_loss: 1.3065 - val_mae: 0.9068\n",
      "Epoch 169/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2335 - mae: 0.8729 - val_loss: 1.3122 - val_mae: 0.9093\n",
      "Epoch 170/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2436 - mae: 0.8767 - val_loss: 1.3080 - val_mae: 0.9084\n",
      "Epoch 171/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2357 - mae: 0.8754 - val_loss: 1.3084 - val_mae: 0.9113\n",
      "Epoch 172/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2328 - mae: 0.8732 - val_loss: 1.3099 - val_mae: 0.9073\n",
      "Epoch 173/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2384 - mae: 0.8727 - val_loss: 1.3080 - val_mae: 0.9103\n",
      "Epoch 174/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2373 - mae: 0.8763 - val_loss: 1.3068 - val_mae: 0.9075\n",
      "Epoch 175/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2342 - mae: 0.8728 - val_loss: 1.3072 - val_mae: 0.9090\n",
      "Epoch 176/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2358 - mae: 0.8736 - val_loss: 1.3053 - val_mae: 0.9015\n",
      "Epoch 177/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2317 - mae: 0.8708 - val_loss: 1.3102 - val_mae: 0.9102\n",
      "Epoch 178/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2343 - mae: 0.8741 - val_loss: 1.3129 - val_mae: 0.9128\n",
      "Epoch 179/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2359 - mae: 0.8742 - val_loss: 1.3095 - val_mae: 0.9119\n",
      "Epoch 180/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2333 - mae: 0.8724 - val_loss: 1.3068 - val_mae: 0.9032\n",
      "Epoch 181/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2312 - mae: 0.8704 - val_loss: 1.3090 - val_mae: 0.9102\n",
      "Epoch 182/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2371 - mae: 0.8741 - val_loss: 1.3121 - val_mae: 0.9084\n",
      "Epoch 183/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2309 - mae: 0.8712 - val_loss: 1.3116 - val_mae: 0.9115\n",
      "Epoch 184/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2282 - mae: 0.8730 - val_loss: 1.3063 - val_mae: 0.9049\n",
      "Epoch 185/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2254 - mae: 0.8690 - val_loss: 1.3093 - val_mae: 0.9107\n",
      "Epoch 186/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2295 - mae: 0.8723 - val_loss: 1.3134 - val_mae: 0.9097\n",
      "Epoch 187/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2374 - mae: 0.8739 - val_loss: 1.3091 - val_mae: 0.9106\n",
      "Epoch 188/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2340 - mae: 0.8741 - val_loss: 1.3068 - val_mae: 0.9080\n",
      "Epoch 189/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2266 - mae: 0.8701 - val_loss: 1.3065 - val_mae: 0.9071\n",
      "Epoch 190/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2288 - mae: 0.8708 - val_loss: 1.3155 - val_mae: 0.9116\n",
      "Epoch 191/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2362 - mae: 0.8741 - val_loss: 1.3073 - val_mae: 0.9066\n",
      "Epoch 192/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2290 - mae: 0.8713 - val_loss: 1.3063 - val_mae: 0.9071\n",
      "Epoch 193/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2302 - mae: 0.8715 - val_loss: 1.3085 - val_mae: 0.9071\n",
      "Epoch 194/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2322 - mae: 0.8719 - val_loss: 1.3095 - val_mae: 0.9082\n",
      "Epoch 195/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2323 - mae: 0.8715 - val_loss: 1.3075 - val_mae: 0.9057\n",
      "Epoch 196/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2309 - mae: 0.8727 - val_loss: 1.3082 - val_mae: 0.9092\n",
      "Epoch 197/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2235 - mae: 0.8706 - val_loss: 1.3059 - val_mae: 0.9048\n",
      "Epoch 198/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2242 - mae: 0.8689 - val_loss: 1.3135 - val_mae: 0.9109\n",
      "Epoch 199/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2361 - mae: 0.8731 - val_loss: 1.3060 - val_mae: 0.9054\n",
      "Epoch 200/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2267 - mae: 0.8705 - val_loss: 1.3078 - val_mae: 0.9079\n",
      "Epoch 201/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2258 - mae: 0.8688 - val_loss: 1.3086 - val_mae: 0.9009\n",
      "Epoch 202/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2276 - mae: 0.8693 - val_loss: 1.3147 - val_mae: 0.9139\n",
      "Epoch 203/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2345 - mae: 0.8755 - val_loss: 1.3064 - val_mae: 0.9050\n",
      "Epoch 204/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2284 - mae: 0.8703 - val_loss: 1.3098 - val_mae: 0.9095\n",
      "Epoch 205/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2262 - mae: 0.8698 - val_loss: 1.3081 - val_mae: 0.9049\n",
      "Epoch 206/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2248 - mae: 0.8693 - val_loss: 1.3134 - val_mae: 0.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ciesl\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6198 - mae: 1.4427  \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Define the model in a HyperModel class\n",
    "class RegressionHyperModel(HyperModel):\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input layer\n",
    "        model.add(Dense(hp.Int(f'units_1', min_value=250, max_value=600, step=10), activation='relu', input_shape=(199,)))\n",
    "        model.add(Dropout(hp.Float(f'dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "        model.add(Dense(hp.Int(f'units_2', min_value=48, max_value=256, step=5), activation='relu',))\n",
    "   \n",
    "        model.add(Dense(hp.Int(f'units_3', min_value=8, max_value=128, step=4), activation='relu',))\n",
    "        # Output layer\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\n",
    "        # Learning Rate Decay with Exponential Decay\n",
    "        initial_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=5*1e-2, sampling='LOG')\n",
    "\n",
    "        # Optimizer with learning rate decay\n",
    "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "        # Compile model\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Instantiate the HyperModel\n",
    "hypermodel = RegressionHyperModel()\n",
    "\n",
    "# Create the tuner\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_mae',  # Objective is to minimize validation MAE\n",
    "    max_trials=500,        # Number of different hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of executions for each trial\n",
    "    directory='hyperparameter_search3',  # Where to save the results\n",
    "    project_name='regression_tuning3'\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "# Run hyperparameter search\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=500,  # Start with a static number of epochs, can adjust if needed\n",
    "             batch_size=9000,  # You can also tune this if desired\n",
    "             validation_data=(X_val, y_val), \n",
    "             callbacks=[early_stopping])\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Train the best model with optimal hyperparameters including batch size and epochs\n",
    "history = tuner.hypermodel.build(best_hps).fit(\n",
    "    X_train, y_train,\n",
    "    epochs=best_hps.get('epochs') if 'epochs' in best_hps.values else 1000,  # Add a fallback for epochs\n",
    "    batch_size=best_hps.get('batch_size') if 'batch_size' in best_hps.values else 9000,  # Add a fallback for batch size\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "results = tuner.hypermodel.build(best_hps).evaluate(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc5010-4933-425b-9b68-40b12416ba72",
   "metadata": {},
   "source": [
    "After identifying the best-performing combination of hyperparameters through the random search process, we can integrate these optimized values into our primary deep learning architecture. By doing so, we ensure that the model is configured to make the most accurate predictions based on the specific characteristics of our dataset.\r\n",
    "\r\n",
    "### Applying Optimized Hyperparameters\r\n",
    "\r\n",
    "Once we have the optimal hyperparameters (such as the number of hidden layers, number of perceptrons, learning rate, dropout rate, activation function, number of epochs, and batch size), we will modify our original model’s architecture to incorporate these values. This process will fine-tune the network for better performance and more accurate predictions.\r\n",
    "\r\n",
    "### Rerunning the Model\r\n",
    "\r\n",
    "With the optimized architecture in place, we will retrain the model on the training set and validate its performance on the test set. By doing this, we can directly observe the improvements resulting from the hyperparameter optimization.\r\n",
    "\r\n",
    "1. **Training the Optimized Model**: The model will be trained again, but now with the hyperparameters that were found to yield the best results during the optimization phase. We expect the model to converge more effectively, leading to improved predictive performance.\r\n",
    "\r\n",
    "2. **Evaluating on the Test Set**: After training, we will evaluate the model on the test set to observe how well it generalizes to unseen data. The test set provides a realistic evaluation of the model’s ability to predict the number of goals scored by both teams.\r\n",
    "\r\n",
    "### Expected Results\r\n",
    "\r\n",
    "By using the optimized hyperparameters, we expect a reduction in the model's **Mean Absolute Error (MAE)** on the test set, compared to the initial architecture. A lower MAE indicates that the model has improved in terms of predicting the actual goals scored during a match, which is critical for making more accurate football match predictions.\r\n",
    "\r\n",
    "We will closely analyze the test set results to ensure the improvements are significant and consistent across various matches. If necessary, further fine-tuning can be performed, but with the optimized architecture, we should see a noticeable enhancement in the model's overall performance.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad64dfe2-2b50-405c-a518-083195e47978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ciesl\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - loss: 7.8101 - mae: 1.7792 - val_loss: 8.2953 - val_mae: 2.3686\n",
      "Epoch 2/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.2054 - mae: 1.8579 - val_loss: 4.1479 - val_mae: 1.3044\n",
      "Epoch 3/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 4.1294 - mae: 1.3198 - val_loss: 3.9703 - val_mae: 1.2910\n",
      "Epoch 4/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.7904 - mae: 1.2627 - val_loss: 2.8882 - val_mae: 1.0486\n",
      "Epoch 5/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.8655 - mae: 1.0449 - val_loss: 2.9919 - val_mae: 1.2227\n",
      "Epoch 6/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.8881 - mae: 1.1458 - val_loss: 2.8935 - val_mae: 1.1892\n",
      "Epoch 7/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.6650 - mae: 1.0698 - val_loss: 2.3651 - val_mae: 0.9757\n",
      "Epoch 8/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.4845 - mae: 0.9663 - val_loss: 2.4306 - val_mae: 0.9604\n",
      "Epoch 9/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.5731 - mae: 0.9878 - val_loss: 2.2916 - val_mae: 0.9145\n",
      "Epoch 10/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.4105 - mae: 0.9523 - val_loss: 2.2771 - val_mae: 1.0170\n",
      "Epoch 11/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.3083 - mae: 0.9821 - val_loss: 2.4535 - val_mae: 1.0737\n",
      "Epoch 12/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.3593 - mae: 1.0082 - val_loss: 2.2292 - val_mae: 1.0154\n",
      "Epoch 13/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.2383 - mae: 0.9728 - val_loss: 2.1402 - val_mae: 0.9598\n",
      "Epoch 14/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.1993 - mae: 0.9385 - val_loss: 2.1259 - val_mae: 0.9397\n",
      "Epoch 15/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.1720 - mae: 0.9351 - val_loss: 2.1249 - val_mae: 0.9953\n",
      "Epoch 16/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.1420 - mae: 0.9614 - val_loss: 2.1070 - val_mae: 0.9853\n",
      "Epoch 17/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.1082 - mae: 0.9503 - val_loss: 2.0531 - val_mae: 0.9595\n",
      "Epoch 18/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.0707 - mae: 0.9349 - val_loss: 2.0278 - val_mae: 0.9513\n",
      "Epoch 19/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.0412 - mae: 0.9252 - val_loss: 2.0127 - val_mae: 0.9559\n",
      "Epoch 20/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.0053 - mae: 0.9345 - val_loss: 2.0071 - val_mae: 0.9458\n",
      "Epoch 21/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.9858 - mae: 0.9283 - val_loss: 1.9599 - val_mae: 0.9418\n",
      "Epoch 22/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.9446 - mae: 0.9189 - val_loss: 1.9359 - val_mae: 0.9268\n",
      "Epoch 23/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.9227 - mae: 0.9107 - val_loss: 1.9136 - val_mae: 0.9333\n",
      "Epoch 24/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.8995 - mae: 0.9155 - val_loss: 1.9012 - val_mae: 0.9359\n",
      "Epoch 25/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.8827 - mae: 0.9157 - val_loss: 1.8824 - val_mae: 0.9292\n",
      "Epoch 26/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.8653 - mae: 0.9066 - val_loss: 1.8692 - val_mae: 0.9243\n",
      "Epoch 27/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.8531 - mae: 0.9071 - val_loss: 1.8628 - val_mae: 0.9268\n",
      "Epoch 28/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.8386 - mae: 0.9043 - val_loss: 1.8479 - val_mae: 0.9226\n",
      "Epoch 29/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.8262 - mae: 0.9031 - val_loss: 1.8415 - val_mae: 0.9254\n",
      "Epoch 30/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.8100 - mae: 0.9022 - val_loss: 1.8338 - val_mae: 0.9240\n",
      "Epoch 31/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7982 - mae: 0.8992 - val_loss: 1.8234 - val_mae: 0.9190\n",
      "Epoch 32/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7917 - mae: 0.9006 - val_loss: 1.8271 - val_mae: 0.9230\n",
      "Epoch 33/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.7911 - mae: 0.9006 - val_loss: 1.8046 - val_mae: 0.9187\n",
      "Epoch 34/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7734 - mae: 0.8977 - val_loss: 1.8018 - val_mae: 0.9217\n",
      "Epoch 35/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7614 - mae: 0.8982 - val_loss: 1.7894 - val_mae: 0.9229\n",
      "Epoch 36/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7563 - mae: 0.8990 - val_loss: 1.7874 - val_mae: 0.9210\n",
      "Epoch 37/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7530 - mae: 0.8998 - val_loss: 1.7747 - val_mae: 0.9201\n",
      "Epoch 38/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7334 - mae: 0.8951 - val_loss: 1.7657 - val_mae: 0.9203\n",
      "Epoch 39/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7299 - mae: 0.8957 - val_loss: 1.7613 - val_mae: 0.9226\n",
      "Epoch 40/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7191 - mae: 0.8949 - val_loss: 1.7574 - val_mae: 0.9235\n",
      "Epoch 41/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7178 - mae: 0.8978 - val_loss: 1.7921 - val_mae: 0.9368\n",
      "Epoch 42/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7300 - mae: 0.9026 - val_loss: 1.7547 - val_mae: 0.9221\n",
      "Epoch 43/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.7153 - mae: 0.8985 - val_loss: 1.7598 - val_mae: 0.9283\n",
      "Epoch 44/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7108 - mae: 0.8997 - val_loss: 1.7382 - val_mae: 0.9228\n",
      "Epoch 45/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6933 - mae: 0.8972 - val_loss: 1.7380 - val_mae: 0.9266\n",
      "Epoch 46/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6881 - mae: 0.8965 - val_loss: 1.7164 - val_mae: 0.9151\n",
      "Epoch 47/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6797 - mae: 0.8916 - val_loss: 1.7132 - val_mae: 0.9165\n",
      "Epoch 48/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6784 - mae: 0.8934 - val_loss: 1.7115 - val_mae: 0.9229\n",
      "Epoch 49/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6713 - mae: 0.8967 - val_loss: 1.7034 - val_mae: 0.9161\n",
      "Epoch 50/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6609 - mae: 0.8899 - val_loss: 1.7023 - val_mae: 0.9169\n",
      "Epoch 51/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6538 - mae: 0.8914 - val_loss: 1.6954 - val_mae: 0.9221\n",
      "Epoch 52/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6510 - mae: 0.8952 - val_loss: 1.6896 - val_mae: 0.9150\n",
      "Epoch 53/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6477 - mae: 0.8908 - val_loss: 1.6858 - val_mae: 0.9187\n",
      "Epoch 54/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6384 - mae: 0.8924 - val_loss: 1.6823 - val_mae: 0.9225\n",
      "Epoch 55/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6351 - mae: 0.8937 - val_loss: 1.6758 - val_mae: 0.9173\n",
      "Epoch 56/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6306 - mae: 0.8911 - val_loss: 1.6698 - val_mae: 0.9178\n",
      "Epoch 57/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6236 - mae: 0.8912 - val_loss: 1.6679 - val_mae: 0.9192\n",
      "Epoch 58/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.6221 - mae: 0.8920 - val_loss: 1.6600 - val_mae: 0.9162\n",
      "Epoch 59/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6154 - mae: 0.8910 - val_loss: 1.6593 - val_mae: 0.9196\n",
      "Epoch 60/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.6123 - mae: 0.8922 - val_loss: 1.6545 - val_mae: 0.9176\n",
      "Epoch 61/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.6058 - mae: 0.8898 - val_loss: 1.6537 - val_mae: 0.9205\n",
      "Epoch 62/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.6032 - mae: 0.8908 - val_loss: 1.6465 - val_mae: 0.9185\n",
      "Epoch 63/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5965 - mae: 0.8892 - val_loss: 1.6450 - val_mae: 0.9198\n",
      "Epoch 64/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5915 - mae: 0.8905 - val_loss: 1.6366 - val_mae: 0.9191\n",
      "Epoch 65/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5859 - mae: 0.8894 - val_loss: 1.6353 - val_mae: 0.9180\n",
      "Epoch 66/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5847 - mae: 0.8886 - val_loss: 1.6289 - val_mae: 0.9178\n",
      "Epoch 67/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5769 - mae: 0.8888 - val_loss: 1.6289 - val_mae: 0.9216\n",
      "Epoch 68/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5772 - mae: 0.8909 - val_loss: 1.6215 - val_mae: 0.9170\n",
      "Epoch 69/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.5716 - mae: 0.8877 - val_loss: 1.6195 - val_mae: 0.9175\n",
      "Epoch 70/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5657 - mae: 0.8878 - val_loss: 1.6155 - val_mae: 0.9200\n",
      "Epoch 71/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5660 - mae: 0.8899 - val_loss: 1.6094 - val_mae: 0.9142\n",
      "Epoch 72/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.5583 - mae: 0.8883 - val_loss: 1.6087 - val_mae: 0.9205\n",
      "Epoch 73/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5603 - mae: 0.8920 - val_loss: 1.6026 - val_mae: 0.9133\n",
      "Epoch 74/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.5520 - mae: 0.8869 - val_loss: 1.5992 - val_mae: 0.9167\n",
      "Epoch 75/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.5495 - mae: 0.8900 - val_loss: 1.5951 - val_mae: 0.9137\n",
      "Epoch 76/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5466 - mae: 0.8879 - val_loss: 1.5924 - val_mae: 0.9157\n",
      "Epoch 77/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5398 - mae: 0.8880 - val_loss: 1.5932 - val_mae: 0.9178\n",
      "Epoch 78/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5392 - mae: 0.8891 - val_loss: 1.5837 - val_mae: 0.9126\n",
      "Epoch 79/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5285 - mae: 0.8850 - val_loss: 1.5826 - val_mae: 0.9195\n",
      "Epoch 80/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5333 - mae: 0.8903 - val_loss: 1.5767 - val_mae: 0.9114\n",
      "Epoch 81/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5280 - mae: 0.8859 - val_loss: 1.5783 - val_mae: 0.9179\n",
      "Epoch 82/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5239 - mae: 0.8883 - val_loss: 1.5830 - val_mae: 0.9151\n",
      "Epoch 83/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5307 - mae: 0.8890 - val_loss: 1.5682 - val_mae: 0.9115\n",
      "Epoch 84/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5109 - mae: 0.8845 - val_loss: 1.5645 - val_mae: 0.9127\n",
      "Epoch 85/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.5137 - mae: 0.8876 - val_loss: 1.5711 - val_mae: 0.9137\n",
      "Epoch 86/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5148 - mae: 0.8862 - val_loss: 1.5618 - val_mae: 0.9164\n",
      "Epoch 87/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5058 - mae: 0.8874 - val_loss: 1.5610 - val_mae: 0.9139\n",
      "Epoch 88/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5042 - mae: 0.8863 - val_loss: 1.5552 - val_mae: 0.9091\n",
      "Epoch 89/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4979 - mae: 0.8840 - val_loss: 1.5513 - val_mae: 0.9135\n",
      "Epoch 90/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4994 - mae: 0.8879 - val_loss: 1.5551 - val_mae: 0.9120\n",
      "Epoch 91/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4977 - mae: 0.8857 - val_loss: 1.5453 - val_mae: 0.9125\n",
      "Epoch 92/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4903 - mae: 0.8876 - val_loss: 1.5423 - val_mae: 0.9092\n",
      "Epoch 93/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.4928 - mae: 0.8857 - val_loss: 1.5450 - val_mae: 0.9086\n",
      "Epoch 94/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4843 - mae: 0.8841 - val_loss: 1.5371 - val_mae: 0.9114\n",
      "Epoch 95/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4811 - mae: 0.8850 - val_loss: 1.5360 - val_mae: 0.9089\n",
      "Epoch 96/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4791 - mae: 0.8849 - val_loss: 1.5379 - val_mae: 0.9127\n",
      "Epoch 97/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4808 - mae: 0.8860 - val_loss: 1.5300 - val_mae: 0.9045\n",
      "Epoch 98/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4733 - mae: 0.8837 - val_loss: 1.5280 - val_mae: 0.9149\n",
      "Epoch 99/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4770 - mae: 0.8892 - val_loss: 1.5287 - val_mae: 0.9045\n",
      "Epoch 100/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.4710 - mae: 0.8820 - val_loss: 1.5212 - val_mae: 0.9136\n",
      "Epoch 101/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4694 - mae: 0.8878 - val_loss: 1.5188 - val_mae: 0.9032\n",
      "Epoch 102/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.4596 - mae: 0.8812 - val_loss: 1.5190 - val_mae: 0.9077\n",
      "Epoch 103/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.4607 - mae: 0.8851 - val_loss: 1.5130 - val_mae: 0.9039\n",
      "Epoch 104/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.4557 - mae: 0.8826 - val_loss: 1.5106 - val_mae: 0.9048\n",
      "Epoch 105/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.4541 - mae: 0.8833 - val_loss: 1.5101 - val_mae: 0.9060\n",
      "Epoch 106/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.4490 - mae: 0.8833 - val_loss: 1.5067 - val_mae: 0.9048\n",
      "Epoch 107/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.4467 - mae: 0.8824 - val_loss: 1.5037 - val_mae: 0.9041\n",
      "Epoch 108/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.4451 - mae: 0.8825 - val_loss: 1.5019 - val_mae: 0.9019\n",
      "Epoch 109/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.4453 - mae: 0.8826 - val_loss: 1.4998 - val_mae: 0.9034\n",
      "Epoch 110/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.4418 - mae: 0.8822 - val_loss: 1.4989 - val_mae: 0.9047\n",
      "Epoch 111/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.4369 - mae: 0.8823 - val_loss: 1.4931 - val_mae: 0.9029\n",
      "Epoch 112/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.4348 - mae: 0.8809 - val_loss: 1.4920 - val_mae: 0.9021\n",
      "Epoch 113/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.4346 - mae: 0.8822 - val_loss: 1.4939 - val_mae: 0.9053\n",
      "Epoch 114/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.4326 - mae: 0.8843 - val_loss: 1.4897 - val_mae: 0.9058\n",
      "Epoch 115/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.4282 - mae: 0.8828 - val_loss: 1.4890 - val_mae: 0.9024\n",
      "Epoch 116/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.4264 - mae: 0.8816 - val_loss: 1.4845 - val_mae: 0.9042\n",
      "Epoch 117/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.4259 - mae: 0.8810 - val_loss: 1.4827 - val_mae: 0.9036\n",
      "Epoch 118/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.4195 - mae: 0.8817 - val_loss: 1.4809 - val_mae: 0.9028\n",
      "Epoch 119/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.4193 - mae: 0.8813 - val_loss: 1.4813 - val_mae: 0.9056\n",
      "Epoch 120/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.4159 - mae: 0.8818 - val_loss: 1.4780 - val_mae: 0.9050\n",
      "Epoch 121/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.4170 - mae: 0.8820 - val_loss: 1.4734 - val_mae: 0.9019\n",
      "Epoch 122/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.4098 - mae: 0.8787 - val_loss: 1.4698 - val_mae: 0.9067\n",
      "Epoch 123/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4134 - mae: 0.8826 - val_loss: 1.4711 - val_mae: 0.9005\n",
      "Epoch 124/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4081 - mae: 0.8810 - val_loss: 1.4732 - val_mae: 0.9043\n",
      "Epoch 125/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4117 - mae: 0.8835 - val_loss: 1.4708 - val_mae: 0.8972\n",
      "Epoch 126/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.4066 - mae: 0.8800 - val_loss: 1.4622 - val_mae: 0.9040\n",
      "Epoch 127/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.4015 - mae: 0.8809 - val_loss: 1.4618 - val_mae: 0.9007\n",
      "Epoch 128/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.4011 - mae: 0.8801 - val_loss: 1.4695 - val_mae: 0.9078\n",
      "Epoch 129/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.4046 - mae: 0.8826 - val_loss: 1.4613 - val_mae: 0.8988\n",
      "Epoch 130/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3964 - mae: 0.8808 - val_loss: 1.4570 - val_mae: 0.9011\n",
      "Epoch 131/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3959 - mae: 0.8805 - val_loss: 1.4572 - val_mae: 0.9033\n",
      "Epoch 132/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3955 - mae: 0.8813 - val_loss: 1.4659 - val_mae: 0.9071\n",
      "Epoch 133/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3984 - mae: 0.8804 - val_loss: 1.4580 - val_mae: 0.9047\n",
      "Epoch 134/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3922 - mae: 0.8821 - val_loss: 1.4548 - val_mae: 0.9032\n",
      "Epoch 135/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3912 - mae: 0.8820 - val_loss: 1.4704 - val_mae: 0.9095\n",
      "Epoch 136/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3987 - mae: 0.8843 - val_loss: 1.4486 - val_mae: 0.9027\n",
      "Epoch 137/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.3853 - mae: 0.8793 - val_loss: 1.4467 - val_mae: 0.9004\n",
      "Epoch 138/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3856 - mae: 0.8784 - val_loss: 1.4487 - val_mae: 0.9020\n",
      "Epoch 139/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3847 - mae: 0.8796 - val_loss: 1.4453 - val_mae: 0.9047\n",
      "Epoch 140/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3792 - mae: 0.8804 - val_loss: 1.4442 - val_mae: 0.9054\n",
      "Epoch 141/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3804 - mae: 0.8806 - val_loss: 1.4531 - val_mae: 0.9103\n",
      "Epoch 142/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3824 - mae: 0.8805 - val_loss: 1.4433 - val_mae: 0.9074\n",
      "Epoch 143/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3748 - mae: 0.8814 - val_loss: 1.4379 - val_mae: 0.9003\n",
      "Epoch 144/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3730 - mae: 0.8766 - val_loss: 1.4438 - val_mae: 0.9061\n",
      "Epoch 145/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3786 - mae: 0.8830 - val_loss: 1.4422 - val_mae: 0.9034\n",
      "Epoch 146/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3740 - mae: 0.8797 - val_loss: 1.4349 - val_mae: 0.9012\n",
      "Epoch 147/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3724 - mae: 0.8807 - val_loss: 1.4365 - val_mae: 0.9000\n",
      "Epoch 148/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3692 - mae: 0.8770 - val_loss: 1.4319 - val_mae: 0.9075\n",
      "Epoch 149/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3670 - mae: 0.8814 - val_loss: 1.4323 - val_mae: 0.9036\n",
      "Epoch 150/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.3650 - mae: 0.8790 - val_loss: 1.4289 - val_mae: 0.9025\n",
      "Epoch 151/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.3633 - mae: 0.8795 - val_loss: 1.4300 - val_mae: 0.9046\n",
      "Epoch 152/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3594 - mae: 0.8790 - val_loss: 1.4249 - val_mae: 0.9018\n",
      "Epoch 153/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3604 - mae: 0.8794 - val_loss: 1.4221 - val_mae: 0.8995\n",
      "Epoch 154/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3579 - mae: 0.8772 - val_loss: 1.4206 - val_mae: 0.9020\n",
      "Epoch 155/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.3558 - mae: 0.8789 - val_loss: 1.4208 - val_mae: 0.9011\n",
      "Epoch 156/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3541 - mae: 0.8772 - val_loss: 1.4211 - val_mae: 0.9005\n",
      "Epoch 157/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3555 - mae: 0.8798 - val_loss: 1.4214 - val_mae: 0.9009\n",
      "Epoch 158/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3553 - mae: 0.8786 - val_loss: 1.4156 - val_mae: 0.9033\n",
      "Epoch 159/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3530 - mae: 0.8795 - val_loss: 1.4139 - val_mae: 0.9018\n",
      "Epoch 160/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3476 - mae: 0.8765 - val_loss: 1.4148 - val_mae: 0.9064\n",
      "Epoch 161/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3480 - mae: 0.8774 - val_loss: 1.4144 - val_mae: 0.9026\n",
      "Epoch 162/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3461 - mae: 0.8779 - val_loss: 1.4163 - val_mae: 0.9052\n",
      "Epoch 163/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.3445 - mae: 0.8789 - val_loss: 1.4152 - val_mae: 0.9016\n",
      "Epoch 164/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3430 - mae: 0.8774 - val_loss: 1.4124 - val_mae: 0.9036\n",
      "Epoch 165/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.3416 - mae: 0.8777 - val_loss: 1.4146 - val_mae: 0.9040\n",
      "Epoch 166/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3424 - mae: 0.8771 - val_loss: 1.4106 - val_mae: 0.9032\n",
      "Epoch 167/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3351 - mae: 0.8759 - val_loss: 1.4089 - val_mae: 0.9018\n",
      "Epoch 168/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3381 - mae: 0.8765 - val_loss: 1.4094 - val_mae: 0.9026\n",
      "Epoch 169/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.3369 - mae: 0.8770 - val_loss: 1.4068 - val_mae: 0.9029\n",
      "Epoch 170/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3388 - mae: 0.8775 - val_loss: 1.4048 - val_mae: 0.9022\n",
      "Epoch 171/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.3383 - mae: 0.8783 - val_loss: 1.4081 - val_mae: 0.8995\n",
      "Epoch 172/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3355 - mae: 0.8757 - val_loss: 1.4036 - val_mae: 0.9049\n",
      "Epoch 173/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3351 - mae: 0.8795 - val_loss: 1.4044 - val_mae: 0.9015\n",
      "Epoch 174/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3329 - mae: 0.8748 - val_loss: 1.4014 - val_mae: 0.9030\n",
      "Epoch 175/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.3303 - mae: 0.8786 - val_loss: 1.4030 - val_mae: 0.8988\n",
      "Epoch 176/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3308 - mae: 0.8745 - val_loss: 1.4012 - val_mae: 0.9039\n",
      "Epoch 177/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.3265 - mae: 0.8765 - val_loss: 1.3992 - val_mae: 0.9021\n",
      "Epoch 178/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3298 - mae: 0.8766 - val_loss: 1.3992 - val_mae: 0.9028\n",
      "Epoch 179/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.3277 - mae: 0.8767 - val_loss: 1.3969 - val_mae: 0.9038\n",
      "Epoch 180/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3244 - mae: 0.8762 - val_loss: 1.3966 - val_mae: 0.9032\n",
      "Epoch 181/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.3251 - mae: 0.8780 - val_loss: 1.3963 - val_mae: 0.9001\n",
      "Epoch 182/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3248 - mae: 0.8741 - val_loss: 1.3948 - val_mae: 0.9049\n",
      "Epoch 183/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3219 - mae: 0.8771 - val_loss: 1.3974 - val_mae: 0.9021\n",
      "Epoch 184/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3223 - mae: 0.8751 - val_loss: 1.3949 - val_mae: 0.9067\n",
      "Epoch 185/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3264 - mae: 0.8793 - val_loss: 1.3972 - val_mae: 0.9017\n",
      "Epoch 186/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.3182 - mae: 0.8735 - val_loss: 1.3951 - val_mae: 0.9110\n",
      "Epoch 187/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.3182 - mae: 0.8769 - val_loss: 1.3939 - val_mae: 0.8997\n",
      "Epoch 188/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3186 - mae: 0.8747 - val_loss: 1.3933 - val_mae: 0.9069\n",
      "Epoch 189/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3225 - mae: 0.8786 - val_loss: 1.3955 - val_mae: 0.8996\n",
      "Epoch 190/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.3189 - mae: 0.8743 - val_loss: 1.3894 - val_mae: 0.9051\n",
      "Epoch 191/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3157 - mae: 0.8769 - val_loss: 1.3915 - val_mae: 0.8966\n",
      "Epoch 192/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3123 - mae: 0.8738 - val_loss: 1.3911 - val_mae: 0.9082\n",
      "Epoch 193/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3187 - mae: 0.8790 - val_loss: 1.3906 - val_mae: 0.8979\n",
      "Epoch 194/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3112 - mae: 0.8729 - val_loss: 1.3858 - val_mae: 0.9073\n",
      "Epoch 195/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3174 - mae: 0.8790 - val_loss: 1.3870 - val_mae: 0.8972\n",
      "Epoch 196/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3133 - mae: 0.8743 - val_loss: 1.3845 - val_mae: 0.9044\n",
      "Epoch 197/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3164 - mae: 0.8771 - val_loss: 1.3845 - val_mae: 0.8965\n",
      "Epoch 198/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3105 - mae: 0.8741 - val_loss: 1.3820 - val_mae: 0.9067\n",
      "Epoch 199/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3138 - mae: 0.8776 - val_loss: 1.3883 - val_mae: 0.8997\n",
      "Epoch 200/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3105 - mae: 0.8753 - val_loss: 1.3848 - val_mae: 0.9079\n",
      "Epoch 201/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.3078 - mae: 0.8769 - val_loss: 1.3837 - val_mae: 0.8968\n",
      "Epoch 202/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3054 - mae: 0.8723 - val_loss: 1.3829 - val_mae: 0.9039\n",
      "Epoch 203/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3084 - mae: 0.8751 - val_loss: 1.3808 - val_mae: 0.9014\n",
      "Epoch 204/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3094 - mae: 0.8762 - val_loss: 1.3793 - val_mae: 0.9064\n",
      "Epoch 205/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3051 - mae: 0.8769 - val_loss: 1.3814 - val_mae: 0.9021\n",
      "Epoch 206/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3050 - mae: 0.8749 - val_loss: 1.3789 - val_mae: 0.9004\n",
      "Epoch 207/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.3027 - mae: 0.8742 - val_loss: 1.3770 - val_mae: 0.9010\n",
      "Epoch 208/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.2998 - mae: 0.8741 - val_loss: 1.3771 - val_mae: 0.9036\n",
      "Epoch 209/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3043 - mae: 0.8749 - val_loss: 1.3780 - val_mae: 0.9055\n",
      "Epoch 210/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3008 - mae: 0.8752 - val_loss: 1.3807 - val_mae: 0.9029\n",
      "Epoch 211/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.3015 - mae: 0.8752 - val_loss: 1.3773 - val_mae: 0.9026\n",
      "Epoch 212/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3018 - mae: 0.8756 - val_loss: 1.3762 - val_mae: 0.9008\n",
      "Epoch 213/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2994 - mae: 0.8735 - val_loss: 1.3741 - val_mae: 0.9030\n",
      "Epoch 214/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2945 - mae: 0.8743 - val_loss: 1.3765 - val_mae: 0.9018\n",
      "Epoch 215/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3001 - mae: 0.8751 - val_loss: 1.3738 - val_mae: 0.9035\n",
      "Epoch 216/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2983 - mae: 0.8743 - val_loss: 1.3754 - val_mae: 0.9029\n",
      "Epoch 217/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2976 - mae: 0.8742 - val_loss: 1.3751 - val_mae: 0.9028\n",
      "Epoch 218/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.2938 - mae: 0.8744 - val_loss: 1.3827 - val_mae: 0.9030\n",
      "Epoch 219/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3028 - mae: 0.8750 - val_loss: 1.3711 - val_mae: 0.9077\n",
      "Epoch 220/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2963 - mae: 0.8758 - val_loss: 1.3757 - val_mae: 0.9008\n",
      "Epoch 221/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2962 - mae: 0.8733 - val_loss: 1.3753 - val_mae: 0.9083\n",
      "Epoch 222/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2980 - mae: 0.8782 - val_loss: 1.3733 - val_mae: 0.9016\n",
      "Epoch 223/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2969 - mae: 0.8735 - val_loss: 1.3683 - val_mae: 0.9038\n",
      "Epoch 224/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2950 - mae: 0.8744 - val_loss: 1.3694 - val_mae: 0.9012\n",
      "Epoch 225/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2936 - mae: 0.8744 - val_loss: 1.3786 - val_mae: 0.9024\n",
      "Epoch 226/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3041 - mae: 0.8770 - val_loss: 1.3685 - val_mae: 0.9012\n",
      "Epoch 227/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2905 - mae: 0.8735 - val_loss: 1.3678 - val_mae: 0.9028\n",
      "Epoch 228/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2938 - mae: 0.8739 - val_loss: 1.3690 - val_mae: 0.8984\n",
      "Epoch 229/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2908 - mae: 0.8725 - val_loss: 1.3720 - val_mae: 0.9028\n",
      "Epoch 230/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2956 - mae: 0.8748 - val_loss: 1.3707 - val_mae: 0.9028\n",
      "Epoch 231/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2884 - mae: 0.8751 - val_loss: 1.3685 - val_mae: 0.9001\n",
      "Epoch 232/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2904 - mae: 0.8726 - val_loss: 1.3699 - val_mae: 0.9041\n",
      "Epoch 233/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2915 - mae: 0.8750 - val_loss: 1.3687 - val_mae: 0.8992\n",
      "Epoch 234/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2874 - mae: 0.8710 - val_loss: 1.3680 - val_mae: 0.9027\n",
      "Epoch 235/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2902 - mae: 0.8766 - val_loss: 1.3739 - val_mae: 0.9028\n",
      "Epoch 236/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2920 - mae: 0.8737 - val_loss: 1.3620 - val_mae: 0.9022\n",
      "Epoch 237/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2923 - mae: 0.8748 - val_loss: 1.3664 - val_mae: 0.8987\n",
      "Epoch 238/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2873 - mae: 0.8722 - val_loss: 1.3796 - val_mae: 0.9083\n",
      "Epoch 239/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2962 - mae: 0.8778 - val_loss: 1.3718 - val_mae: 0.9041\n",
      "Epoch 240/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2882 - mae: 0.8747 - val_loss: 1.3655 - val_mae: 0.8993\n",
      "Epoch 241/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2883 - mae: 0.8724 - val_loss: 1.3784 - val_mae: 0.9080\n",
      "Epoch 242/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2978 - mae: 0.8777 - val_loss: 1.3744 - val_mae: 0.9064\n",
      "Epoch 243/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2888 - mae: 0.8750 - val_loss: 1.3646 - val_mae: 0.9024\n",
      "Epoch 244/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2944 - mae: 0.8762 - val_loss: 1.3789 - val_mae: 0.9032\n",
      "Epoch 245/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2906 - mae: 0.8740 - val_loss: 1.3758 - val_mae: 0.9142\n",
      "Epoch 246/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.2927 - mae: 0.8781 - val_loss: 1.3692 - val_mae: 0.9037\n",
      "Epoch 247/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2955 - mae: 0.8767 - val_loss: 1.3699 - val_mae: 0.9037\n",
      "Epoch 248/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2875 - mae: 0.8748 - val_loss: 1.3750 - val_mae: 0.9134\n",
      "Epoch 249/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2887 - mae: 0.8781 - val_loss: 1.3685 - val_mae: 0.9002\n",
      "Epoch 250/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2898 - mae: 0.8732 - val_loss: 1.3619 - val_mae: 0.9042\n",
      "Epoch 251/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2844 - mae: 0.8748 - val_loss: 1.3637 - val_mae: 0.9008\n",
      "Epoch 252/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2844 - mae: 0.8738 - val_loss: 1.3638 - val_mae: 0.9028\n",
      "Epoch 253/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2840 - mae: 0.8759 - val_loss: 1.3619 - val_mae: 0.8998\n",
      "Epoch 254/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2815 - mae: 0.8718 - val_loss: 1.3573 - val_mae: 0.9030\n",
      "Epoch 255/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2811 - mae: 0.8739 - val_loss: 1.3603 - val_mae: 0.8995\n",
      "Epoch 256/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2796 - mae: 0.8717 - val_loss: 1.3597 - val_mae: 0.9027\n",
      "Epoch 257/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2809 - mae: 0.8751 - val_loss: 1.3622 - val_mae: 0.8990\n",
      "Epoch 258/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.2839 - mae: 0.8731 - val_loss: 1.3582 - val_mae: 0.9039\n",
      "Epoch 259/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2810 - mae: 0.8750 - val_loss: 1.3585 - val_mae: 0.8996\n",
      "Epoch 260/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2818 - mae: 0.8709 - val_loss: 1.3585 - val_mae: 0.9035\n",
      "Epoch 261/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2834 - mae: 0.8749 - val_loss: 1.3647 - val_mae: 0.8995\n",
      "Epoch 262/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2799 - mae: 0.8725 - val_loss: 1.3638 - val_mae: 0.9050\n",
      "Epoch 263/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2768 - mae: 0.8737 - val_loss: 1.3625 - val_mae: 0.9018\n",
      "Epoch 264/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2804 - mae: 0.8721 - val_loss: 1.3598 - val_mae: 0.9014\n",
      "Epoch 265/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2750 - mae: 0.8708 - val_loss: 1.3620 - val_mae: 0.9016\n",
      "Epoch 266/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2759 - mae: 0.8711 - val_loss: 1.3617 - val_mae: 0.9043\n",
      "Epoch 267/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2798 - mae: 0.8732 - val_loss: 1.3620 - val_mae: 0.9023\n",
      "Epoch 268/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2727 - mae: 0.8714 - val_loss: 1.3602 - val_mae: 0.9039\n",
      "Epoch 269/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2782 - mae: 0.8730 - val_loss: 1.3583 - val_mae: 0.9005\n",
      "Epoch 270/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2732 - mae: 0.8707 - val_loss: 1.3610 - val_mae: 0.9063\n",
      "Epoch 271/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2784 - mae: 0.8742 - val_loss: 1.3598 - val_mae: 0.9011\n",
      "Epoch 272/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2764 - mae: 0.8727 - val_loss: 1.3587 - val_mae: 0.9005\n",
      "Epoch 273/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2755 - mae: 0.8713 - val_loss: 1.3561 - val_mae: 0.9030\n",
      "Epoch 274/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2734 - mae: 0.8725 - val_loss: 1.3579 - val_mae: 0.8992\n",
      "Epoch 275/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.2741 - mae: 0.8704 - val_loss: 1.3597 - val_mae: 0.9036\n",
      "Epoch 276/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2781 - mae: 0.8742 - val_loss: 1.3573 - val_mae: 0.9000\n",
      "Epoch 277/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2717 - mae: 0.8704 - val_loss: 1.3568 - val_mae: 0.9029\n",
      "Epoch 278/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2720 - mae: 0.8716 - val_loss: 1.3581 - val_mae: 0.8998\n",
      "Epoch 279/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2729 - mae: 0.8702 - val_loss: 1.3567 - val_mae: 0.9026\n",
      "Epoch 280/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2714 - mae: 0.8723 - val_loss: 1.3582 - val_mae: 0.9029\n",
      "Epoch 281/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2732 - mae: 0.8722 - val_loss: 1.3580 - val_mae: 0.9009\n",
      "Epoch 282/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2706 - mae: 0.8712 - val_loss: 1.3570 - val_mae: 0.9020\n",
      "Epoch 283/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2728 - mae: 0.8715 - val_loss: 1.3581 - val_mae: 0.8999\n",
      "Epoch 284/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2725 - mae: 0.8720 - val_loss: 1.3575 - val_mae: 0.9031\n",
      "Epoch 285/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2712 - mae: 0.8715 - val_loss: 1.3600 - val_mae: 0.8999\n",
      "Epoch 286/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2722 - mae: 0.8703 - val_loss: 1.3601 - val_mae: 0.9053\n",
      "Epoch 287/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2727 - mae: 0.8726 - val_loss: 1.3616 - val_mae: 0.9026\n",
      "Epoch 288/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2718 - mae: 0.8720 - val_loss: 1.3623 - val_mae: 0.9057\n",
      "Epoch 289/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2739 - mae: 0.8730 - val_loss: 1.3571 - val_mae: 0.9034\n",
      "Epoch 290/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2673 - mae: 0.8698 - val_loss: 1.3618 - val_mae: 0.9053\n",
      "Epoch 291/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2709 - mae: 0.8717 - val_loss: 1.3597 - val_mae: 0.9039\n",
      "Epoch 292/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2699 - mae: 0.8718 - val_loss: 1.3607 - val_mae: 0.9034\n",
      "Epoch 293/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2721 - mae: 0.8716 - val_loss: 1.3555 - val_mae: 0.9035\n",
      "Epoch 294/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2724 - mae: 0.8724 - val_loss: 1.3581 - val_mae: 0.9040\n",
      "Epoch 295/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2673 - mae: 0.8711 - val_loss: 1.3575 - val_mae: 0.9011\n",
      "Epoch 296/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2733 - mae: 0.8722 - val_loss: 1.3586 - val_mae: 0.9003\n",
      "Epoch 297/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2694 - mae: 0.8696 - val_loss: 1.3597 - val_mae: 0.9060\n",
      "Epoch 298/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2706 - mae: 0.8736 - val_loss: 1.3587 - val_mae: 0.9012\n",
      "Epoch 299/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2680 - mae: 0.8713 - val_loss: 1.3631 - val_mae: 0.9038\n",
      "Epoch 300/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2768 - mae: 0.8715 - val_loss: 1.3581 - val_mae: 0.9016\n",
      "Epoch 301/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2693 - mae: 0.8708 - val_loss: 1.3558 - val_mae: 0.9043\n",
      "Epoch 302/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2699 - mae: 0.8728 - val_loss: 1.3694 - val_mae: 0.9038\n",
      "Epoch 303/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.2728 - mae: 0.8717 - val_loss: 1.3639 - val_mae: 0.9105\n",
      "Epoch 304/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2731 - mae: 0.8750 - val_loss: 1.3583 - val_mae: 0.9003\n",
      "Epoch 305/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2696 - mae: 0.8713 - val_loss: 1.3857 - val_mae: 0.9140\n",
      "Epoch 306/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2891 - mae: 0.8772 - val_loss: 1.3850 - val_mae: 0.9157\n",
      "Epoch 307/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2880 - mae: 0.8777 - val_loss: 1.3669 - val_mae: 0.9018\n",
      "Epoch 308/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2813 - mae: 0.8732 - val_loss: 1.3588 - val_mae: 0.9100\n",
      "Epoch 309/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2711 - mae: 0.8745 - val_loss: 1.3638 - val_mae: 0.9016\n",
      "Epoch 310/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2716 - mae: 0.8686 - val_loss: 1.3569 - val_mae: 0.9070\n",
      "Epoch 311/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2707 - mae: 0.8731 - val_loss: 1.3617 - val_mae: 0.9035\n",
      "Epoch 312/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2702 - mae: 0.8696 - val_loss: 1.3585 - val_mae: 0.9092\n",
      "Epoch 313/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2757 - mae: 0.8779 - val_loss: 1.3576 - val_mae: 0.8987\n",
      "Epoch 314/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.2660 - mae: 0.8679 - val_loss: 1.3537 - val_mae: 0.9065\n",
      "Epoch 315/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2679 - mae: 0.8729 - val_loss: 1.3568 - val_mae: 0.9018\n",
      "Epoch 316/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.2628 - mae: 0.8685 - val_loss: 1.3585 - val_mae: 0.9055\n",
      "Epoch 317/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2688 - mae: 0.8741 - val_loss: 1.3569 - val_mae: 0.9034\n",
      "Epoch 318/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2648 - mae: 0.8709 - val_loss: 1.3537 - val_mae: 0.9030\n",
      "Epoch 319/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2629 - mae: 0.8694 - val_loss: 1.3542 - val_mae: 0.9036\n",
      "Epoch 320/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2662 - mae: 0.8706 - val_loss: 1.3580 - val_mae: 0.9029\n",
      "Epoch 321/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2588 - mae: 0.8705 - val_loss: 1.3547 - val_mae: 0.9048\n",
      "Epoch 322/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2657 - mae: 0.8720 - val_loss: 1.3522 - val_mae: 0.9000\n",
      "Epoch 323/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2632 - mae: 0.8686 - val_loss: 1.3533 - val_mae: 0.9013\n",
      "Epoch 324/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2631 - mae: 0.8693 - val_loss: 1.3559 - val_mae: 0.9014\n",
      "Epoch 325/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.2599 - mae: 0.8696 - val_loss: 1.3554 - val_mae: 0.9050\n",
      "Epoch 326/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2638 - mae: 0.8704 - val_loss: 1.3549 - val_mae: 0.9040\n",
      "Epoch 327/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2635 - mae: 0.8702 - val_loss: 1.3582 - val_mae: 0.9019\n",
      "Epoch 328/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2617 - mae: 0.8683 - val_loss: 1.3581 - val_mae: 0.9042\n",
      "Epoch 329/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2633 - mae: 0.8705 - val_loss: 1.3574 - val_mae: 0.9033\n",
      "Epoch 330/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2607 - mae: 0.8703 - val_loss: 1.3566 - val_mae: 0.9037\n",
      "Epoch 331/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2612 - mae: 0.8698 - val_loss: 1.3585 - val_mae: 0.9029\n",
      "Epoch 332/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2616 - mae: 0.8703 - val_loss: 1.3589 - val_mae: 0.9009\n",
      "Epoch 333/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2656 - mae: 0.8689 - val_loss: 1.3574 - val_mae: 0.9024\n",
      "Epoch 334/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2594 - mae: 0.8701 - val_loss: 1.3558 - val_mae: 0.9068\n",
      "Epoch 335/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2624 - mae: 0.8719 - val_loss: 1.3567 - val_mae: 0.9025\n",
      "Epoch 336/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2631 - mae: 0.8706 - val_loss: 1.3592 - val_mae: 0.9004\n",
      "Epoch 337/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2582 - mae: 0.8667 - val_loss: 1.3605 - val_mae: 0.9057\n",
      "Epoch 338/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2607 - mae: 0.8708 - val_loss: 1.3535 - val_mae: 0.9013\n",
      "Epoch 339/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2600 - mae: 0.8686 - val_loss: 1.3548 - val_mae: 0.9016\n",
      "Epoch 340/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2580 - mae: 0.8688 - val_loss: 1.3574 - val_mae: 0.9026\n",
      "Epoch 341/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2573 - mae: 0.8677 - val_loss: 1.3545 - val_mae: 0.9039\n",
      "Epoch 342/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2590 - mae: 0.8702 - val_loss: 1.3575 - val_mae: 0.9017\n",
      "Epoch 343/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2608 - mae: 0.8685 - val_loss: 1.3579 - val_mae: 0.9065\n",
      "Epoch 344/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2582 - mae: 0.8698 - val_loss: 1.3596 - val_mae: 0.9027\n",
      "Epoch 345/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2577 - mae: 0.8677 - val_loss: 1.3594 - val_mae: 0.9037\n",
      "Epoch 346/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2613 - mae: 0.8696 - val_loss: 1.3554 - val_mae: 0.8998\n",
      "Epoch 347/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.2556 - mae: 0.8665 - val_loss: 1.3634 - val_mae: 0.9097\n",
      "Epoch 348/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.2604 - mae: 0.8714 - val_loss: 1.3570 - val_mae: 0.9003\n",
      "Epoch 349/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2595 - mae: 0.8685 - val_loss: 1.3547 - val_mae: 0.9019\n",
      "Epoch 350/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2551 - mae: 0.8673 - val_loss: 1.3604 - val_mae: 0.9066\n",
      "Epoch 351/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2549 - mae: 0.8687 - val_loss: 1.3572 - val_mae: 0.9049\n",
      "Epoch 352/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2582 - mae: 0.8699 - val_loss: 1.3568 - val_mae: 0.9023\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 1.4863 - mae: 0.9542\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.013556832247938065)\n",
    "model = Sequential()\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(320, activation='relu', input_shape=(199,)))  # 278 is the number of input features\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(83, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "# model.add(Dropout(0.1))  # Optional, to prevent overfitting\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=9000, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "results = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f044505-a787-45b8-bbe4-4ef27ba1ca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4863 - mae: 0.9542 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.431609869003296, 0.9289669990539551]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42624e5-8839-48ae-b45c-2cec46cb406b",
   "metadata": {},
   "source": [
    "After rerunning the model with the optimized hyperparameters, we can observe a significant reduction in the prediction error. The **Mean Absolute Error (MAE)** has declined substantially, reaching a value of around **0.92** on the test set. This result is a clear indication that the hyperparameter tuning process has greatly improved the model’s performance.\n",
    "\n",
    "### Conclusion on Model Performance\n",
    "\n",
    "The reduction in MAE demonstrates that the optimized architecture is well-suited for this type of football match prediction task. Achieving an MAE of around **0.92** suggests that, on average, the model’s predictions are quite close to the actual goals scored by both teams. This level of accuracy is a considerable improvement from our initial attempts and represents a strong result for such a complex and unpredictable problem.\n",
    "\n",
    "### Challenges and Random Factors\n",
    "\n",
    "While this value of **0.92** is close to optimal for our model, it is important to recognize the inherent challenges of this task:\n",
    "- Football matches are influenced by a variety of random factors such as team morale, unexpected events (like injuries or red cards), and form fluctuations that are difficult to capture in data-driven models.\n",
    "- Human elements, such as players' physical condition and managerial decisions, add unpredictability to the game, making it extremely challenging to obtain more accurate predictions through modeling alone.\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Given the complexity of the task and the various random factors affecting match outcomes, further reducing the MAE beyond **0.92** would be immensely difficult without introducing additional, high-quality data or advanced techniques. However, the current results demonstrate that our model has achieved a reliable level of predictive accuracy, making it a strong tool for predicting goals in football matches.\n",
    "\n",
    "We can now confidently proceed with this model, knowing that it is well-tuned for the task at hand, and further attempts to improve it may yield diminishing returns due to the random nature of the domain. Let's save the model for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f171f85-16ec-4083-8c99-1ac1c9288ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5500 - mae: 0.9698  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4403669834136963, 0.9361046552658081]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save(\"model_fully_optimal.keras\")\n",
    "from tensorflow.keras.models import load_model\n",
    "# # \n",
    "model = load_model(\"model_fully_optimal.keras\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d20d8-83ad-4b42-826d-e1b61ddc7bd0",
   "metadata": {},
   "source": [
    "### Matching Team Names and Over/Under 2.5 Goals Odds to Fixtures\r\n",
    "\r\n",
    "For the test dataset, the next step involves matching the team names and over/under 2.5 goals odds to the corresponding fixtures. This is crucial because it allows us to align our predictions with the actual match data, so we know exactly what we are dealing with for each game.\r\n",
    "\r\n",
    "To do this, we will:\r\n",
    "\r\n",
    "1. **Match Team Names**: \r\n",
    "   First, we will ensure that the team names in the test set align with those in the training set and the original datasets. This step is important to avoid discrepancies in naming conventions (e.g., abbreviations, misspellings). Any necessary transformations or mappings will be applied here.\r\n",
    "\r\n",
    "2. **Link Over/Under 2.5 Goals Odds**:\r\n",
    "   The next task is to incorporate the over/under 2.5 goals odds into the test dataset. These odds are key to determining whether the bookmaker expects a high-scoring game (over 2.5 goals) or a low-scoring one (under 2.5 goals). By merging the odds with the test data, we will be able to directly compare our goal predictions with the bookmaker’s odds.\r\n",
    "\r\n",
    "### Summing Predicted Goals and Comparing with Actual Results\r\n",
    "\r\n",
    "Once we have aligned the team names and merged the odds, we can proceed to calculate the predicted total goals for each match and compare it with the actual results.\r\n",
    "\r\n",
    "1. **Summing Predicted Goals**:\r\n",
    "   After obtaining the predictions for goals scored by both teams, we will sum the predicted goals for each match to get a total goal prediction. This total will be used to determine whether the predicted number of goals is higher or lower than 2.5, matching the over/under 2.5 goal betting line.\r\n",
    "\r\n",
    "2. **Comparison with Actual Results**:\r\n",
    "   For each fixture in the test set, we will then compare:\r\n",
    "   - The **predicted total goals** with the **actual total goals** scored in the match.\r\n",
    "   - The bookmaker's **over/under 2.5 odds** with our prediction of whether the total goals will be over or under 2.5.\r\n",
    "\r\n",
    "This comparison allows us to evaluate the performance of our model in terms of correctly predicting the total goals scored and how well our predictions align wiareas for potential improvement.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8e9dd-3fc0-4762-958b-cf133ad58382",
   "metadata": {},
   "source": [
    "Additionally, we need to find a way to make the model useful in gambling and figure out how to incorporate it into betting on matches so that it generates profit. This involves developing a strategy where the model's predictions can be compared with bookmaker odds to identify profitable opportunities. By leveraging the model’s predictions, we aim to place smart bets, maximize returns, and manage risks, ultimately using it to gain an edge in the betting market.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "355c6b66-750c-409d-a6d5-5aded1e1ca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "res = pd.DataFrame(y_pred, columns = ['FTHG', 'FTAG'])\n",
    "\n",
    "over_under2 = pd.read_csv('2_5_goals.csv')[-550:-40]\n",
    "over_under2.reset_index(inplace = True, drop = True)\n",
    "\n",
    "trg = target[-550:-40]\n",
    "trg.reset_index(inplace = True, drop = True)\n",
    "next_fixtures2 = pd.read_csv('next_fixture_teams.csv')[-550:-40]\n",
    "next_fixtures2.reset_index(inplace = True, drop = True)\n",
    "over_under2 = pd.read_csv('2_5_goals.csv')[-700:-40]\n",
    "over_under2.reset_index(inplace = True, drop = True)\n",
    "next_fixtures3 = pd.merge(next_fixtures2[['HomeTeam',\t'AwayTeam']], over_under2, on =['HomeTeam',\t'AwayTeam'], how = 'left')\n",
    "next_fixtures3.drop_duplicates(['HomeTeam',\t'AwayTeam'],inplace= True, keep = 'last')\n",
    "\n",
    "next_fixtures2[['FTHG', 'FTAG']] = y_pred\n",
    "next_fixtures2['goals_total_pred'] = next_fixtures2['FTHG']+next_fixtures2['FTAG']\n",
    "next_fixtures2[['goals_home', 'goals_away']] = trg[['FTHG', 'FTAG']]\n",
    "next_fixtures2['total_goals_scored'] = next_fixtures2['goals_home']+next_fixtures2['goals_away']\n",
    "\n",
    "next_fixtures4 = pd.merge(next_fixtures2, next_fixtures3, on =['HomeTeam',\t'AwayTeam'], how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30f54d44-990a-496b-9782-c460e6de29df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>Date</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>goals_total_pred</th>\n",
       "      <th>goals_home</th>\n",
       "      <th>goals_away</th>\n",
       "      <th>total_goals_scored</th>\n",
       "      <th>Avg&gt;2.5</th>\n",
       "      <th>Avg&lt;2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mirandes</td>\n",
       "      <td>Valladolid</td>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>1.125890</td>\n",
       "      <td>1.326753</td>\n",
       "      <td>2.452643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huesca</td>\n",
       "      <td>Oviedo</td>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>1.228806</td>\n",
       "      <td>1.100068</td>\n",
       "      <td>2.328874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Santander</td>\n",
       "      <td>Elche</td>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>1.354604</td>\n",
       "      <td>1.418217</td>\n",
       "      <td>2.772822</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cartagena</td>\n",
       "      <td>Alcorcon</td>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>1.279109</td>\n",
       "      <td>0.873631</td>\n",
       "      <td>2.152740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.66</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>1.605459</td>\n",
       "      <td>0.939871</td>\n",
       "      <td>2.545331</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    HomeTeam    AwayTeam        Date      FTHG      FTAG  goals_total_pred  \\\n",
       "0   Mirandes  Valladolid  2024-05-04  1.125890  1.326753          2.452643   \n",
       "1     Huesca      Oviedo  2024-05-04  1.228806  1.100068          2.328874   \n",
       "2  Santander       Elche  2024-05-04  1.354604  1.418217          2.772822   \n",
       "3  Cartagena    Alcorcon  2024-05-04  1.279109  0.873631          2.152740   \n",
       "4   Dortmund    Augsburg  2024-05-04  1.605459  0.939871          2.545331   \n",
       "\n",
       "   goals_home  goals_away  total_goals_scored  Avg>2.5  Avg<2.5  \n",
       "0         0.0         1.0                 1.0     2.45     1.52  \n",
       "1         0.0         2.0                 2.0     2.84     1.40  \n",
       "2         3.0         1.0                 4.0     2.01     1.77  \n",
       "3         1.0         0.0                 1.0     2.66     1.45  \n",
       "4         5.0         1.0                 6.0     1.32     3.33  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_fixtures4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9326c-0db8-448a-8e3f-0965e830c93b",
   "metadata": {},
   "source": [
    "### Betting Strategy: Over/Under 2.5 Goals Line\n",
    "\n",
    "The primary strategy we are implementing revolves around using the **over/under 2.5 goals betting line** to estimate how much profit we could generate by betting on either side of the market before each match. In this approach, we aim to test whether consistently betting on **over 2.5 goals** or **under 2.5 goals** yields a profit over time.\n",
    "\n",
    "#### Assumptions and Betting Mechanics\n",
    "\n",
    "For each bet, we assume a **fixed stake of 1 unit**. The calculation of potential winnings is as follows:\n",
    "\n",
    "1. **Over 2.5 Goals Bet**:\n",
    "   - If a match ends with **more than 2.5 goals** (i.e., 3 or more goals are scored), we win the bet.\n",
    "   - Our winnings for a successful bet on over 2.5 goals are calculated using the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d104007-266e-419f-9f3c-af5f2be1f514",
   "metadata": {},
   "source": [
    "$ \\text{Winnings} = 1 \\times \\text{Over 2.5 odds} \\times 0.88$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad4a2ad-7e33-4cc4-adaf-a61dd76a95b1",
   "metadata": {},
   "source": [
    "Here, the **over 2.5 odds** represent the bookmaker's odds for the bet, and the factor of **0.88** accounts for tax deductions (assuming an 12% tax on winnings). If our bet wins, this formula gives us the payout based on the odds provided by the bookmaker.\n",
    "\n",
    "2. **Under 2.5 Goals Bet**:\n",
    "   - Conversely, if we decide to bet on **under 2.5 goals**, and the match ends with **fewer than 2.5 goals** (i.e., 0, 1, or 2 goals are scored), the same formula applies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766c833-3a98-4d10-9e26-a115a52aed50",
   "metadata": {},
   "source": [
    "$\\text{Winnings} = 1 \\times \\text{Under 2.5 odds} \\times 0.88 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0187da-69ab-40fe-9778-7c8b6b2c6fe0",
   "metadata": {},
   "source": [
    "Just like with the over 2.5 goals bet, this formula calculates the payout if our bet on under 2.5 goals is successful.\n",
    "\n",
    "3. **No Payout for Losing Bets**:\n",
    "   - In cases where the result of the match does not align with our bet, the payout is 0. For example, if we bet on over 2.5 goals and the match ends with fewer than 2.5 goals, or we bet on under 2.5 goals and more than 2.5 goals are scored, the payout is:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace49f0-a68a-42e7-9d51-2194f75ee3ee",
   "metadata": {},
   "source": [
    "$\\text{Winnings} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312b1e7-19e7-415e-b166-28ae5684418c",
   "metadata": {},
   "source": [
    "This simple win/loss dynamic ensures that our betting approach is straightforward, and it allows us to evaluate how often the model's predictions align with the actual match outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35691cb9-cfee-4841-9f10-e6c2a665ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def money_won_goals_over(row, goals):\n",
    "    if row['goals_total_pred'] > goals:\n",
    "        if row['total_goals_scored'] >2.5:\n",
    "             return row['Avg>2.5']\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def money_won_goals_under(row, goals):\n",
    "    if row['goals_total_pred'] < goals:\n",
    "        if row['total_goals_scored'] <2.5:\n",
    "             return row['Avg<2.5']\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ae11a-6a43-4c2f-95d2-a18a21223c0b",
   "metadata": {},
   "source": [
    "### Strategy to Profit from the Model: Over 2.5 Goals Betting Example\n",
    "\n",
    "The way we aim to profit from this model is somewhat complex, as it involves multiple layers of decision-making and parameter tuning. Let’s break down the process, using **betting on over 2.5 goals** as an example.\n",
    "\n",
    "#### Step 1: Setting Thresholds for Model Predictions and Odds\n",
    "\n",
    "To implement this strategy, we first need to propose two threshold values:\n",
    "\n",
    "1. **Threshold for predicted goals (t1):** This value is based on the model’s predictions for the number of goals scored in a match. The range of possible values for this threshold is approximately **t1 ~ [0.5, 4.5]**. This means we only place a bet on over 2.5 goals if the predicted goals by the model for a match exceed this threshold.\n",
    "   \n",
    "2. **Threshold for over 2.5 odds (t2):** This is the minimum value for the bookmaker’s odds for betting on over 2.5 goals. The range for this threshold is **t2 ~ [1.4, 2.6]**, meaning we only bet if the bookmaker’s odds for over 2.5 goals exceed this threshold.\n",
    "\n",
    "#### Step 2: Filtering Matches Based on Thresholds\n",
    "\n",
    "For each combination of these thresholds, **t1** and **t2**, we filter the matches to only include those where both conditions are met:\n",
    "- The model predicts **more than t1 goals** for the match.\n",
    "- The bookmaker’s odds for **over 2.5 goals** are greater than **t2**.\n",
    "\n",
    "By varying the thresholds, we create different groups of matches. Each group represents a set of matches where both the predicted goals and the odds exceed the respective thresholds. These groups will form the basis for our betting strategy.\n",
    "\n",
    "#### Step 3: Calculating Yield and Money Won\n",
    "\n",
    "For each group of matches that meets the conditions (i.e., predicted goals > t1 and odds > t2), we calculate two key metrics:\n",
    "1. **Yield:** This is a common measure in gambling used to assess profitability. The **yield** is calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2680ad11-5e74-4442-86d6-cfc71d1d6d07",
   "metadata": {},
   "source": [
    "$\\text{Yield} = \\frac{\\text{Total Money Won from All Matches}}{\\text{Total Stake (Number of Bets)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a416ba-1383-4916-9db2-c25b33da31f6",
   "metadata": {},
   "source": [
    " In simpler terms, the yield represents the average return on each bet placed in that group of matches. A yield greater than 1 indicates that the strategy is profitable, while a yield less than 1 suggests a loss.\n",
    "\n",
    "   - **Explanation of Yield in Gambling:** The yield is a key performance indicator in gambling. It reflects how much profit or loss is made relative to the total amount wagered. A yield of 1 would mean breaking even (i.e., no profit or loss). For example, a yield of 1.10 would mean a 10% profit on the total amount wagered, while a yield of 0.90 would indicate a 10% loss.\n",
    "\n",
    "2. **Total Money Won:** The amount of money won for each group of matches is calculated using the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6cb42-260a-41a1-b3b1-a9e068662163",
   "metadata": {},
   "source": [
    "$\\text{Total Money Won} = (\\text{Yield} - 1) \\times \\text{Number of Matches in the Group}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfafd9b2-51f1-48da-8ec6-3b03eb0622c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "   This calculation gives us the overall profit or loss from betting on the matches that meet the threshold conditions for **t1** and **t2**. The total stake in each case is the number of matches in that group (since we assume a stake of 1 unit per match).\n",
    "\n",
    "#### Example of Calculation:\n",
    "\n",
    "Let’s assume we have the following thresholds:\n",
    "- t1 = 2.8 (the model predicts more than 2.8 goals)\n",
    "- t2 = 1.8 (the bookmaker’s odds for over 2.5 goals are greater than 1.8)\n",
    "\n",
    "We filter the matches to include only those where the model’s predicted goals exceed 2.8 and the odds for over 2.5 goals are greater than 1.8. For this group, we calculate the total money won and yield.\n",
    "\n",
    "If, after evaluating all matches that meet these criteria, the yield is 1.15, this means that on average, we make a 15% profit on each bet placed in this group. If 100 matches meet the conditions, the total money won would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4b1e4-0fa9-41a6-8a30-18e1c11980c2",
   "metadata": {},
   "source": [
    "$\\text{Total Money Won} = (1.15 - 1) \\times 100 = 0.15 \\times 100 = 15 \\text{ units of profit}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208faed0-6fba-4b5b-8209-ac44e6e65dad",
   "metadata": {},
   "source": [
    "#### Step 4: Testing and Optimizing Thresholds\n",
    "\n",
    "To maximize profitability, we will test different combinations of **t1** and **t2** across a range of values. By analyzing the yield and total money won for each combination, we aim to identify the optimal thresholds that maximize long-term profit. This process allows us to fine-tune the model to generate consistent returns from betting on over 2.5 goals.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The overall strategy involves filtering matches based on thresholds for predicted goals and bookmaker odds, and then calculating the yield and profit for each group of matches that meets the criteria. By testing various threshold combinations, we can refine our betting strategy to achieve the highest possible yield, thereby increasing profitability in the long run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ffae771c-6cce-4396-bb3a-f1adbcc16df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_1 = np.linspace(0.5,4.5,81)\n",
    "range_2 = np.linspace(1.4,2.6,61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2d692082-c7d9-4977-baaa-5594acb77f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_list = []\n",
    "winnings_list = []\n",
    "yield_list = []\n",
    "i_s = []\n",
    "j_s = []\n",
    "money_won_over_all_matches = []\n",
    "for i in range_1:\n",
    "    for j in range_2:\n",
    "        next_fixtures4['stake'] = next_fixtures4.apply(money_won_goals_over, axis=1,args=(i,))\n",
    "        no_of_matches = len(next_fixtures4.loc[(next_fixtures4['goals_total_pred'] > i) & (next_fixtures4['Avg>2.5']>j)])\n",
    "        winnings = next_fixtures4.loc[(next_fixtures4['goals_total_pred'] > i) & (next_fixtures4['Avg>2.5']>j)]['stake'].sum()*0.88\n",
    "        if no_of_matches != 0:\n",
    "            yield_ratio = winnings / no_of_matches\n",
    "        else:\n",
    "            yield_ratio = 0\n",
    "        matches_list.append(no_of_matches)\n",
    "        winnings_list.append(winnings)\n",
    "        yield_list.append(yield_ratio)\n",
    "        i_s.append(i)\n",
    "        j_s.append(j)\n",
    "        money_won_over_all_matches.append((yield_ratio-1)*no_of_matches)\n",
    "results_df_over = pd.DataFrame({'goals_total_over':i_s,\n",
    "                                'over_2_5_val': j_s,\n",
    "                                'number_of_matches': matches_list,\n",
    "                                'money_won': winnings_list,\n",
    "                                'yield':yield_list,\n",
    "                                'money_won_over_all_matches':money_won_over_all_matches\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4d45464-0ea5-4701-ad40-d4f1aafcaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_conditions = results_df_over.sort_values('money_won_over_all_matches', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45fcdb8-b47e-40d5-b8ce-4e7cf521f340",
   "metadata": {},
   "source": [
    "### Practical Example of Calculations\n",
    "\n",
    "In reality, after running our model and applying the thresholds, we can organize the results into a table. The table lists various combinations of predicted goals (t1) and over 2.5 odds (t2), and is **sorted by the highest amount of money won for each group** of matches. The key metrics displayed in this table include the **number of matches**, the **yield**, and the **total money won** for each group that meets the respective threshold conditions.\n",
    "\n",
    "#### Observations:\n",
    "\n",
    "As we analyze the table, we can notice several interesting patterns:\n",
    "- **Substantial Number of Groups with Yield > 1**: A considerable number of match groups demonstrate a **yield greater than 1**, indicating that these groups are profitable. A yield higher than 1 means that, on average, the bets placed on these matches resulted in a positive return. This suggests that the model is successfully identifying profitable opportunities based on the combination of predicted goals and the offered odds.\n",
    "  \n",
    "- **Significant Total Money Won**: In addition to the positive yield, many of these groups show a **total money won** that exceeds **2 units**. This means that, for those specific combinations of thresholds, the model's predictions not only resulted in positive returns but also generated a meaningful amount of profit.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "For instance, consider one group where:\n",
    "- The **predicted goals threshold (t1)** is set at 2.7, meaning we only bet on matches where the model predicts more than 2.7 goals.\n",
    "- The **over 2.5 odds threshold (t2)** is set at 2.0, meaning we only place a bet if the odds for over 2.5 goals exceed 2.0.\n",
    "\n",
    "If this group contains 28 matches, with a yield of 1.115 and total money won equal to 3.24 units, this would imply:\n",
    "- A **11.5% average profit** on the amount staked across all 28 matches.\n",
    "- The **total profit** for this group of matches is **3.24 units**, meaning we made 3.24 units of profit from betting on these matches with the given thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dd1b5b00-6c35-4497-82c6-c8859e90e411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goals_total_over</th>\n",
       "      <th>over_2_5_val</th>\n",
       "      <th>number_of_matches</th>\n",
       "      <th>money_won</th>\n",
       "      <th>yield</th>\n",
       "      <th>money_won_over_all_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.74</td>\n",
       "      <td>13</td>\n",
       "      <td>16.5440</td>\n",
       "      <td>1.272615</td>\n",
       "      <td>3.5440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>2.70</td>\n",
       "      <td>2.00</td>\n",
       "      <td>28</td>\n",
       "      <td>31.2400</td>\n",
       "      <td>1.115714</td>\n",
       "      <td>3.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>2.65</td>\n",
       "      <td>2.00</td>\n",
       "      <td>34</td>\n",
       "      <td>37.1096</td>\n",
       "      <td>1.091459</td>\n",
       "      <td>3.1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>12</td>\n",
       "      <td>15.0040</td>\n",
       "      <td>1.250333</td>\n",
       "      <td>3.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>2.65</td>\n",
       "      <td>2.06</td>\n",
       "      <td>27</td>\n",
       "      <td>29.9464</td>\n",
       "      <td>1.109126</td>\n",
       "      <td>2.9464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>3.25</td>\n",
       "      <td>1.72</td>\n",
       "      <td>12</td>\n",
       "      <td>14.8808</td>\n",
       "      <td>1.240067</td>\n",
       "      <td>2.8808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>3.25</td>\n",
       "      <td>1.74</td>\n",
       "      <td>12</td>\n",
       "      <td>14.8808</td>\n",
       "      <td>1.240067</td>\n",
       "      <td>2.8808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>3.25</td>\n",
       "      <td>1.68</td>\n",
       "      <td>18</td>\n",
       "      <td>20.8736</td>\n",
       "      <td>1.159644</td>\n",
       "      <td>2.8736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.72</td>\n",
       "      <td>14</td>\n",
       "      <td>16.5440</td>\n",
       "      <td>1.181714</td>\n",
       "      <td>2.5440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.68</td>\n",
       "      <td>20</td>\n",
       "      <td>22.5368</td>\n",
       "      <td>1.126840</td>\n",
       "      <td>2.5368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>11</td>\n",
       "      <td>13.4464</td>\n",
       "      <td>1.222400</td>\n",
       "      <td>2.4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11</td>\n",
       "      <td>13.4464</td>\n",
       "      <td>1.222400</td>\n",
       "      <td>2.4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.80</td>\n",
       "      <td>11</td>\n",
       "      <td>13.4464</td>\n",
       "      <td>1.222400</td>\n",
       "      <td>2.4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.82</td>\n",
       "      <td>11</td>\n",
       "      <td>13.4464</td>\n",
       "      <td>1.222400</td>\n",
       "      <td>2.4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.84</td>\n",
       "      <td>11</td>\n",
       "      <td>13.4464</td>\n",
       "      <td>1.222400</td>\n",
       "      <td>2.4464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      goals_total_over  over_2_5_val  number_of_matches  money_won     yield  \\\n",
       "3311              3.20          1.74                 13    16.5440  1.272615   \n",
       "2714              2.70          2.00                 28    31.2400  1.115714   \n",
       "2653              2.65          2.00                 34    37.1096  1.091459   \n",
       "3312              3.20          1.76                 12    15.0040  1.250333   \n",
       "2656              2.65          2.06                 27    29.9464  1.109126   \n",
       "3371              3.25          1.72                 12    14.8808  1.240067   \n",
       "3372              3.25          1.74                 12    14.8808  1.240067   \n",
       "3369              3.25          1.68                 18    20.8736  1.159644   \n",
       "3310              3.20          1.72                 14    16.5440  1.181714   \n",
       "3308              3.20          1.68                 20    22.5368  1.126840   \n",
       "3313              3.20          1.78                 11    13.4464  1.222400   \n",
       "3317              3.20          1.86                 11    13.4464  1.222400   \n",
       "3314              3.20          1.80                 11    13.4464  1.222400   \n",
       "3315              3.20          1.82                 11    13.4464  1.222400   \n",
       "3316              3.20          1.84                 11    13.4464  1.222400   \n",
       "\n",
       "      money_won_over_all_matches  \n",
       "3311                      3.5440  \n",
       "2714                      3.2400  \n",
       "2653                      3.1096  \n",
       "3312                      3.0040  \n",
       "2656                      2.9464  \n",
       "3371                      2.8808  \n",
       "3372                      2.8808  \n",
       "3369                      2.8736  \n",
       "3310                      2.5440  \n",
       "3308                      2.5368  \n",
       "3313                      2.4464  \n",
       "3317                      2.4464  \n",
       "3314                      2.4464  \n",
       "3315                      2.4464  \n",
       "3316                      2.4464  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_over.sort_values('money_won_over_all_matches', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "id": "642a5188-b936-46fc-9a09-efb90e2fab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_conditions.to_csv('over_conditions.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568c690-6e6f-4ab7-8ec2-d998f9616d8f",
   "metadata": {},
   "source": [
    "### Maximizing Matches and Profits\r\n",
    "\r\n",
    "To further enhance the betting strategy, we can increase the number of matches available for betting by focusing on the **15 most profitable groups** identified from our analysis. These groups represent the best combinations of predicted goals (t1) and over 2.5 odds (t2), which generated the highest yields and total profits.\r\n",
    "\r\n",
    "#### Overall Profitability\r\n",
    "\r\n",
    "When we combine the results from these 15 most profitable groups, the **overall profitability** comes out to be **4.83** units. This means that, across all the matches considered in these groups, if we had placed a **1-unit stake on each match**, by the end of the analyzed period, we would have accumulated a profit of **4.83 units**.\r\n",
    "\r\n",
    "In other words:\r\n",
    "- **Total number of bets placed:** One unit per match for each match in the top 15 groups.\r\n",
    "- **Overall profit:** After summing up the wins and losses from all these bets, we are left with 4.83 units of profit.\r\n",
    "\r\n",
    "This suggests that the combination of thresholds for these 15 groups has proven to be particularly effective, yielding consistent profits across a larger set of matches.\r\n",
    "\r\n",
    "#### Why This Matters:\r\n",
    "\r\n",
    "Maximizing the number of matches available to bet on, while still maintaining profitability, is crucial for generating sustained returns. By focusing on the top 15 groups:\r\n",
    "- We ensure that our model is not overly restrictive, allowing us to place bets on a substantial number of matches.\r\n",
    "- At the same time, we maintain a profitable betting strategy, as shown by the 4.83-unit gain, ensuring that the overall approach remains successful in the long term.\r\n",
    "\r\n",
    "This balance between maximizing the number of opportunities and ensuring profitability is key to a robust and scalable betting strategy.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c372281-5dea-41fb-b14d-0c3f520140e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ciesl\\AppData\\Local\\Temp\\ipykernel_36744\\1991751933.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_considered_matches_over = pd.concat([all_considered_matches_over, matches_to_add])\n"
     ]
    }
   ],
   "source": [
    "all_considered_matches_over = pd.DataFrame(columns = next_fixtures4.columns)\n",
    "for index, row in over_conditions.iterrows():\n",
    "    next_fixtures4['stake'] = next_fixtures4.apply(money_won_goals_over, axis=1,args=(row['goals_total_over'],))\n",
    "    matches_to_add = next_fixtures4.loc[(next_fixtures4['goals_total_pred']>row['goals_total_over']) & (next_fixtures4['Avg>2.5']>row['over_2_5_val'])]\n",
    "    all_considered_matches_over = pd.concat([all_considered_matches_over, matches_to_add])\n",
    "all_considered_matches_over.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b1fb7c06-ee15-48b6-971b-c0de9a3cb3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.833599999999997"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_considered_matches_over['stake'].sum()*0.88/len(all_considered_matches_over)-1)*len(all_considered_matches_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e6fd5-69d7-4aaa-bc16-22c7e0f43861",
   "metadata": {},
   "source": [
    "### Betting on Under 2.5 Goals\r\n",
    "\r\n",
    "The same reasoning applies when it comes to betting on **under 2.5 goals** in matches. In this case, while there are **fewer matches** that meet the conditions for betting, the **accumulated yield** is significantly higher, making this strategy highly profitable.\r\n",
    "\r\n",
    "#### Higher Yield with Fewer Matches\r\n",
    "\r\n",
    "Although the number of potential matches to bet on is smaller compared to over 2.5 goals, the return on investment for these bets is more substantial. After analyzing the top groups for under 2.5 goals betting, we observe that the **total yield** across all matches leads to a profit of over **11 units**.\r\n",
    "\r\n",
    "This means:\r\n",
    "- **Fewer betting opportunities:** Since the under 2.5 goals line typically applies to lower-scoring games, the number of matches where the odds and predicted goals align with our thresholds is naturally reduced.\r\n",
    "- **Higher profitability per bet:** Despite the smaller number of matches, the higher yield indicates that these bets are more reliable and profitable on average.\r\n",
    "\r\n",
    "#### Total Profit:\r\n",
    "\r\n",
    "With this strategy, if we had placed a **1-unit stake on each match** in the qualifying groups, the **total profit** across all matches would be **11 units**. This demonstrates that focusing on under 2.5 goals betting, even with fewer matches, can result in a more **lucrative outcome** compared to over 2.5 goer 2.5 goals markets.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71dfd898-4417-4a41-adeb-d1774db74e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_list_under = []\n",
    "winnings_list_under = []\n",
    "yield_list_under = []\n",
    "i_s_under = []\n",
    "j_s_under = []\n",
    "money_won_over_all_matches_under = []\n",
    "for i in range_1:\n",
    "    for j in range_2:\n",
    "        next_fixtures4['stake'] = next_fixtures4.apply(money_won_goals_under, axis=1,args=(i,))\n",
    "        no_of_matches_under = len(next_fixtures4.loc[(next_fixtures4['goals_total_pred'] < i) & (next_fixtures4['Avg<2.5']>j)])\n",
    "        winnings_under = next_fixtures4.loc[(next_fixtures4['goals_total_pred'] < i) & (next_fixtures4['Avg<2.5']>j)]['stake'].sum()*0.88\n",
    "        if no_of_matches_under != 0:\n",
    "            yield_ratio_under = winnings_under / no_of_matches_under\n",
    "        else:\n",
    "            yield_ratio_under = 0\n",
    "        matches_list_under.append(no_of_matches_under)\n",
    "        winnings_list_under.append(winnings_under)\n",
    "        yield_list_under.append(yield_ratio_under)\n",
    "        i_s_under.append(i)\n",
    "        j_s_under.append(j)\n",
    "        money_won_over_all_matches_under.append((yield_ratio_under-1)*no_of_matches_under)\n",
    "results_df_under = pd.DataFrame({'goals_total_under':i_s_under,\n",
    "                                'under_2_5_val': j_s_under,\n",
    "                                'number_of_matches': matches_list_under,\n",
    "                                'money_won': winnings_list_under,\n",
    "                                'yield':yield_list_under,\n",
    "                                'money_won_over_all_matches':money_won_over_all_matches_under\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6b1a5bfa-70e4-4e35-bcbe-65f42cf453d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_conditions = results_df_under.sort_values('money_won_over_all_matches', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d7320e86-c2a8-46d0-9588-3ee8aea861f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goals_total_under</th>\n",
       "      <th>under_2_5_val</th>\n",
       "      <th>number_of_matches</th>\n",
       "      <th>money_won</th>\n",
       "      <th>yield</th>\n",
       "      <th>money_won_over_all_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.32</td>\n",
       "      <td>35</td>\n",
       "      <td>46.7808</td>\n",
       "      <td>1.336594</td>\n",
       "      <td>11.7808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.34</td>\n",
       "      <td>35</td>\n",
       "      <td>46.7808</td>\n",
       "      <td>1.336594</td>\n",
       "      <td>11.7808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.36</td>\n",
       "      <td>35</td>\n",
       "      <td>46.7808</td>\n",
       "      <td>1.336594</td>\n",
       "      <td>11.7808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.38</td>\n",
       "      <td>34</td>\n",
       "      <td>44.6952</td>\n",
       "      <td>1.314565</td>\n",
       "      <td>10.6952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.40</td>\n",
       "      <td>33</td>\n",
       "      <td>42.5920</td>\n",
       "      <td>1.290667</td>\n",
       "      <td>9.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.42</td>\n",
       "      <td>31</td>\n",
       "      <td>40.4712</td>\n",
       "      <td>1.305523</td>\n",
       "      <td>9.4712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.44</td>\n",
       "      <td>31</td>\n",
       "      <td>40.4712</td>\n",
       "      <td>1.305523</td>\n",
       "      <td>9.4712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.46</td>\n",
       "      <td>29</td>\n",
       "      <td>38.3064</td>\n",
       "      <td>1.320910</td>\n",
       "      <td>9.3064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2.34</td>\n",
       "      <td>27</td>\n",
       "      <td>35.8688</td>\n",
       "      <td>1.328474</td>\n",
       "      <td>8.8688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2.32</td>\n",
       "      <td>27</td>\n",
       "      <td>35.8688</td>\n",
       "      <td>1.328474</td>\n",
       "      <td>8.8688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      goals_total_under  under_2_5_val  number_of_matches  money_won  \\\n",
       "2608               2.60           2.32                 35    46.7808   \n",
       "2609               2.60           2.34                 35    46.7808   \n",
       "2610               2.60           2.36                 35    46.7808   \n",
       "2611               2.60           2.38                 34    44.6952   \n",
       "2612               2.60           2.40                 33    42.5920   \n",
       "2613               2.60           2.42                 31    40.4712   \n",
       "2614               2.60           2.44                 31    40.4712   \n",
       "2615               2.60           2.46                 29    38.3064   \n",
       "2548               2.55           2.34                 27    35.8688   \n",
       "2547               2.55           2.32                 27    35.8688   \n",
       "\n",
       "         yield  money_won_over_all_matches  \n",
       "2608  1.336594                     11.7808  \n",
       "2609  1.336594                     11.7808  \n",
       "2610  1.336594                     11.7808  \n",
       "2611  1.314565                     10.6952  \n",
       "2612  1.290667                      9.5920  \n",
       "2613  1.305523                      9.4712  \n",
       "2614  1.305523                      9.4712  \n",
       "2615  1.320910                      9.3064  \n",
       "2548  1.328474                      8.8688  \n",
       "2547  1.328474                      8.8688  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_under.sort_values('money_won_over_all_matches', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "id": "6695c12d-5583-4e2a-9fc7-324195f443ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_conditions.to_csv('under_conditions.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9e1a261d-83c6-48e5-8c97-dbe5f5b46138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ciesl\\AppData\\Local\\Temp\\ipykernel_36744\\3657945737.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_considered_matches_under = pd.concat([all_considered_matches_under, matches_to_add])\n"
     ]
    }
   ],
   "source": [
    "all_considered_matches_under = pd.DataFrame(columns = next_fixtures4.columns)\n",
    "for index, row in under_conditions.iterrows():\n",
    "    next_fixtures4['stake'] = next_fixtures4.apply(money_won_goals_under, axis=1,args=(row['goals_total_under'],))\n",
    "    matches_to_add = next_fixtures4.loc[(next_fixtures4['goals_total_pred']<row['goals_total_under']) & (next_fixtures4['Avg<2.5']>row['under_2_5_val'])]\n",
    "    all_considered_matches_under = pd.concat([all_considered_matches_under, matches_to_add])\n",
    "all_considered_matches_under.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7e2fa97-b348-4ba4-b5cc-216c28eb2474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.780800000000005"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_considered_matches_under['stake'].sum()*0.88/len(all_considered_matches_under)-1)*len(all_considered_matches_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dc5b0f22-fa1a-48ad-87bb-51f7e0f1d263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goals_total_under</th>\n",
       "      <th>under_2_5_val</th>\n",
       "      <th>number_of_matches</th>\n",
       "      <th>money_won</th>\n",
       "      <th>yield</th>\n",
       "      <th>money_won_over_all_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.32</td>\n",
       "      <td>35</td>\n",
       "      <td>46.7808</td>\n",
       "      <td>1.336594</td>\n",
       "      <td>11.7808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.34</td>\n",
       "      <td>35</td>\n",
       "      <td>46.7808</td>\n",
       "      <td>1.336594</td>\n",
       "      <td>11.7808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.36</td>\n",
       "      <td>35</td>\n",
       "      <td>46.7808</td>\n",
       "      <td>1.336594</td>\n",
       "      <td>11.7808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.38</td>\n",
       "      <td>34</td>\n",
       "      <td>44.6952</td>\n",
       "      <td>1.314565</td>\n",
       "      <td>10.6952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.40</td>\n",
       "      <td>33</td>\n",
       "      <td>42.5920</td>\n",
       "      <td>1.290667</td>\n",
       "      <td>9.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.42</td>\n",
       "      <td>31</td>\n",
       "      <td>40.4712</td>\n",
       "      <td>1.305523</td>\n",
       "      <td>9.4712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.44</td>\n",
       "      <td>31</td>\n",
       "      <td>40.4712</td>\n",
       "      <td>1.305523</td>\n",
       "      <td>9.4712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>2.60</td>\n",
       "      <td>2.46</td>\n",
       "      <td>29</td>\n",
       "      <td>38.3064</td>\n",
       "      <td>1.320910</td>\n",
       "      <td>9.3064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2.34</td>\n",
       "      <td>27</td>\n",
       "      <td>35.8688</td>\n",
       "      <td>1.328474</td>\n",
       "      <td>8.8688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2.32</td>\n",
       "      <td>27</td>\n",
       "      <td>35.8688</td>\n",
       "      <td>1.328474</td>\n",
       "      <td>8.8688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      goals_total_under  under_2_5_val  number_of_matches  money_won  \\\n",
       "2608               2.60           2.32                 35    46.7808   \n",
       "2609               2.60           2.34                 35    46.7808   \n",
       "2610               2.60           2.36                 35    46.7808   \n",
       "2611               2.60           2.38                 34    44.6952   \n",
       "2612               2.60           2.40                 33    42.5920   \n",
       "2613               2.60           2.42                 31    40.4712   \n",
       "2614               2.60           2.44                 31    40.4712   \n",
       "2615               2.60           2.46                 29    38.3064   \n",
       "2548               2.55           2.34                 27    35.8688   \n",
       "2547               2.55           2.32                 27    35.8688   \n",
       "\n",
       "         yield  money_won_over_all_matches  \n",
       "2608  1.336594                     11.7808  \n",
       "2609  1.336594                     11.7808  \n",
       "2610  1.336594                     11.7808  \n",
       "2611  1.314565                     10.6952  \n",
       "2612  1.290667                      9.5920  \n",
       "2613  1.305523                      9.4712  \n",
       "2614  1.305523                      9.4712  \n",
       "2615  1.320910                      9.3064  \n",
       "2548  1.328474                      8.8688  \n",
       "2547  1.328474                      8.8688  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25eaac-b71f-4a21-9915-b71e2a3ad09d",
   "metadata": {},
   "source": [
    "### Summary:\r\n",
    "\r\n",
    "- **Over 2.5 goals betting** offers a larger pool of matches to bet on, since games with higher goal totals tend to be more common, especially in certain leagues or when teams with strong offensive capabilities face each other. This provides more opportunities to place bets. However, due to the higher frequency of such matches and the relatively lower odds associated with them, the yield is smaller, meaning that the profit margin per match is reduced. Nonetheless, when we aggregate the bets across all qualifying matches, this strategy still results in a respectable cumulative profit of **4.83 units**. This makes it a steady, lower-risk option with more frequent opportunities to bet.\r\n",
    "\r\n",
    "- **Under 2.5 goals betting**, on the other hand, involves fewer matches because lower-scoring games tend to be less frequent, especially in leagues where attacking play is dominant. As a result, the number of qualifying matches for under 2.5 goals is smaller. However, the **higher yield** in these bets compensates for the lower volume. The odds for under 2.5 goals are often more favorable, and when combined with our model's accuracy in predicting lower-scoring games, this strategy yields a **total profit of over 11 units** across all matches. This makes it a higher-reward option, though it may come with slightly fewer betting opportunities.\r\n",
    "\r\n",
    "- By **combining both strategies**, we can create a more balanced and diversified betting approach. The higher volume of bets in the **over 2.5 goals** market ensures consistent betting opportunities and gradual profit accumulation, while the higher profitability per match in the **under 2.5 goals** market boosts the overall returns. This blend allows us to take advantage of both high-scoring and low-scoring matches, optimizing the potential for profit while spreading the risk. By leveraging the strengths of both strategies, we are better positioned to maximize returns and mitigate the impact of unpredictable match outcomes, ensuring a more stable long-term betting strategy.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
